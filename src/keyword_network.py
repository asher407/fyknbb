"""
å…³é”®è¯å…³ç³»ç½‘ç»œå¯è§†åŒ–æ¨¡å— (æ•´åˆç‰ˆ)

æ•´åˆäº† keyword_network.pyã€demo_network.py å’Œ test_keyword_network.py çš„åŠŸèƒ½ã€‚
æä¾›å…³é”®è¯æå–ã€å…³ç³»ç½‘ç»œæ„å»ºå’Œå¯è§†åŒ–åŠŸèƒ½ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
    1. å…³é”®è¯æå–ï¼šä½¿ç”¨ jieba åˆ†è¯ä»æ ‡é¢˜ä¸­æå–å…³é”®è¯
    2. ç½‘ç»œæ„å»ºï¼šåŸºäºå…³é”®è¯åœ¨åŒä¸€çƒ­æœæ¡ç›®ä¸­çš„å…±ç°å…³ç³»æ„å»ºç½‘ç»œ
    3. ç½‘ç»œåˆ†æï¼šè®¡ç®—èŠ‚ç‚¹ä¸­å¿ƒæ€§ç­‰ç½‘ç»œæŒ‡æ ‡
    4. å¯è§†åŒ–ï¼šç”Ÿæˆäº¤äº’å¼ç½‘ç»œå›¾ï¼ˆHTMLï¼‰å’Œé™æ€å›¾ï¼ˆPNGï¼‰
    5. å‘½ä»¤è¡Œæ¥å£ï¼šæ”¯æŒå¤„ç†å•ä¸ªæ–‡ä»¶æˆ–æ•´ä¸ªç›®å½•

ä½¿ç”¨ç¤ºä¾‹ï¼š
    # å‘½ä»¤è¡Œä½¿ç”¨
    python keyword_network.py data_processed/2025-01/2025-01-01.json

    # Python API ä½¿ç”¨
    from keyword_network import KeywordNetwork
    processor = KeywordNetwork()
    processor.process_file("data.json", "output")
"""

import argparse
import json
import os
import re
import sys
import time
from collections import Counter, defaultdict
from typing import Any, Dict, List, Optional, Set

# å¯é€‰ä¾èµ–ï¼Œè¿è¡Œæ—¶æ£€æŸ¥
try:
    import jieba

    JIEBA_AVAILABLE = True
except ImportError:
    JIEBA_AVAILABLE = False

try:
    import networkx as nx

    NETWORKX_AVAILABLE = True
except ImportError:
    NETWORKX_AVAILABLE = False

try:
    from pyvis.network import Network

    PYVIS_AVAILABLE = True
except ImportError:
    PYVIS_AVAILABLE = False

try:
    import matplotlib.pyplot as plt

    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False


class KeywordNetwork:
    """
    å…³é”®è¯å…³ç³»ç½‘ç»œå¤„ç†å™¨
    """

    def __init__(
        self,
        min_word_length: int = 2,
        stopwords: Optional[Set[str]] = None,
        font_family: str = "DejaVu Serif",
        font_size: int = 20,
        static_font_family: str = "DejaVu Serif",
        static_font_size: int = 16,
        node_label_font_size: int = 10,
    ):
        """
        åˆå§‹åŒ–å…³é”®è¯ç½‘ç»œå¤„ç†å™¨

        å‚æ•°ï¼š
            min_word_length: æœ€å°è¯è¯­é•¿åº¦
            stopwords: è‡ªå®šä¹‰åœç”¨è¯é›†åˆï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤åœç”¨è¯
            font_family: äº¤äº’å¼ç½‘ç»œå›¾å­—ä½“ (é»˜è®¤: Microsoft YaHei)
            font_size: äº¤äº’å¼ç½‘ç»œå›¾å­—ä½“å¤§å° (é»˜è®¤: 20)
            static_font_family: é™æ€å›¾å­—ä½“ (é»˜è®¤: sans-serif)
            static_font_size: é™æ€å›¾æ ‡é¢˜å­—ä½“å¤§å° (é»˜è®¤: 16)
            node_label_font_size: é™æ€å›¾èŠ‚ç‚¹æ ‡ç­¾å­—ä½“å¤§å° (é»˜è®¤: 10)
        """
        if not JIEBA_AVAILABLE:
            raise ImportError("jieba åº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install jieba")

        self.min_word_length = min_word_length

        # é»˜è®¤åœç”¨è¯é›†åˆï¼ˆç²¾ç®€ç‰ˆï¼‰
        self.default_stopwords = {
            "çš„",
            "äº†",
            "å’Œ",
            "æ˜¯",
            "åœ¨",
            "åˆ°",
            "ä¸€",
            "ä¸ª",
            "ä¸º",
            "ä¸­",
            "å›åº”",
            "ä»€ä¹ˆ",
            "æ€ä¹ˆ",
            "è¿™ä¹ˆ",
            "ä¸ºä»€ä¹ˆ",
            "ä¸è¦",
            "çœŸçš„",
            "ä¸æ˜¯",
            "å°±æ˜¯",
            "å¯èƒ½",
            "è¦æ±‚",
            "è¿˜æ˜¯",
            "å°æ—¶",
            "ç–‘ä¼¼",
            "å—",
            "å‘¢",
            "å§",
            "å•Š",
            "å“¦",
            "è¿™",
            "é‚£",
            "æœ‰",
            "æ²¡æœ‰",
            "æ²¡",
            "å¾ˆ",
            "æ¯”",
            "æ›´",
            "æœ€",
            "å°±",
            "è¿˜",
            "ä¹Ÿ",
            "è¢«",
            "æŠŠ",
            "å‘",
            "è®©",
            "ç»™",
            "ä»",
        }

        self.stopwords = stopwords if stopwords is not None else self.default_stopwords

        # å­—ä½“é…ç½®
        self.font_family = font_family
        self.font_size = font_size
        self.static_font_family = static_font_family
        self.static_font_size = static_font_size
        self.node_label_font_size = node_label_font_size

        # è‡ªå®šä¹‰è¯å…¸ï¼ˆä¿æŒæ•´ä½“è¯†åˆ«ï¼‰
        self.custom_words = [
            "ç‹æ¥šé’¦",
            "èµµéœ²æ€",
            "è‚–æˆ˜",
            "ç‹ä¸€åš",
            "æ˜“çƒŠåƒçº",
            "è¿ªä¸½çƒ­å·´",
            "æ¨å¹‚",
            "åˆ˜äº¦è²",
            "èƒ¡æ­Œ",
            "å‘¨æ°ä¼¦",
        ]

        for word in self.custom_words:
            jieba.add_word(word, freq=1000)

    def load_json(self, file_path: str) -> Dict[str, Any]:
        """
        åŠ è½½ JSON æ–‡ä»¶

        å‚æ•°ï¼š
            file_path: JSON æ–‡ä»¶è·¯å¾„

        è¿”å›ï¼š
            JSON æ•°æ®å­—å…¸
        """
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)

    def extract_keywords(self, text: str) -> List[str]:
        """
        ä»å•æ¡æ–‡æœ¬ä¸­æå–å…³é”®è¯

        å‚æ•°ï¼š
            text: è¾“å…¥æ–‡æœ¬

        è¿”å›ï¼š
            å…³é”®è¯åˆ—è¡¨
        """
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦å’Œæ ‡ç‚¹
        text = re.sub(r"[^\w\u4e00-\u9fff]", " ", text)

        # ä½¿ç”¨ jieba åˆ†è¯
        words = jieba.cut(text)

        # è¿‡æ»¤æ¡ä»¶
        keywords = []
        for word in words:
            word = word.strip()
            if len(word) < self.min_word_length:
                continue
            if word in self.stopwords:
                continue
            if word.isdigit():
                continue
            if re.match(r"^[a-zA-Z]{1,2}$", word):  # è¿‡æ»¤å•ä¸ªæˆ–ä¸¤ä¸ªå­—æ¯çš„è‹±æ–‡
                continue

            keywords.append(word)

        return keywords

    def extract_keywords_from_data(self, data: Dict[str, Any]) -> List[List[str]]:
        """
        ä»æ•°æ®ä¸­æå–æ‰€æœ‰æ¡ç›®çš„å…³é”®è¯

        å‚æ•°ï¼š
            data: JSON æ•°æ®ï¼ŒåŒ…å« 'data' å­—æ®µ

        è¿”å›ï¼š
            æ¯ä¸ªæ¡ç›®çš„å…³é”®è¯åˆ—è¡¨çš„åˆ—è¡¨
        """
        items = data.get("data", [])
        keywords_by_item = []

        for item in items:
            title = item.get("title", "")
            if not title:
                continue

            keywords = self.extract_keywords(title)
            if keywords:  # åªä¿ç•™æœ‰å…³é”®è¯çš„æ¡ç›®
                keywords_by_item.append(keywords)

        return keywords_by_item

    def build_cooccurrence_network(
        self, keywords_by_item: List[List[str]], min_cooccurrence: int = 1
    ) -> nx.Graph:
        """
        æ„å»ºå…³é”®è¯å…±ç°ç½‘ç»œ

        å‚æ•°ï¼š
            keywords_by_item: æ¯ä¸ªæ¡ç›®çš„å…³é”®è¯åˆ—è¡¨
            min_cooccurrence: æœ€å°å…±ç°æ¬¡æ•°é˜ˆå€¼

        è¿”å›ï¼š
            networkx å›¾å¯¹è±¡
        """
        if not NETWORKX_AVAILABLE:
            raise ImportError("networkx åº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install networkx")

        # ç»Ÿè®¡å…³é”®è¯é¢‘ç‡å’Œå…±ç°æ¬¡æ•°
        keyword_freq = Counter()
        cooccurrence_counts = defaultdict(int)

        for keywords in keywords_by_item:
            # æ›´æ–°å…³é”®è¯é¢‘ç‡
            for keyword in keywords:
                keyword_freq[keyword] += 1

            # æ›´æ–°å…±ç°æ¬¡æ•°ï¼ˆåŒä¸€æ¡ç›®å†…çš„å…³é”®è¯ä¸¤ä¸¤å…±ç°ï¼‰
            for i in range(len(keywords)):
                for j in range(i + 1, len(keywords)):
                    pair = tuple(sorted([keywords[i], keywords[j]]))
                    cooccurrence_counts[pair] += 1

        # åˆ›å»ºå›¾
        G = nx.Graph()

        # æ·»åŠ èŠ‚ç‚¹ï¼ˆå…³é”®è¯ï¼‰
        for keyword, freq in keyword_freq.items():
            if freq >= 1:  # è‡³å°‘å‡ºç°ä¸€æ¬¡
                G.add_node(keyword, size=freq, frequency=freq)

        # æ·»åŠ è¾¹ï¼ˆå…±ç°å…³ç³»ï¼‰
        for (kw1, kw2), count in cooccurrence_counts.items():
            if count >= min_cooccurrence and kw1 in G.nodes() and kw2 in G.nodes():
                G.add_edge(kw1, kw2, weight=count)

        return G

    def calculate_network_metrics(self, graph: nx.Graph) -> Dict[str, Any]:
        """
        è®¡ç®—ç½‘ç»œæŒ‡æ ‡

        å‚æ•°ï¼š
            graph: networkx å›¾å¯¹è±¡

        è¿”å›ï¼š
            åŒ…å«å„ç§ç½‘ç»œæŒ‡æ ‡çš„å­—å…¸
        """
        if len(graph.nodes()) == 0:
            return {}

        metrics = {}

        # åŸºç¡€æŒ‡æ ‡
        metrics["num_nodes"] = graph.number_of_nodes()
        metrics["num_edges"] = graph.number_of_edges()
        metrics["density"] = nx.density(graph)

        # åº¦ä¸­å¿ƒæ€§
        try:
            degree_centrality = nx.degree_centrality(graph)
            if degree_centrality:
                top_degree = sorted(
                    degree_centrality.items(), key=lambda x: x[1], reverse=True
                )[:10]
                metrics["top_degree_centrality"] = top_degree
        except:
            pass

        # å¹³å‡èšç±»ç³»æ•°
        try:
            metrics["average_clustering"] = nx.average_clustering(graph)
        except:
            pass

        return metrics

    def visualize_network(
        self,
        graph: nx.Graph,
        output_path: str,
        title: str = "å…³é”®è¯å…³ç³»ç½‘ç»œ",
        height: str = "800px",
        width: str = "100%",
    ) -> str:
        """
        ç”Ÿæˆäº¤äº’å¼ç½‘ç»œå¯è§†åŒ–ï¼ˆHTMLï¼‰

        å‚æ•°ï¼š
            graph: networkx å›¾å¯¹è±¡
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            title: ç½‘ç»œå›¾æ ‡é¢˜
            height: ç”»å¸ƒé«˜åº¦
            width: ç”»å¸ƒå®½åº¦

        è¿”å›ï¼š
            è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if not PYVIS_AVAILABLE:
            raise ImportError("pyvis åº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install pyvis")

        if len(graph.nodes()) == 0:
            print("è­¦å‘Šï¼šå›¾ä¸­æ²¡æœ‰èŠ‚ç‚¹ï¼Œè·³è¿‡å¯è§†åŒ–")
            return ""

        # åˆ›å»º pyvis ç½‘ç»œ
        net = Network(height=height, width=width, directed=False, notebook=False)

        # æ·»åŠ èŠ‚ç‚¹å’Œè¾¹åˆ° pyvis
        net.from_nx(graph)

        # è®¾ç½®åŸºæœ¬é€‰é¡¹
        options = {
            "nodes": {
                "borderWidth": 2,
                "font": {"size": self.font_size, "face": self.font_family},
                "scaling": {"min": 20, "max": 60},
            },
            "edges": {"color": "#848484", "smooth": {"type": "continuous"}, "width": 2},
            "physics": {"enabled": True, "solver": "forceAtlas2Based"},
        }

        # ä¿®å¤ï¼šä½¿ç”¨ set_options æ–¹æ³•æ­£ç¡®ä¼ é€’é€‰é¡¹
        import json

        net.set_options(json.dumps(options))

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_path), exist_ok=True)

        # ä¿å­˜ä¸º HTML
        net.save_graph(output_path)

        # åŒæ—¶ç”Ÿæˆä¸€ä¸ªé™æ€ PNG å›¾åƒï¼ˆå¯é€‰ï¼‰
        if MATPLOTLIB_AVAILABLE:
            try:
                static_path = output_path.replace(".html", ".png")
                self._save_static_plot(graph, static_path, title)
            except Exception as e:
                print(f"è­¦å‘Šï¼šé™æ€å›¾ç”Ÿæˆå¤±è´¥: {e}")

        return output_path

    def _save_static_plot(self, graph: nx.Graph, output_path: str, title: str):
        """
        ç”Ÿæˆé™æ€ç½‘ç»œå›¾ï¼ˆPNGï¼‰

        å‚æ•°ï¼š
            graph: networkx å›¾å¯¹è±¡
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            title: å›¾æ ‡é¢˜
        """

        # å­—ä½“æ£€æŸ¥ä¸å›é€€æœºåˆ¶
        def get_available_font():
            """æ£€æŸ¥å­—ä½“æ˜¯å¦å¯ç”¨ï¼Œå¦‚æœä¸å¯ç”¨åˆ™å°è¯•æ›¿ä»£å­—ä½“"""
            import matplotlib.font_manager as fm

            # é¦–é€‰å­—ä½“åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
            font_candidates = [
                self.static_font_family,  # ç”¨æˆ·æŒ‡å®šçš„å­—ä½“
                "Noto Sans CJK SC",  # Ubuntu ä¸­æ–‡é»˜è®¤å­—ä½“
                "DejaVu Sans",  # Linux å¹¿æ³›å¯ç”¨å­—ä½“
                "Ubuntu",  # Ubuntu é»˜è®¤è‹±æ–‡å­—ä½“
                "WenQuanYi Micro Hei",  # æ–‡æ³‰é©¿å¾®ç±³é»‘
                "sans-serif",  # ç³»ç»Ÿé»˜è®¤
            ]

            # è·å–ç³»ç»Ÿå¯ç”¨å­—ä½“
            available_fonts = [f.name for f in fm.fontManager.ttflist]

            # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªå¯ç”¨çš„å­—ä½“
            for font_name in font_candidates:
                if font_name in available_fonts:
                    if font_name != self.static_font_family:
                        print(
                            f"æç¤º: ä½¿ç”¨æ›¿ä»£å­—ä½“ '{font_name}' (åŸå­—ä½“ '{self.static_font_family}' ä¸å¯ç”¨)"
                        )
                    return font_name

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•å€™é€‰å­—ä½“ï¼Œä½¿ç”¨é»˜è®¤
            print("è­¦å‘Š: æ‰€æœ‰å€™é€‰å­—ä½“éƒ½ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“ 'sans-serif'")
            return "sans-serif"

        try:
            # è·å–å¯ç”¨çš„å­—ä½“
            actual_font_family = get_available_font()
        except Exception as e:
            print(f"å­—ä½“æ£€æŸ¥å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“")
            actual_font_family = "sans-serif"

        plt.figure(figsize=(12, 10))

        # è®¡ç®—èŠ‚ç‚¹å¤§å°ï¼ˆåŸºäºåº¦ï¼‰
        node_sizes = []
        for node in graph.nodes():
            size = (
                graph.nodes[node].get("size", 1)
                if hasattr(graph.nodes[node], "get")
                else 1
            )
            node_sizes.append(size * 50)

        # è®¡ç®—èŠ‚ç‚¹é¢œè‰²ï¼ˆåŸºäºåº¦ï¼‰
        degrees = dict(graph.degree())
        max_degree = max(degrees.values()) if degrees else 1
        node_colors = [degrees[node] / max_degree for node in graph.nodes()]

        # ç»˜åˆ¶ç½‘ç»œ
        pos = nx.spring_layout(graph, seed=42)

        # ç»˜åˆ¶è¾¹
        nx.draw_networkx_edges(graph, pos, alpha=0.3, edge_color="gray")

        # ç»˜åˆ¶èŠ‚ç‚¹
        nx.draw_networkx_nodes(
            graph,
            pos,
            node_size=node_sizes,
            node_color=node_colors,
            cmap="viridis",
            alpha=0.8,
        )

        # ç»˜åˆ¶æ ‡ç­¾ï¼ˆåªæ˜¾ç¤ºé«˜é¢‘è¯ï¼‰
        labels = {}
        for node in graph.nodes():
            freq = (
                graph.nodes[node].get("frequency", 0)
                if hasattr(graph.nodes[node], "get")
                else 0
            )
            if freq >= 3:  # åªæ˜¾ç¤ºå‡ºç°3æ¬¡ä»¥ä¸Šçš„è¯
                labels[node] = node

        nx.draw_networkx_labels(
            graph,
            pos,
            labels,
            font_size=self.node_label_font_size,
            font_family=actual_font_family,
        )

        plt.title(title, fontsize=self.static_font_size, fontfamily=actual_font_family)
        plt.axis("off")
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches="tight")
        plt.close()

    def save_metrics(self, metrics: Dict[str, Any], output_path: str):
        """
        ä¿å­˜ç½‘ç»œæŒ‡æ ‡åˆ° JSON æ–‡ä»¶

        å‚æ•°ï¼š
            metrics: ç½‘ç»œæŒ‡æ ‡å­—å…¸
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        os.makedirs(os.path.dirname(output_path), exist_ok=True)

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(metrics, f, ensure_ascii=False, indent=2)

    def process_file(
        self,
        input_json_path: str,
        output_dir: str = "output/keyword_networks",
        min_cooccurrence: int = 1,
    ) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ª JSON æ–‡ä»¶

        å‚æ•°ï¼š
            input_json_path: è¾“å…¥ JSON æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            min_cooccurrence: æœ€å°å…±ç°æ¬¡æ•°é˜ˆå€¼

        è¿”å›ï¼š
            å¤„ç†ç»“æœå­—å…¸
        """
        print(f"å¤„ç†æ–‡ä»¶: {input_json_path}")

        # åŠ è½½æ•°æ®
        data = self.load_json(input_json_path)

        # æå–å…³é”®è¯
        keywords_by_item = self.extract_keywords_from_data(data)

        if not keywords_by_item:
            print(f"è­¦å‘Š: {input_json_path} ä¸­æ²¡æœ‰æå–åˆ°å…³é”®è¯")
            return {}

        # æ„å»ºç½‘ç»œ
        graph = self.build_cooccurrence_network(keywords_by_item, min_cooccurrence)

        if len(graph.nodes()) == 0:
            print(f"è­¦å‘Š: {input_json_path} æ„å»ºçš„ç½‘ç»œæ²¡æœ‰èŠ‚ç‚¹")
            return {}

        # è®¡ç®—æŒ‡æ ‡
        metrics = self.calculate_network_metrics(graph)

        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        base_name = os.path.splitext(os.path.basename(input_json_path))[0]
        output_base = os.path.join(output_dir, base_name)

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)

        # ä¿å­˜ç½‘ç»œå¯è§†åŒ–
        html_path = f"{output_base}_network.html"
        self.visualize_network(graph, html_path, title=f"å…³é”®è¯å…³ç³»ç½‘ç»œ - {base_name}")

        # ä¿å­˜ç½‘ç»œæŒ‡æ ‡
        metrics_path = f"{output_base}_metrics.json"
        self.save_metrics(metrics, metrics_path)

        result = {
            "input_file": input_json_path,
            "output_html": html_path,
            "output_metrics": metrics_path,
            "num_items": len(keywords_by_item),
            "num_nodes": graph.number_of_nodes(),
            "num_edges": graph.number_of_edges(),
            "metrics": metrics,
        }

        print(
            f"å®Œæˆ: ç”Ÿæˆç½‘ç»œå›¾ {html_path} (èŠ‚ç‚¹: {graph.number_of_nodes()}, è¾¹: {graph.number_of_edges()})"
        )
        return result

    def process_directory(
        self,
        input_dir: str,
        output_dir: str = "output/keyword_networks",
        min_cooccurrence: int = 1,
    ) -> List[Dict[str, Any]]:
        """
        å¤„ç†ç›®å½•ä¸‹çš„æ‰€æœ‰ JSON æ–‡ä»¶ï¼ˆåˆå¹¶æ‰€æœ‰æ–‡ä»¶æ•°æ®ï¼‰

        å‚æ•°ï¼š
            input_dir: è¾“å…¥ç›®å½•
            output_dir: è¾“å‡ºç›®å½•
            min_cooccurrence: æœ€å°å…±ç°æ¬¡æ•°é˜ˆå€¼

        è¿”å›ï¼š
            å¤„ç†ç»“æœåˆ—è¡¨ï¼ˆåŒ…å«å•ä¸ªç»“æœçš„åˆ—è¡¨ä»¥ä¿æŒå…¼å®¹æ€§ï¼‰
        """
        print(f"å¤„ç†ç›®å½•ï¼ˆåˆå¹¶æ¨¡å¼ï¼‰: {input_dir}")

        all_keywords_by_item = []
        file_count = 0
        processed_files = []

        # æ”¶é›†æ‰€æœ‰æ–‡ä»¶çš„æ•°æ®ï¼ˆé€’å½’éå†ï¼‰
        for root, _, files in os.walk(input_dir):
            for file in files:
                if file.endswith(".json") and not file.endswith(".bak"):
                    input_path = os.path.join(root, file)
                    print(f"  è¯»å–æ–‡ä»¶: {input_path}")

                    # åŠ è½½æ•°æ®
                    try:
                        data = self.load_json(input_path)
                        keywords_by_item = self.extract_keywords_from_data(data)

                        if keywords_by_item:
                            all_keywords_by_item.extend(keywords_by_item)
                            file_count += 1
                            processed_files.append(input_path)
                        else:
                            print(f"  è­¦å‘Š: {input_path} ä¸­æ²¡æœ‰æå–åˆ°å…³é”®è¯")
                    except Exception as e:
                        print(f"  é”™è¯¯: å¤„ç†æ–‡ä»¶ {input_path} æ—¶å‡ºé”™: {e}")

        if not all_keywords_by_item:
            print(f"è­¦å‘Š: ç›®å½• {input_dir} ä¸­æ²¡æœ‰æå–åˆ°ä»»ä½•å…³é”®è¯")
            return []

        print(f"  ä» {file_count} ä¸ªæ–‡ä»¶ä¸­æå–äº† {len(all_keywords_by_item)} ä¸ªæ¡ç›®")

        # æ„å»ºç½‘ç»œ
        graph = self.build_cooccurrence_network(all_keywords_by_item, min_cooccurrence)

        if len(graph.nodes()) == 0:
            print(f"è­¦å‘Š: ç›®å½• {input_dir} æ„å»ºçš„ç½‘ç»œæ²¡æœ‰èŠ‚ç‚¹")
            return []

        # è®¡ç®—æŒ‡æ ‡
        metrics = self.calculate_network_metrics(graph)

        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶åï¼ˆä½¿ç”¨ç›®å½•åï¼‰
        dir_name = os.path.basename(os.path.normpath(input_dir))
        if not dir_name or dir_name == ".":
            dir_name = "combined"

        output_base = os.path.join(output_dir, dir_name)

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)

        # ä¿å­˜ç½‘ç»œå¯è§†åŒ–
        html_path = f"{output_base}_network.html"
        self.visualize_network(
            graph, html_path, title=f"å…³é”®è¯å…³ç³»ç½‘ç»œ - {dir_name} ({file_count}ä¸ªæ–‡ä»¶)"
        )

        # ä¿å­˜ç½‘ç»œæŒ‡æ ‡
        metrics_path = f"{output_base}_metrics.json"
        self.save_metrics(metrics, metrics_path)

        result = {
            "input_dir": input_dir,
            "processed_files": processed_files,
            "output_html": html_path,
            "output_metrics": metrics_path,
            "num_files": file_count,
            "num_items": len(all_keywords_by_item),
            "num_nodes": graph.number_of_nodes(),
            "num_edges": graph.number_of_edges(),
            "metrics": metrics,
        }

        print(
            f"å®Œæˆ: ç”Ÿæˆåˆå¹¶ç½‘ç»œå›¾ {html_path} "
            f"(æ¥è‡ª {file_count} ä¸ªæ–‡ä»¶, "
            f"èŠ‚ç‚¹: {graph.number_of_nodes()}, "
            f"è¾¹: {graph.number_of_edges()})"
        )

        # è¿”å›åŒ…å«å•ä¸ªç»“æœçš„åˆ—è¡¨ä»¥ä¿æŒä¸ç°æœ‰ä»£ç çš„å…¼å®¹æ€§
        return [result]

    def _generate_summary_report(self, results: List[Dict[str, Any]], output_dir: str):
        """
        ç”Ÿæˆå¤„ç†ç»“æœçš„æ±‡æ€»æŠ¥å‘Š

        å‚æ•°ï¼š
            results: å¤„ç†ç»“æœåˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
        """
        summary = {
            "total_files": len(results),
            "total_nodes": sum(r.get("num_nodes", 0) for r in results),
            "total_edges": sum(r.get("num_edges", 0) for r in results),
            "files": results,
        }

        summary_path = os.path.join(output_dir, "summary_report.json")
        with open(summary_path, "w", encoding="utf-8") as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

        print(f"ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š: {summary_path}")


def print_banner():
    """æ‰“å°ç¨‹åºæ¨ªå¹…"""
    banner = """
    ========================================
        å…³é”®è¯å…³ç³»ç½‘ç»œå¯è§†åŒ–å·¥å…·
    ========================================

    åŠŸèƒ½ï¼š
    1. ä»å¾®åšçƒ­æœæ•°æ®ä¸­æå–å…³é”®è¯
    2. æ„å»ºå…³é”®è¯å…±ç°ç½‘ç»œ
    3. è®¡ç®—ç½‘ç»œåˆ†ææŒ‡æ ‡
    4. ç”Ÿæˆäº¤äº’å¼ç½‘ç»œå¯è§†åŒ–

    æ”¯æŒè¾“å…¥ï¼šå•ä¸ªJSONæ–‡ä»¶æˆ–æ•´ä¸ªç›®å½•
    è¾“å‡ºç›®å½•ï¼šoutput/keyword_networks/
    ========================================
    """
    print(banner)


def print_summary(results, elapsed_time):
    """æ‰“å°å¤„ç†ç»“æœæ‘˜è¦"""
    print("\n" + "=" * 50)
    print("å¤„ç†å®Œæˆï¼")
    print("=" * 50)

    if not results:
        print("æœªå¤„ç†ä»»ä½•æ–‡ä»¶")
        return

    total_files = len(results)
    total_nodes = sum(r.get("num_nodes", 0) for r in results)
    total_edges = sum(r.get("num_edges", 0) for r in results)

    print(f"å¤„ç†æ—¶é—´: {elapsed_time:.2f} ç§’")
    print(f"å¤„ç†æ–‡ä»¶æ•°: {total_files}")
    print(f"æ€»èŠ‚ç‚¹æ•°: {total_nodes}")
    print(f"æ€»è¾¹æ•°: {total_edges}")
    print(f"å¹³å‡èŠ‚ç‚¹/æ–‡ä»¶: {total_nodes / total_files:.1f}")
    print(f"å¹³å‡è¾¹/æ–‡ä»¶: {total_edges / total_files:.1f}")

    # æ˜¾ç¤ºå‰å‡ ä¸ªæ–‡ä»¶çš„è¯¦ç»†ä¿¡æ¯
    print("\nå‰5ä¸ªæ–‡ä»¶è¯¦æƒ…:")
    print("-" * 50)
    for i, result in enumerate(results[:5]):
        input_file = os.path.basename(result["input_file"])
        num_nodes = result.get("num_nodes", 0)
        num_edges = result.get("num_edges", 0)
        html_file = os.path.basename(result["output_html"])
        print(f"{i + 1}. {input_file}: {num_nodes} èŠ‚ç‚¹, {num_edges} è¾¹ â†’ {html_file}")

    if len(results) > 5:
        print(f"... è¿˜æœ‰ {len(results) - 5} ä¸ªæ–‡ä»¶")

    # æä¾›ä½¿ç”¨æç¤º
    print("\n" + "=" * 50)
    print("ä½¿ç”¨æç¤º:")
    print("=" * 50)
    print("1. æ‰“å¼€ç”Ÿæˆçš„HTMLæ–‡ä»¶è¿›è¡Œäº¤äº’å¼ç½‘ç»œæ¢ç´¢")
    print("2. æŸ¥çœ‹JSONæŒ‡æ ‡æ–‡ä»¶è·å–è¯¦ç»†ç½‘ç»œåˆ†ææ•°æ®")
    print("3. ä½¿ç”¨æµè§ˆå™¨æ‰“å¼€HTMLæ–‡ä»¶æŸ¥çœ‹äº¤äº’å¼ç½‘ç»œå›¾")


def validate_input_path(input_path):
    """éªŒè¯è¾“å…¥è·¯å¾„"""
    if not os.path.exists(input_path):
        print(f"é”™è¯¯: è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {input_path}")
        return False

    if os.path.isfile(input_path) and not input_path.endswith(".json"):
        print(f"é”™è¯¯: è¾“å…¥æ–‡ä»¶å¿…é¡»æ˜¯JSONæ ¼å¼: {input_path}")
        return False

    return True


def quick_test():
    """
    å¿«é€ŸåŠŸèƒ½æµ‹è¯•
    """
    print("=== å¿«é€ŸåŠŸèƒ½æµ‹è¯• ===\n")

    test_data = {
        "date": "2025-01-01",
        "count": 3,
        "data": [
            {
                "rank": 1,
                "title": "èµµéœ²æ€å‘é•¿æ–‡å›åº”",
                "category": "æ–‡å¨±",
                "heat": 1000.0,
                "date": "2025-01-01",
            },
            {
                "rank": 2,
                "title": "æ–°å¹´å¿«ä¹",
                "category": "ç”Ÿæ´»",
                "heat": 800.0,
                "date": "2025-01-01",
            },
            {
                "rank": 3,
                "title": "ç§åœ°å§ç›´æ’­",
                "category": "å½±è§†",
                "heat": 600.0,
                "date": "2025-01-01",
            },
        ],
    }

    import tempfile

    with tempfile.NamedTemporaryFile(
        mode="w", suffix=".json", delete=False, encoding="utf-8"
    ) as f:
        json.dump(test_data, f, ensure_ascii=False)
        temp_file = f.name

    try:
        processor = KeywordNetwork(min_word_length=2)

        # æµ‹è¯•æ•°æ®åŠ è½½
        data = processor.load_json(temp_file)
        print("âœ… æ•°æ®åŠ è½½æµ‹è¯•é€šè¿‡")

        # æµ‹è¯•å…³é”®è¯æå–
        keywords_by_item = processor.extract_keywords_from_data(data)
        print(f"âœ… å…³é”®è¯æå–æµ‹è¯•é€šè¿‡ï¼Œæå–åˆ° {len(keywords_by_item)} ä¸ªæ¡ç›®")

        # æµ‹è¯•ç½‘ç»œæ„å»º
        graph = processor.build_cooccurrence_network(keywords_by_item)
        print(f"âœ… ç½‘ç»œæ„å»ºæµ‹è¯•é€šè¿‡ï¼Œæ„å»ºäº† {graph.number_of_nodes()} ä¸ªèŠ‚ç‚¹")

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.unlink(temp_file)

        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        return True

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        if os.path.exists(temp_file):
            os.unlink(temp_file)
        return False


def load_config(config_file: str) -> Optional[Dict[str, Any]]:
    """
    ä»é…ç½®æ–‡ä»¶åŠ è½½å‚æ•°

    å‚æ•°ï¼š
        config_file: é…ç½®æ–‡ä»¶è·¯å¾„ (JSONæ ¼å¼)

    è¿”å›ï¼š
        é…ç½®å‚æ•°å­—å…¸ï¼Œå¤±è´¥æ—¶è¿”å›None
    """
    try:
        with open(config_file, "r", encoding="utf-8") as f:
            config = json.load(f)
        return config
    except FileNotFoundError:
        print(f"é”™è¯¯: é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        return None
    except json.JSONDecodeError as e:
        print(f"é”™è¯¯: é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
        return None
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶: {e}")
        return None


def main():
    """
    å‘½ä»¤è¡Œå…¥å£ç‚¹
    """
    parser = argparse.ArgumentParser(description="å…³é”®è¯å…³ç³»ç½‘ç»œå¯è§†åŒ–å·¥å…·")
    parser.add_argument("input", help="è¾“å…¥æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„", nargs="?")
    parser.add_argument(
        "-c",
        "--config",
        help="é…ç½®æ–‡ä»¶è·¯å¾„ (JSONæ ¼å¼)",
    )
    parser.add_argument(
        "-o",
        "--output",
        default="output/keyword_networks",
        help="è¾“å‡ºç›®å½• (é»˜è®¤: output/keyword_networks)",
    )
    parser.add_argument(
        "-m",
        "--min-cooccurrence",
        type=int,
        default=1,
        help="æœ€å°å…±ç°æ¬¡æ•°é˜ˆå€¼ (é»˜è®¤: 1)",
    )
    parser.add_argument(
        "-l", "--min-length", type=int, default=2, help="æœ€å°è¯è¯­é•¿åº¦ (é»˜è®¤: 2)"
    )
    parser.add_argument(
        "--font-family",
        default="Noto Sans CJK SC",
        help="äº¤äº’å¼ç½‘ç»œå›¾å­—ä½“ (é»˜è®¤: Noto Sans CJK SC)",
    )
    parser.add_argument(
        "--font-size",
        type=int,
        default=20,
        help="äº¤äº’å¼ç½‘ç»œå›¾å­—ä½“å¤§å° (é»˜è®¤: 20)",
    )
    parser.add_argument(
        "--static-font-family",
        default="DejaVu Sans",
        help="é™æ€å›¾å­—ä½“ (é»˜è®¤: DejaVu Sans)",
    )
    parser.add_argument(
        "--static-font-size",
        type=int,
        default=16,
        help="é™æ€å›¾æ ‡é¢˜å­—ä½“å¤§å° (é»˜è®¤: 16)",
    )
    parser.add_argument(
        "--node-label-font-size",
        type=int,
        default=10,
        help="é™æ€å›¾èŠ‚ç‚¹æ ‡ç­¾å­—ä½“å¤§å° (é»˜è®¤: 10)",
    )
    parser.add_argument("-t", "--test", action="store_true", help="è¿è¡Œå¿«é€ŸåŠŸèƒ½æµ‹è¯•")
    parser.add_argument("-q", "--quiet", action="store_true", help="å®‰é™æ¨¡å¼ï¼Œå‡å°‘è¾“å‡º")

    args = parser.parse_args()

    # å¦‚æœæä¾›äº†é…ç½®æ–‡ä»¶ï¼ŒåŠ è½½å¹¶æ›´æ–°å‚æ•°
    if args.config:
        config = load_config(args.config)
        if config is None:
            return 1  # load_configå·²ç»æ‰“å°äº†é”™è¯¯ä¿¡æ¯

        # æ›´æ–°å‚æ•°ï¼Œé…ç½®æ–‡ä»¶çš„ä¼˜å…ˆçº§é«˜äºå‘½ä»¤è¡Œå‚æ•°
        if "processing_settings" in config:
            ps = config["processing_settings"]
            if "min_word_length" in ps:
                args.min_length = ps["min_word_length"]
            if "min_cooccurrence" in ps:
                args.min_cooccurrence = ps["min_cooccurrence"]

        if "output_settings" in config and "output_dir" in config["output_settings"]:
            args.output = config["output_settings"]["output_dir"]

        # åªæœ‰åœ¨å‘½ä»¤è¡Œæœªæä¾›è¾“å…¥æ—¶ï¼Œæ‰ä½¿ç”¨é…ç½®æ–‡ä»¶çš„è¾“å…¥è·¯å¾„
        if (
            not args.input
            and "input_settings" in config
            and "input_path" in config["input_settings"]
        ):
            args.input = config["input_settings"]["input_path"]

        # æ›´æ–°å­—ä½“é…ç½®
        if "visualization_settings" in config:
            vs = config["visualization_settings"]
            if "html_output" in vs and "font_family" in vs["html_output"]:
                args.font_family = vs["html_output"]["font_family"]
            if "html_output" in vs and "font_size" in vs["html_output"]:
                args.font_size = vs["html_output"]["font_size"]
            if "static_output" in vs and "font_family" in vs["static_output"]:
                args.static_font_family = vs["static_output"]["font_family"]
            if "static_output" in vs and "font_size" in vs["static_output"]:
                args.static_font_size = vs["static_output"]["font_size"]
            # èŠ‚ç‚¹æ ‡ç­¾å­—ä½“å¤§å°å¯èƒ½éœ€è¦åœ¨configä¸­æ·»åŠ æ–°å­—æ®µ
            if "node_label_font_size" in vs:
                args.node_label_font_size = vs["node_label_font_size"]

    # æµ‹è¯•æ¨¡å¼
    if args.test:
        success = quick_test()
        return 0 if success else 1

    # æ­£å¸¸æ¨¡å¼
    else:
        # æ­£å¸¸æ¨¡å¼éœ€è¦è¾“å…¥å‚æ•°
        if not args.input:
            print("é”™è¯¯: éœ€è¦æŒ‡å®šè¾“å…¥æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„")
            print("ä½¿ç”¨ --help æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯")
            return 1

        if not args.quiet:
            print_banner()

        # éªŒè¯è¾“å…¥è·¯å¾„
        if not validate_input_path(args.input):
            return 1

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(args.output, exist_ok=True)

        # åˆ›å»ºå¤„ç†å™¨
        try:
            processor = KeywordNetwork(
                min_word_length=args.min_length,
                font_family=args.font_family,
                font_size=args.font_size,
                static_font_family=args.static_font_family,
                static_font_size=args.static_font_size,
                node_label_font_size=args.node_label_font_size,
            )
        except ImportError as e:
            print(f"é”™è¯¯: {e}")
            print("è¯·å®‰è£…æ‰€éœ€ä¾èµ–: pip install jieba networkx pyvis matplotlib")
            return 1
        except Exception as e:
            print(f"é”™è¯¯: æ— æ³•åˆ›å»ºå…³é”®è¯å¤„ç†å™¨: {e}")
            return 1

        # æ ¹æ®è¾“å…¥ç±»å‹è¿›è¡Œå¤„ç†
        try:
            start_time = time.time()

            if os.path.isfile(args.input):
                print(f"æ­£åœ¨å¤„ç†æ–‡ä»¶: {args.input}")
                result = processor.process_file(
                    args.input, args.output, min_cooccurrence=args.min_cooccurrence
                )
                results = [result] if result else []
            else:
                print(f"æ­£åœ¨å¤„ç†ç›®å½•: {args.input}")
                results = processor.process_directory(
                    args.input, args.output, min_cooccurrence=args.min_cooccurrence
                )

            elapsed_time = time.time() - start_time

            if results:
                if not args.quiet:
                    print_summary(results, elapsed_time)
            else:
                print("æ²¡æœ‰å¤„ç†ä»»ä½•æœ‰æ•ˆæ–‡ä»¶")

        except KeyboardInterrupt:
            print("\n\nç”¨æˆ·ä¸­æ–­å¤„ç†")
            return 0
        except Exception as e:
            print(f"\nå¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback

            traceback.print_exc()
            return 1

    return 0


if __name__ == "__main__":
    sys.exit(main())
