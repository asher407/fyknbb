"""
å¾®åšçƒ­æœå†å²æ•°æ®çˆ¬è™«æ¨¡å—

æœ¬æ¨¡å—ç”¨äºçˆ¬å–å¾®åšçƒ­æœå†å²æ•°æ®ç½‘ç«™ï¼ˆhttps://weibo-trending-hot-history.vercel.app/hots/{date}ï¼‰
å¹¶å°†æ•°æ®æŒ‰æ—¥æœŸä¿å­˜ä¸ºJSONæ ¼å¼ã€‚

ç±»å®šä¹‰ï¼š
    WeiboHotScraper: å¾®åšçƒ­æœçˆ¬è™«ä¸»ç±»
    RealtimeHotScraper: å®æ—¶çƒ­æœçˆ¬è™«ç±»

ä¸»è¦åŠŸèƒ½ï¼š
    1. çˆ¬å–æŒ‡å®šæ—¥æœŸèŒƒå›´çš„å¾®åšçƒ­æœæ•°æ®
    2. è§£æHTMLé¡µé¢ä¸­çš„çƒ­æœæ¡ç›®ä¿¡æ¯
    3. å°†æ•°æ®æŒ‰æœˆä»½ç»„ç»‡å¹¶ä¿å­˜ä¸ºJSONæ–‡ä»¶
    4. æä¾›é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶

ä½¿ç”¨ç¤ºä¾‹ï¼š
    scraper = WeiboHotScraper(output_dir="data")
    scraper.scrape_range("2025-01-01", "2025-12-12")
    - å®æ—¶çˆ¬è™«é¡µé¢è¿è¡Œå‘½ä»¤ï¼š
    python -m streamlit run src/gui/app.py
"""

import json
import logging
import os
import re
import time
from dataclasses import asdict, dataclass
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple

import requests
from bs4 import BeautifulSoup


@dataclass
class HotItem:
    """
    çƒ­æœæ¡ç›®æ•°æ®ç±»

    å±æ€§ï¼š
        rank: æ’åï¼ˆç¬¬å‡ åï¼‰
        title: çƒ­æœæ ‡é¢˜
        category: åˆ†ç±»ï¼ˆå¦‚ï¼šæ˜æ˜Ÿã€ç¤¾ä¼šç­‰ï¼‰
        heat: çƒ­åº¦å€¼ï¼ˆå•ä½ï¼šä¸‡ï¼‰
        reads: é˜…è¯»é‡ï¼ˆå•ä½ï¼šäº¿/ä¸‡ï¼‰
        discussions: è®¨è®ºé‡ï¼ˆå•ä½ï¼šä¸‡ï¼‰
        originals: åŸåˆ›é‡ï¼ˆå•ä½ï¼šä¸‡ï¼‰
        date: æ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰
    """

    rank: int
    title: str
    category: str
    heat: float
    reads: float
    discussions: float
    originals: float
    date: str


class WeiboHotScraper:
    """
    å¾®åšçƒ­æœçˆ¬è™«ä¸»ç±»

    åŠŸèƒ½ï¼š
        - çˆ¬å–æŒ‡å®šæ—¥æœŸçš„å¾®åšçƒ­æœæ•°æ®
        - è§£æHTMLä¸­çš„çƒ­æœæ¡ç›®ä¿¡æ¯
        - å°†æ•°æ®ä¿å­˜ä¸ºJSONæ ¼å¼æ–‡ä»¶
        - æŒ‰æœˆä»½ç»„ç»‡æ•°æ®æ–‡ä»¶

    å‚æ•°ï¼š
        base_url (str): ç›®æ ‡ç½‘ç«™çš„åŸºç¡€URL
        output_dir (str): è¾“å‡ºç›®å½•è·¯å¾„
        delay (float): è¯·æ±‚é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé˜²æ­¢è¢«å°IP
        timeout (int): è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        max_retries (int): æœ€å¤§é‡è¯•æ¬¡æ•°

    æ–¹æ³•ï¼š
        fetch_page(date: str) -> Optional[str]: è·å–æŒ‡å®šæ—¥æœŸçš„é¡µé¢HTML
        parse_page(html: str, date: str) -> List[HotItem]: è§£æHTMLæå–çƒ­æœæ•°æ®
        save_data(data: List[HotItem], date: str) -> bool: ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶
        scrape_date(date: str) -> bool: çˆ¬å–å¹¶ä¿å­˜æŒ‡å®šæ—¥æœŸçš„æ•°æ®
        scrape_range(start_date: str, end_date: str) -> Dict[str, Any]: çˆ¬å–æ—¥æœŸèŒƒå›´çš„æ•°æ®
    """

    def __init__(
        self,
        base_url: str = "https://weibo-trending-hot-history.vercel.app/hots",
        output_dir: str = "data",
        delay: float = 1.0,
        timeout: int = 10,
        max_retries: int = 3,
    ):
        """
        åˆå§‹åŒ–çˆ¬è™«

        å‚æ•°ï¼š
            base_url: ç›®æ ‡ç½‘ç«™çš„åŸºç¡€URL
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
            delay: è¯·æ±‚é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        self.base_url = base_url
        self.output_dir = output_dir
        self.delay = delay
        self.timeout = timeout
        self.max_retries = max_retries

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)

        # é…ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        )
        self.logger = logging.getLogger(__name__)

        # é…ç½®requestsä¼šè¯
        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                "Accept-Encoding": "gzip, deflate, br",
            }
        )

    def fetch_page(self, date: str) -> Optional[str]:
        """
        è·å–æŒ‡å®šæ—¥æœŸçš„é¡µé¢HTML

        å‚æ•°ï¼š
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸ºYYYY-MM-DD

        è¿”å›ï¼š
            HTMLå­—ç¬¦ä¸²æˆ–Noneï¼ˆå¦‚æœè¯·æ±‚å¤±è´¥ï¼‰
        """
        url = f"{self.base_url}/{date}"

        for attempt in range(self.max_retries):
            try:
                self.logger.info(
                    f"æ­£åœ¨è·å– {date} çš„æ•°æ® (å°è¯• {attempt + 1}/{self.max_retries})"
                )
                response = self.session.get(url, timeout=self.timeout)
                response.raise_for_status()

                # æ£€æŸ¥æ˜¯å¦è¿”å›æœ‰æ•ˆå†…å®¹
                if response.status_code == 200 and len(response.text) > 100:
                    # æ£€æŸ¥é¡µé¢æ˜¯å¦åŒ…å«æœ‰æ•ˆæ•°æ®
                    # æ³¨æ„ï¼šæœ‰äº›é¡µé¢å¯èƒ½åŒ…å«"æ²¡æœ‰æ‰¾åˆ°"æˆ–"404"ä½†ä»æœ‰å†…å®¹ï¼Œéœ€è¦æ›´æ™ºèƒ½çš„åˆ¤æ–­
                    html_text = response.text

                    # æ£€æŸ¥æ˜¯å¦æœ‰çƒ­æœæ¡ç›®çš„ç‰¹å¾
                    has_hot_features = (
                        "æŸ¥çœ‹å¾®åšè¯é¢˜" in html_text
                        or "text-xl" in html_text
                        or "inline-flex" in html_text
                    )

                    # å¦‚æœé¡µé¢æœ‰çƒ­æœç‰¹å¾ï¼Œå³ä½¿åŒ…å«"æ²¡æœ‰æ‰¾åˆ°"æˆ–"404"ä¹Ÿè¿”å›
                    if has_hot_features:
                        self.logger.info(f"æˆåŠŸè·å– {date} çš„æ•°æ®ï¼ˆæœ‰çƒ­æœç‰¹å¾ï¼‰")
                        return html_text

                    # å¦åˆ™ï¼Œæ£€æŸ¥æ˜¯å¦çœŸçš„æ˜¯æ— æ•ˆé¡µé¢
                    if "æ²¡æœ‰æ‰¾åˆ°" in html_text or "404" in html_text:
                        # è¿›ä¸€æ­¥ç¡®è®¤ï¼šæ£€æŸ¥é¡µé¢æ˜¯å¦çœŸçš„å¾ˆå°æˆ–è€…æ²¡æœ‰å®é™…å†…å®¹ç»“æ„
                        soup = BeautifulSoup(html_text, "html.parser")
                        a_tags = soup.find_all("a")

                        # å¦‚æœå‡ ä¹æ²¡æœ‰aæ ‡ç­¾ï¼Œå¯èƒ½æ˜¯çœŸçš„æ²¡æœ‰æ•°æ®
                        if len(a_tags) < 10:  # æ™®é€šé¡µé¢é€šå¸¸æœ‰å¾ˆå¤šaæ ‡ç­¾
                            self.logger.warning(f"{date} æ²¡æœ‰æ•°æ®ï¼ˆé¡µé¢ç»“æ„ç®€å•ï¼‰")
                            return None
                        else:
                            # æœ‰å¾ˆå¤šaæ ‡ç­¾ï¼Œå¯èƒ½åªæ˜¯é¡µé¢åŒ…å«404æ–‡æœ¬ä½†æœ‰å®é™…å†…å®¹
                            self.logger.info(f"è·å– {date} çš„æ•°æ®ï¼ˆæœ‰aæ ‡ç­¾ç»“æ„ï¼‰")
                            return html_text

                    self.logger.info(f"æˆåŠŸè·å– {date} çš„æ•°æ®")
                    return html_text
                else:
                    self.logger.warning(f"{date} è¿”å›äº†ç©ºé¡µé¢æˆ–æ— æ•ˆå“åº”")

            except requests.exceptions.RequestException as e:
                self.logger.error(f"è·å– {date} æ•°æ®å¤±è´¥: {e}")
                if attempt < self.max_retries - 1:
                    wait_time = 2**attempt  # æŒ‡æ•°é€€é¿
                    self.logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    self.logger.error(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè·³è¿‡ {date}")

            except Exception as e:
                self.logger.error(f"è·å– {date} æ•°æ®æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
                break

        return None

    def _extract_number(self, text: str) -> float:
        """
        ä»æ–‡æœ¬ä¸­æå–æ•°å­—ï¼Œå¤„ç†ä¸­æ–‡å•ä½

        å‚æ•°ï¼š
            text: åŒ…å«æ•°å­—å’Œå•ä½çš„æ–‡æœ¬ï¼ˆå¦‚ï¼š"855.15ä¸‡", "1.50äº¿", "8210"ï¼‰

        è¿”å›ï¼š
            è½¬æ¢åçš„æµ®ç‚¹æ•°ï¼ˆç»Ÿä¸€è½¬æ¢ä¸ºä¸‡å•ä½ï¼‰
        """
        try:
            # ç§»é™¤ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
            text = text.strip()

            # åŒ¹é…æ•°å­—éƒ¨åˆ†ï¼ˆæ”¯æŒå°æ•°ï¼‰
            num_match = re.search(r"[\d\.]+", text)
            if not num_match:
                return 0.0

            num = float(num_match.group())

            # æ ¹æ®å•ä½è¿›è¡Œæ¢ç®—ï¼ˆç»Ÿä¸€è½¬æ¢ä¸ºä¸‡å•ä½ï¼‰
            if "äº¿" in text:
                num *= 10000  # äº¿è½¬æ¢ä¸ºä¸‡
            elif "ä¸‡" in text:
                pass  # å·²ç»æ˜¯ä¸‡å•ä½
            else:
                # æ²¡æœ‰å•ä½çš„æ•°å­—ï¼Œé™¤ä»¥10000è½¬æ¢ä¸ºä¸‡å•ä½
                # ä¾‹å¦‚ "8210" -> 0.821ä¸‡
                num /= 10000

            return round(num, 2)
        except Exception as e:
            self.logger.warning(f"è§£ææ•°å­—å¤±è´¥: {text}, é”™è¯¯: {e}")
            return 0.0

    def _parse_rank(self, h2_tag) -> Tuple[int, str]:
        """
        ä»h2æ ‡ç­¾ä¸­è§£ææ’åå’Œæ ‡é¢˜

        å‚æ•°ï¼š
            h2_tag: BeautifulSoupçš„h2æ ‡ç­¾å¯¹è±¡

        è¿”å›ï¼š
            (æ’å, æ ‡é¢˜) å…ƒç»„
        """
        try:
            # å¤„ç†å¯èƒ½çš„ä¸¤ç§æ ¼å¼ï¼š
            # æ ¼å¼1: <h2 class="text-xl">ç¬¬1åï¼šæé“è¢«åˆ¤20å¹´</h2>
            # æ ¼å¼2: <h2 class="text-xl"><span class="sr-only">ç¬¬<!-- -->1<!-- -->åï¼š</span>åŒè½¨ç©ºé™</h2>

            # è·å–å®Œæ•´çš„æ–‡æœ¬ï¼ˆåŒ…æ‹¬spanå†…çš„æ–‡æœ¬ï¼‰
            full_text = h2_tag.get_text(strip=True)

            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ’å
            rank_match = re.search(r"ç¬¬\s*(\d+)\s*å", full_text)
            if rank_match:
                rank = int(rank_match.group(1))

                # æå–æ ‡é¢˜ï¼šç§»é™¤æ’åéƒ¨åˆ†
                # æ³¨æ„ï¼šBeautifulSoupçš„get_text()å·²ç»å¤„ç†äº†HTMLæ³¨é‡Š
                title = re.sub(r"ç¬¬\s*\d+\s*åï¼š?", "", full_text).strip()

                # å¦‚æœæ ‡é¢˜ä¸ºç©ºï¼Œå°è¯•ä»spanåçš„æ–‡æœ¬è·å–
                if not title:
                    # æŸ¥æ‰¾spanæ ‡ç­¾
                    span_tag = h2_tag.find("span", class_="sr-only")
                    if span_tag:
                        # è·å–spanä¹‹åçš„æ‰€æœ‰æ–‡æœ¬
                        span_text = span_tag.get_text(strip=True)
                        title = full_text.replace(span_text, "").strip()

                return rank, title
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ’åæ ¼å¼ï¼Œå°è¯•å…¶ä»–æ ¼å¼
                rank_match = re.search(r"(\d+)", full_text)
                if rank_match:
                    rank = int(rank_match.group(1))
                    # ç§»é™¤æ•°å­—å’Œåˆ†éš”ç¬¦è·å–æ ‡é¢˜
                    title = re.sub(r"^\d+[\.:ï¼š]\s*", "", full_text).strip()
                    return rank, title
                else:
                    # é»˜è®¤æ’åä¸º0
                    return 0, full_text.strip()

        except Exception as e:
            self.logger.warning(f"è§£ææ’åå¤±è´¥: {str(h2_tag)[:50]}..., é”™è¯¯: {e}")
            return 0, h2_tag.get_text(strip=True) if hasattr(
                h2_tag, "get_text"
            ) else str(h2_tag)

    def parse_page(self, html: str, date: str) -> List[HotItem]:
        """
        è§£æHTMLé¡µé¢ï¼Œæå–çƒ­æœæ¡ç›®ä¿¡æ¯

        å‚æ•°ï¼š
            html: é¡µé¢HTMLå­—ç¬¦ä¸²
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸ºYYYY-MM-DD

        è¿”å›ï¼š
            çƒ­æœæ¡ç›®åˆ—è¡¨
        """
        hot_items = []

        try:
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            self.logger.debug(f"HTMLé•¿åº¦: {len(html)}")
            if len(html) < 1000:
                self.logger.warning(f"HTMLå¤ªçŸ­ï¼Œå¯èƒ½æœ‰é—®é¢˜: {html[:500]}...")

            # å°è¯•ä½¿ç”¨lxmlè§£æå™¨ï¼Œå¦‚æœä¸å¯ç”¨åˆ™å›é€€åˆ°html.parser
            try:
                soup = BeautifulSoup(html, "lxml")
                self.logger.debug("ä½¿ç”¨lxmlè§£æå™¨")
            except Exception as e:
                self.logger.warning(f"lxmlè§£æå™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨html.parser: {e}")
                soup = BeautifulSoup(html, "html.parser")
                self.logger.debug("ä½¿ç”¨html.parserè§£æå™¨")

            # æŸ¥æ‰¾æ‰€æœ‰çš„<a>æ ‡ç­¾ï¼Œä½†åªè¿‡æ»¤å‡ºçœŸæ­£çš„çƒ­æœæ¡ç›®
            # çœŸæ­£çš„çƒ­æœæ¡ç›®é€šå¸¸æœ‰ï¼šaria-labelåŒ…å«"æŸ¥çœ‹å¾®åšè¯é¢˜"ï¼Œæˆ–è€…åŒ…å«h2.text-xlå’Œdata div
            all_a_tags = soup.find_all("a")
            hot_a_tags = []

            # è°ƒè¯•ï¼šè®°å½•å‰å‡ ä¸ªaæ ‡ç­¾çš„ç»“æ„
            if len(all_a_tags) == 0:
                self.logger.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•<a>æ ‡ç­¾ï¼Œæ£€æŸ¥HTMLç»“æ„...")
                # æ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰å†…å®¹
                if soup.title:
                    self.logger.debug(f"é¡µé¢æ ‡é¢˜: {soup.title.string}")
                # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–å…ƒç´ 
                h2_tags = soup.find_all("h2")
                self.logger.debug(f"æ‰¾åˆ°{h2_tags}ä¸ªh2æ ‡ç­¾")
            else:
                self.logger.debug(f"æ‰¾åˆ°ç¬¬ä¸€ä¸ªaæ ‡ç­¾é¢„è§ˆ: {str(all_a_tags[0])[:200]}...")

            for a_tag in all_a_tags:
                aria_label = a_tag.get("aria-label", "")
                h2_tag = a_tag.find("h2", class_="text-xl")
                data_container = a_tag.find("div", class_="flex")

                # æ£€æŸ¥æ˜¯å¦ä¸ºçœŸæ­£çš„çƒ­æœæ¡ç›®
                if (("æŸ¥çœ‹å¾®åšè¯é¢˜" in aria_label) and h2_tag and data_container) or (
                    h2_tag and data_container
                ):
                    hot_a_tags.append(a_tag)

            self.logger.info(
                f"æ‰¾åˆ° {len(all_a_tags)} ä¸ª<a>æ ‡ç­¾ï¼Œå…¶ä¸­ {len(hot_a_tags)} ä¸ªæ˜¯çƒ­æœæ¡ç›®"
            )

            for a_tag in hot_a_tags:
                try:
                    # æŸ¥æ‰¾h2æ ‡ç­¾
                    h2_tag = a_tag.find("h2", class_="text-xl")
                    if not h2_tag:
                        continue

                    # è§£ææ’åå’Œæ ‡é¢˜
                    rank, title = self._parse_rank(h2_tag)

                    # æŸ¥æ‰¾åŒ…å«åˆ†ç±»å’Œæ•°æ®çš„divå®¹å™¨
                    data_container = a_tag.find("div", class_="flex")
                    if not data_container:
                        continue

                    # æå–æ‰€æœ‰æ•°æ®æ ‡ç­¾
                    data_tags = data_container.find_all("div", class_="inline-flex")

                    # åˆå§‹åŒ–æ•°æ®å­—æ®µ
                    category = ""
                    heat = 0.0
                    reads = 0.0
                    discussions = 0.0
                    originals = 0.0

                    for data_tag in data_tags:
                        text = data_tag.get_text(strip=True)

                        # æ ¹æ®æ–‡æœ¬å†…å®¹åˆ¤æ–­æ•°æ®ç±»å‹
                        # å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯æ•°æ®å­—æ®µ
                        if "ğŸ”¥" in text:
                            # çƒ­åº¦
                            heat = self._extract_number(text.replace("ğŸ”¥", ""))
                        elif "é˜…è¯»" in text:
                            # é˜…è¯»é‡
                            reads = self._extract_number(text.replace("é˜…è¯»", ""))
                        elif "è®¨è®º" in text:
                            # è®¨è®ºé‡
                            discussions = self._extract_number(text.replace("è®¨è®º", ""))
                        elif "åŸåˆ›" in text:
                            # åŸåˆ›é‡
                            originals = self._extract_number(text.replace("åŸåˆ›", ""))
                        else:
                            # å¤„ç†åˆ†ç±»æ ‡ç­¾ - æ›´å…¨é¢çš„åˆ†ç±»æ£€æµ‹
                            category_keywords = {
                                "æ˜æ˜Ÿ": "æ˜æ˜Ÿ",
                                "ç¤¾ä¼š": "ç¤¾ä¼š",
                                "å¨±ä¹": "å¨±ä¹",
                                "ä½“è‚²": "ä½“è‚²",
                                "ç§‘æŠ€": "ç§‘æŠ€",
                                "æ¸¸æˆ": "æ¸¸æˆ",
                                "ç¾é£Ÿ": "ç¾é£Ÿ",
                                "è´¢ç»": "è´¢ç»",
                                "æ—¶å°š": "æ—¶å°š",
                                "æ•™è‚²": "æ•™è‚²",
                                "å¥åº·": "å¥åº·",
                                "æ—…æ¸¸": "æ—…æ¸¸",
                                "æ±½è½¦": "æ±½è½¦",
                                "åŠ¨æ¼«": "åŠ¨æ¼«",
                                "å†›äº‹": "å†›äº‹",
                                "æ•°ç ": "æ•°ç ",
                                "éŸ³ä¹": "éŸ³ä¹",
                                "ç”µå½±": "ç”µå½±",
                                "ç”µè§†å‰§": "ç”µè§†å‰§",
                                "ç»¼è‰º": "ç»¼è‰º",
                                "æç¬‘": "æç¬‘",
                                "æƒ…æ„Ÿ": "æƒ…æ„Ÿ",
                                "ç”Ÿæ´»": "ç”Ÿæ´»",
                                "å®¶å±…": "å®¶å±…",
                                "è‚²å„¿": "è‚²å„¿",
                                "å® ç‰©": "å® ç‰©",
                                "æ‘„å½±": "æ‘„å½±",
                                "ç»˜ç”»": "ç»˜ç”»",
                                "è¯»ä¹¦": "è¯»ä¹¦",
                                "å†™ä½œ": "å†™ä½œ",
                                "èŒåœº": "èŒåœº",
                                "æ³•å¾‹": "æ³•å¾‹",
                                "æ”¿æ²»": "æ”¿æ²»",
                                "å†å²": "å†å²",
                                "æ–‡åŒ–": "æ–‡åŒ–",
                                "è‰ºæœ¯": "è‰ºæœ¯",
                                "ç§‘å­¦": "ç§‘å­¦",
                                "è‡ªç„¶": "è‡ªç„¶",
                                "ç¯ä¿": "ç¯ä¿",
                                "å…¬ç›Š": "å…¬ç›Š",
                                "å®—æ•™": "å®—æ•™",
                                "å¿ƒç†": "å¿ƒç†",
                                "æ˜Ÿåº§": "æ˜Ÿåº§",
                                "å½©ç¥¨": "å½©ç¥¨",
                                "è‚¡ç¥¨": "è‚¡ç¥¨",
                                "æˆ¿äº§": "æˆ¿äº§",
                                "åˆ›ä¸š": "åˆ›ä¸š",
                                "äº’è”ç½‘": "äº’è”ç½‘",
                                "æ‰‹æœº": "æ‰‹æœº",
                                "ç”µè„‘": "ç”µè„‘",
                                "è½¯ä»¶": "è½¯ä»¶",
                                "ç½‘ç»œ": "ç½‘ç»œ",
                                "ç”µå•†": "ç”µå•†",
                                "ç›´æ’­": "ç›´æ’­",
                                "ç½‘çº¢": "ç½‘çº¢",
                                "ç¾å¦†": "ç¾å¦†",
                                "æœé¥°": "æœé¥°",
                                "é‹åŒ…": "é‹åŒ…",
                                "ç å®": "ç å®",
                                "æ‰‹è¡¨": "æ‰‹è¡¨",
                                "å®¶å…·": "å®¶å…·",
                                "å®¶ç”µ": "å®¶ç”µ",
                                "å¨å…·": "å¨å…·",
                                "é£Ÿå“": "é£Ÿå“",
                                "é¥®æ–™": "é¥®æ–™",
                                "é…’æ°´": "é…’æ°´",
                                "çƒŸè‰": "çƒŸè‰",
                                "è¯å“": "è¯å“",
                                "åŒ»ç–—": "åŒ»ç–—",
                                "åŒ»é™¢": "åŒ»é™¢",
                                "å­¦æ ¡": "å­¦æ ¡",
                                "æ•™è‚²æœºæ„": "æ•™è‚²æœºæ„",
                                "å…¬å¸": "å…¬å¸",
                                "å·¥å‚": "å·¥å‚",
                                "å†œæ‘": "å†œæ‘",
                                "åŸå¸‚": "åŸå¸‚",
                                "äº¤é€š": "äº¤é€š",
                                "èˆªç©º": "èˆªç©º",
                                "é“è·¯": "é“è·¯",
                                "å…¬è·¯": "å…¬è·¯",
                                "æµ·è¿": "æµ·è¿",
                                "å¤©æ°”": "å¤©æ°”",
                                "åœ°éœ‡": "åœ°éœ‡",
                                "å°é£": "å°é£",
                                "æ´ªæ°´": "æ´ªæ°´",
                                "ç«ç¾": "ç«ç¾",
                                "äº‹æ•…": "äº‹æ•…",
                                "çŠ¯ç½ª": "çŠ¯ç½ª",
                                "è­¦å¯Ÿ": "è­¦å¯Ÿ",
                                "æ³•é™¢": "æ³•é™¢",
                                "ç›‘ç‹±": "ç›‘ç‹±",
                                "æ­»äº¡": "æ­»äº¡",
                                "å‡ºç”Ÿ": "å‡ºç”Ÿ",
                                "ç»“å©š": "ç»“å©š",
                                "ç¦»å©š": "ç¦»å©š",
                                "æ‹çˆ±": "æ‹çˆ±",
                                "åˆ†æ‰‹": "åˆ†æ‰‹",
                                "æ±‚å©š": "æ±‚å©š",
                                "å©šç¤¼": "å©šç¤¼",
                                "ç”Ÿæ—¥": "ç”Ÿæ—¥",
                                "èŠ‚æ—¥": "èŠ‚æ—¥",
                                "æ˜¥èŠ‚": "æ˜¥èŠ‚",
                                "ä¸­ç§‹": "ä¸­ç§‹",
                                "ç«¯åˆ": "ç«¯åˆ",
                                "æ¸…æ˜": "æ¸…æ˜",
                                "å›½åº†": "å›½åº†",
                                "å…ƒæ—¦": "å…ƒæ—¦",
                                "åœ£è¯": "åœ£è¯",
                            }

                            # æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«ä»»ä½•åˆ†ç±»å…³é”®è¯
                            for keyword, cat in category_keywords.items():
                                if keyword in text:
                                    category = cat
                                    break

                    # åˆ›å»ºHotItemå¯¹è±¡ï¼ˆç¡®ä¿æœ‰æœ‰æ•ˆæ•°æ®ï¼‰
                    if rank > 0 and title:  # åŸºæœ¬éªŒè¯
                        hot_item = HotItem(
                            rank=rank,
                            title=title,
                            category=category,
                            heat=heat,
                            reads=reads,
                            discussions=discussions,
                            originals=originals,
                            date=date,
                        )
                        hot_items.append(hot_item)
                    else:
                        self.logger.debug(f"è·³è¿‡æ— æ•ˆæ¡ç›®: rank={rank}, title={title}")

                except Exception as e:
                    self.logger.warning(f"è§£æå•ä¸ªçƒ­æœæ¡ç›®å¤±è´¥: {e}")
                    continue

            # æŒ‰æ’åæ’åº
            hot_items.sort(key=lambda x: x.rank)

            self.logger.info(f"æˆåŠŸè§£æ {len(hot_items)} ä¸ªçƒ­æœæ¡ç›®")

            # å¦‚æœæ²¡æœ‰è§£æåˆ°ä»»ä½•æ¡ç›®ï¼Œå¯èƒ½æ˜¯é¡µé¢ç»“æ„ä¸åŒï¼Œå°è¯•å¤‡ç”¨è§£ææ–¹æ³•
            if len(hot_items) == 0:
                self.logger.warning("ä½¿ç”¨ä¸»è§£ææ–¹æ³•æœªæ‰¾åˆ°æ•°æ®ï¼Œå°è¯•å¤‡ç”¨è§£ææ–¹æ³•...")
                hot_items = self._parse_page_backup(soup, date)

        except Exception as e:
            self.logger.error(f"è§£æé¡µé¢å¤±è´¥: {e}")

        return hot_items

    def save_data(self, data: List[HotItem], date: str) -> bool:
        """
        ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶

        å‚æ•°ï¼š
            data: çƒ­æœæ¡ç›®åˆ—è¡¨
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸ºYYYY-MM-DD

        è¿”å›ï¼š
            ä¿å­˜æˆåŠŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        try:
            # è§£ææ—¥æœŸï¼Œåˆ›å»ºæœˆä»½ç›®å½•
            date_obj = datetime.strptime(date, "%Y-%m-%d")
            year_month = date_obj.strftime("%Y-%m")

            # åˆ›å»ºæœˆä»½ç›®å½•
            month_dir = os.path.join(self.output_dir, year_month)
            os.makedirs(month_dir, exist_ok=True)

            # æ„å»ºæ–‡ä»¶è·¯å¾„
            filename = f"{date}.json"
            filepath = os.path.join(month_dir, filename)

            # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            data_dicts = [asdict(item) for item in data]

            # ä¿å­˜ä¸ºJSONæ–‡ä»¶
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(
                    {"date": date, "count": len(data), "data": data_dicts},
                    f,
                    ensure_ascii=False,
                    indent=2,
                )

            self.logger.info(f"æ•°æ®å·²ä¿å­˜åˆ° {filepath}")
            return True

        except Exception as e:
            self.logger.error(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")
            return False

    def scrape_date_with_html(self, date: str, html: str) -> bool:
        """
        ä½¿ç”¨æä¾›çš„HTMLçˆ¬å–å¹¶ä¿å­˜æŒ‡å®šæ—¥æœŸçš„æ•°æ®ï¼ˆç”¨äºæµ‹è¯•ï¼‰

        å‚æ•°ï¼š
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸ºYYYY-MM-DD
            html: é¡µé¢HTMLå­—ç¬¦ä¸²

        è¿”å›ï¼š
            çˆ¬å–æˆåŠŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        self.logger.info(f"å¼€å§‹è§£æ {date} çš„æ•°æ®...")

        # è§£æé¡µé¢
        hot_items = self.parse_page(html, date)
        if not hot_items:
            self.logger.warning(f"{date} æ²¡æœ‰è§£æåˆ°çƒ­æœæ•°æ®")

        # ä¿å­˜æ•°æ®
        success = self.save_data(hot_items, date)

        if success:
            self.logger.info(f"æˆåŠŸå®Œæˆ {date} çš„æ•°æ®è§£æ")
        else:
            self.logger.error(f"{date} çš„æ•°æ®è§£æå¤±è´¥")

        return success

    def scrape_date(self, date: str) -> bool:
        """
        çˆ¬å–å¹¶ä¿å­˜æŒ‡å®šæ—¥æœŸçš„æ•°æ®

        å‚æ•°ï¼š
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸ºYYYY-MM-DD

        è¿”å›ï¼š
            çˆ¬å–æˆåŠŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        self.logger.info(f"å¼€å§‹çˆ¬å– {date} çš„æ•°æ®...")

        # è·å–é¡µé¢HTML
        html = self.fetch_page(date)
        if not html:
            self.logger.error(f"æ— æ³•è·å– {date} çš„é¡µé¢")
            return False

        # ç­‰å¾…å»¶è¿Ÿ
        time.sleep(self.delay)

        # è§£æé¡µé¢
        hot_items = self.parse_page(html, date)
        if not hot_items:
            self.logger.warning(f"{date} æ²¡æœ‰è§£æåˆ°çƒ­æœæ•°æ®")
            # ä»ç„¶å°è¯•ä¿å­˜ç©ºæ•°æ®ä»¥è®°å½•è¯¥æ—¥æœŸå·²è¢«å¤„ç†

        # ä¿å­˜æ•°æ®
        success = self.save_data(hot_items, date)

        if success:
            self.logger.info(f"æˆåŠŸå®Œæˆ {date} çš„æ•°æ®çˆ¬å–")
        else:
            self.logger.error(f"{date} çš„æ•°æ®çˆ¬å–å¤±è´¥")

        return success

    def scrape_range(self, start_date: str, end_date: str) -> Dict[str, Any]:
        """
        çˆ¬å–æŒ‡å®šæ—¥æœŸèŒƒå›´çš„æ•°æ®

        å‚æ•°ï¼š
            start_date: å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYY-MM-DD
            end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYY-MM-DD

        è¿”å›ï¼š
            åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        stats = {"total_dates": 0, "successful": 0, "failed": 0, "failed_dates": []}

        try:
            # è§£ææ—¥æœŸèŒƒå›´
            start = datetime.strptime(start_date, "%Y-%m-%d")
            end = datetime.strptime(end_date, "%Y-%m-%d")

            # ç”Ÿæˆæ—¥æœŸåˆ—è¡¨
            current = start
            date_list = []

            while current <= end:
                date_list.append(current.strftime("%Y-%m-%d"))
                current += timedelta(days=1)

            stats["total_dates"] = len(date_list)
            self.logger.info(f"éœ€è¦çˆ¬å– {len(date_list)} å¤©çš„æ•°æ®")

            # éå†æ—¥æœŸå¹¶çˆ¬å–
            for i, date in enumerate(date_list, 1):
                self.logger.info(f"è¿›åº¦: {i}/{len(date_list)} ({date})")

                try:
                    success = self.scrape_date(date)

                    if success:
                        stats["successful"] += 1
                    else:
                        stats["failed"] += 1
                        stats["failed_dates"].append(date)

                except Exception as e:
                    self.logger.error(f"çˆ¬å– {date} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
                    stats["failed"] += 1
                    stats["failed_dates"].append(date)

                # åœ¨æ—¥æœŸä¹‹é—´æ·»åŠ é¢å¤–å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                if i < len(date_list):
                    time.sleep(self.delay * 0.5)

            # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
            self.logger.info(
                f"çˆ¬å–å®Œæˆï¼æˆåŠŸ: {stats['successful']}, å¤±è´¥: {stats['failed']}"
            )
            if stats["failed_dates"]:
                self.logger.warning(f"å¤±è´¥çš„æ—¥æœŸ: {stats['failed_dates']}")

        except ValueError as e:
            self.logger.error(f"æ—¥æœŸæ ¼å¼é”™è¯¯: {e}")
        except Exception as e:
            self.logger.error(f"çˆ¬å–èŒƒå›´æ•°æ®æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

        return stats

    def _parse_page_backup(self, soup: BeautifulSoup, date: str) -> List[HotItem]:
        """
        å¤‡ç”¨è§£ææ–¹æ³•ï¼Œç”¨äºå¤„ç†ä¸åŒçš„é¡µé¢ç»“æ„

        å‚æ•°ï¼š
            soup: BeautifulSoupå¯¹è±¡
            date: æ—¥æœŸå­—ç¬¦ä¸²

        è¿”å›ï¼š
            çƒ­æœæ¡ç›®åˆ—è¡¨
        """
        hot_items = []

        try:
            # å¤‡ç”¨æ–¹æ³•ï¼šæŸ¥æ‰¾æ‰€æœ‰åŒ…å«çƒ­æœæ•°æ®çš„å®¹å™¨
            # æœ‰äº›é¡µé¢å¯èƒ½æœ‰ä¸åŒçš„ç»“æ„
            containers = soup.find_all("div", class_="rounded-lg")

            for container in containers:
                try:
                    # å°è¯•ä¸åŒçš„é€‰æ‹©å™¨ç»„åˆ
                    h2_tag = container.find("h2", class_="text-xl")
                    if not h2_tag:
                        continue

                    rank, title = self._parse_rank(h2_tag)

                    # æŸ¥æ‰¾æ•°æ®
                    data_div = container.find("div", class_="flex")
                    if not data_div:
                        continue

                    data_tags = data_div.find_all("div", class_="inline-flex")

                    # åˆå§‹åŒ–æ•°æ®å­—æ®µ
                    category = ""
                    heat = 0.0
                    reads = 0.0
                    discussions = 0.0
                    originals = 0.0

                    for data_tag in data_tags:
                        text = data_tag.get_text(strip=True)

                        if (
                            "æ˜æ˜Ÿ" in text
                            or "ç¤¾ä¼š" in text
                            or "å¨±ä¹" in text
                            or "ä½“è‚²" in text
                        ):
                            category = text
                        elif "ğŸ”¥" in text:
                            heat = self._extract_number(text.replace("ğŸ”¥", ""))
                        elif "é˜…è¯»" in text:
                            reads = self._extract_number(text.replace("é˜…è¯»", ""))
                        elif "è®¨è®º" in text:
                            discussions = self._extract_number(text.replace("è®¨è®º", ""))
                        elif "åŸåˆ›" in text:
                            originals = self._extract_number(text.replace("åŸåˆ›", ""))
                        elif "æ¸¸æˆ" in text:
                            category = "æ¸¸æˆ"
                        elif "ç¾é£Ÿ" in text:
                            category = "ç¾é£Ÿ"
                        elif "è´¢ç»" in text:
                            category = "è´¢ç»"

                    if rank > 0 and title:
                        hot_item = HotItem(
                            rank=rank,
                            title=title,
                            category=category,
                            heat=heat,
                            reads=reads,
                            discussions=discussions,
                            originals=originals,
                            date=date,
                        )
                        hot_items.append(hot_item)

                except Exception as e:
                    self.logger.debug(f"å¤‡ç”¨è§£ææ–¹æ³•å¤„ç†å•ä¸ªæ¡ç›®å¤±è´¥: {e}")
                    continue

            self.logger.info(f"å¤‡ç”¨æ–¹æ³•è§£æåˆ° {len(hot_items)} ä¸ªçƒ­æœæ¡ç›®")

        except Exception as e:
            self.logger.error(f"å¤‡ç”¨è§£ææ–¹æ³•å¤±è´¥: {e}")

        return hot_items

    def test_parse(self, date: str = "2024-12-13") -> None:
        """
        æµ‹è¯•è§£æåŠŸèƒ½

        å‚æ•°ï¼š
            date: æµ‹è¯•æ—¥æœŸ
        """
        print(f"æµ‹è¯•è§£æåŠŸèƒ½ - æ—¥æœŸ: {date}")
        print("=" * 60)

        # è·å–é¡µé¢HTML
        html = self.fetch_page(date)
        if not html:
            print(f"æ— æ³•è·å– {date} çš„é¡µé¢")
            return

        # è§£æé¡µé¢
        hot_items = self.parse_page(html, date)

        # æ˜¾ç¤ºå‰5ä¸ªç»“æœ
        print(f"å…±è§£æåˆ° {len(hot_items)} ä¸ªçƒ­æœæ¡ç›®")
        print("\nå‰5ä¸ªçƒ­æœæ¡ç›®:")
        print("-" * 60)

        for i, item in enumerate(hot_items[:5]):
            print(f"{i + 1}. æ’å: {item.rank}")
            print(f"   æ ‡é¢˜: {item.title}")
            print(f"   åˆ†ç±»: {item.category}")
            print(f"   çƒ­åº¦: {item.heat}ä¸‡")
            print(f"   é˜…è¯»: {item.reads}ä¸‡")
            print(f"   è®¨è®º: {item.discussions}ä¸‡")
            print(f"   åŸåˆ›: {item.originals}ä¸‡")
            print()

        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        if hot_items:
            ranks = [item.rank for item in hot_items]
            print(f"æ’åèŒƒå›´: {min(ranks)} - {max(ranks)}")
            print(
                f"å¹³å‡çƒ­åº¦: {sum(item.heat for item in hot_items) / len(hot_items):.2f}ä¸‡"
            )
            print(f"æ€»é˜…è¯»é‡: {sum(item.reads for item in hot_items):.2f}ä¸‡")
            print(f"æ€»è®¨è®ºé‡: {sum(item.discussions for item in hot_items):.2f}ä¸‡")

        print("=" * 60)


def main():
    """
    ä¸»å‡½æ•°ï¼Œç”¨äºæµ‹è¯•å’Œæ¼”ç¤ºçˆ¬è™«åŠŸèƒ½

    åŠŸèƒ½ï¼š
        1. åˆ›å»ºçˆ¬è™«å®ä¾‹
        2. çˆ¬å–æŒ‡å®šæ—¥æœŸèŒƒå›´çš„å¾®åšçƒ­æœæ•°æ®
        3. å°†æ•°æ®ä¿å­˜åˆ°dataç›®å½•
    """
    print("å¾®åšçƒ­æœå†å²æ•°æ®çˆ¬è™«")
    print("=" * 60)
    print("ä½¿ç”¨æ–¹æ³•:")
    print("  1. python src/scrap.py                # è¿è¡Œå®Œæ•´çˆ¬å–")
    print("  2. python src/scrap.py test          # æµ‹è¯•è§£æåŠŸèƒ½")
    print("  3. python src/scrap.py single <date> # çˆ¬å–å•æ—¥æ•°æ®")
    print("=" * 60)

    import sys

    try:
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        scraper = WeiboHotScraper(
            output_dir="data",
            delay=2.0,  # é€‚å½“å»¶è¿Ÿï¼Œé¿å…è¢«å°IP
            max_retries=3,
        )

        # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
        if len(sys.argv) > 1:
            if sys.argv[1] == "test":
                # æµ‹è¯•æ¨¡å¼
                print("è¿è¡Œæµ‹è¯•æ¨¡å¼...")
                test_date = sys.argv[2] if len(sys.argv) > 2 else "2024-12-13"
                scraper.test_parse(test_date)
                return
            elif sys.argv[1] == "single" and len(sys.argv) > 2:
                # å•æ—¥çˆ¬å–æ¨¡å¼
                date = sys.argv[2]
                print(f"çˆ¬å–å•æ—¥æ•°æ®: {date}")
                success = scraper.scrape_date(date)
                if success:
                    print(f"æˆåŠŸçˆ¬å– {date} çš„æ•°æ®")
                else:
                    print(f"çˆ¬å– {date} çš„æ•°æ®å¤±è´¥")
                return
            elif sys.argv[1] == "help":
                print("å¸®åŠ©ä¿¡æ¯:")
                print("  test [date]    - æµ‹è¯•è§£æåŠŸèƒ½ï¼Œå¯é€‰æ—¥æœŸå‚æ•°")
                print("  single <date>  - çˆ¬å–å•æ—¥æ•°æ®")
                print("  help          - æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯")
                return

        # é»˜è®¤æ¨¡å¼ï¼šå®Œæ•´çˆ¬å–
        # è®¾ç½®æ—¥æœŸèŒƒå›´ï¼ˆæ ¹æ®ç”¨æˆ·è¦æ±‚ï¼‰
        start_date = "2025-01-01"
        end_date = "2025-12-12"

        print(f"å¼€å§‹çˆ¬å– {start_date} åˆ° {end_date} çš„æ•°æ®...")
        print("æ•°æ®å°†ä¿å­˜åˆ° data/ ç›®å½•ä¸‹ï¼ŒæŒ‰æœˆä»½ç»„ç»‡")
        print("æ³¨æ„ï¼šè¿™éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
        print("=" * 60)

        # å¼€å§‹çˆ¬å–
        stats = scraper.scrape_range(start_date, end_date)

        # è¾“å‡ºç»“æœ
        print("\nçˆ¬å–å®Œæˆï¼")
        print(f"æ€»å¤©æ•°: {stats['total_dates']}")
        print(f"æˆåŠŸ: {stats['successful']}")
        print(f"å¤±è´¥: {stats['failed']}")

        if stats["failed_dates"]:
            print(f"å¤±è´¥çš„æ—¥æœŸ: {', '.join(stats['failed_dates'][:5])}")
            if len(stats["failed_dates"]) > 5:
                print(f"... ä»¥åŠ {len(stats['failed_dates']) - 5} ä¸ªæ›´å¤š")

        print(f"\næ•°æ®å·²ä¿å­˜åˆ° {scraper.output_dir}/ ç›®å½•")

    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­ï¼Œçˆ¬è™«å·²åœæ­¢")
    except Exception as e:
        print(f"ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        import traceback

        traceback.print_exc()


class RealtimeHotScraper:
    """
    å¾®åšå®æ—¶çƒ­æœçˆ¬è™«

    åŠŸèƒ½ï¼š
        - çˆ¬å–å¾®åšå®æ—¶çƒ­æœ Top 50
        - è§£ææ’åã€æ ‡é¢˜ã€çƒ­åº¦
        - æ”¯æŒç¼“å­˜å’Œé”™è¯¯å¤„ç†

    æ•°æ®ç»“æ„ï¼š
        {
            "rank": 3,
            "title": "æ­å¼€æ—¥æœ¬ä¼ªè£…å—å®³è€…çš„çœŸé¢ç›®"
        }
    """

    def __init__(
        self,
        timeout: int = 10,
        max_retries: int = 3,
        delay: float = 1.0,
    ):
        """
        åˆå§‹åŒ–å®æ—¶çƒ­æœçˆ¬è™«

        å‚æ•°ï¼š
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            delay: è¯·æ±‚é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰
        """
        # é»˜è®¤ä½¿ç”¨æ¦œå•é¡µï¼ˆæ›´ç¨³å®šã€æ— éœ€ç™»å½•ï¼‰
        self.base_url = "https://s.weibo.com/top/summary?cate=realtimehot"
        self.timeout = timeout
        self.max_retries = max_retries
        self.delay = delay

        # é…ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        )
        self.logger = logging.getLogger(__name__)

        # é…ç½®requestsä¼šè¯
        self.session = requests.Session()
        # æ›´å®Œæ•´çš„ User-Agent åˆ—è¡¨ï¼ˆæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨ï¼‰
        user_agents = [
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        ]
        import random
        chosen_ua = random.choice(user_agents)
        
        self.session.headers.update(
            {
                "User-Agent": chosen_ua,
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                "Accept-Encoding": "gzip, deflate, br",
                "DNT": "1",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
                "Referer": "https://weibo.com/",
            }
        )
        
        # æ·»åŠ cookieæ¥æ¨¡æ‹Ÿå·²è®¿é—®ç”¨æˆ·ï¼ˆå¯é€‰ï¼Œä½†æœ‰åŠ©äºç»•è¿‡æŸäº›é™åˆ¶ï¼‰
        self.session.cookies.update({
            "login_guide": "0",
        })

    def fetch_realtime_page(self) -> Optional[str]:
        """
        è·å–å¾®åšå®æ—¶çƒ­æœé¡µé¢ HTML

        å°è¯•å¤šä¸ªç­–ç•¥ï¼š
        1. æ–°çš„çƒ­æœä¸“ç”¨é¡µé¢ï¼ˆhttps://weibo.com/hot/searchï¼‰
        2. ç›´æ¥è®¿é—®å¾®åšé¦–é¡µï¼ˆhttps://weibo.com/ï¼‰
        3. æœç´¢é¡µé¢URL

        è¿”å›ï¼š
            HTML å­—ç¬¦ä¸²æˆ– Noneï¼ˆå¦‚æœè¯·æ±‚å¤±è´¥ï¼‰
        """
        # å°è¯•çš„ URL åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
        urls_to_try = [
            # ç­–ç•¥1ï¼šæ–°çš„æ¦œå•é¡µï¼ˆå®æ—¶çƒ­æœï¼‰ï¼šæ›´ç¨³å®š
            "https://s.weibo.com/top/summary?cate=realtimehot",
            # ç­–ç•¥2ï¼šçƒ­æœé¡µé¢
            "https://weibo.com/hot/search",
            # ç­–ç•¥3ï¼šå¾®åšé¦–é¡µ
            "https://weibo.com/",
            # ç­–ç•¥4ï¼šæœç´¢é¦–é¡µ
            "https://weibo.com/search/index",
        ]

        for url_idx, url in enumerate(urls_to_try, 1):
            for attempt in range(self.max_retries):
                try:
                    self.logger.info(f"[ç­–ç•¥ {url_idx}] æ­£åœ¨è·å–å®æ—¶çƒ­æœé¡µé¢ (å°è¯• {attempt + 1}/{self.max_retries})")
                    self.logger.debug(f"ç›®æ ‡ URL: {url}")
                    
                    # allow_redirects=True ç¡®ä¿è·Ÿéšé‡å®šå‘é“¾
                    response = self.session.get(
                        url, 
                        timeout=self.timeout, 
                        allow_redirects=True,
                        verify=True
                    )
                    response.raise_for_status()

                    if response.status_code == 200 and len(response.text) > 5000:
                        self.logger.info(f"[ç­–ç•¥ {url_idx}] æˆåŠŸè·å–é¡µé¢ (æœ€ç»ˆ URL: {response.url}, å¤§å°: {len(response.text)} bytes)")
                        
                        # æ£€æŸ¥é¡µé¢ä¸­æ˜¯å¦åŒ…å«çƒ­æœç›¸å…³å†…å®¹
                        html_text = response.text.lower()
                        if (
                            "realtimehot" in response.url.lower()
                            or "summary" in response.url.lower()
                            or "çƒ­æœ" in html_text
                            or "å®æ—¶" in html_text
                            or "weibo" in html_text
                        ):
                            self.logger.info(f"[ç­–ç•¥ {url_idx}] é¡µé¢åŒ…å«çƒ­æœç›¸å…³å†…å®¹ï¼Œè¿”å›")
                            return response.text
                        else:
                            self.logger.warning(f"[ç­–ç•¥ {url_idx}] é¡µé¢å†…å®¹ä¸åŒ…å«çƒ­æœæ•°æ®ï¼Œå°è¯•ä¸‹ä¸€ä¸ªç­–ç•¥")
                    else:
                        self.logger.warning(f"[ç­–ç•¥ {url_idx}] é¡µé¢å“åº”ä¸å®Œæ•´: {response.status_code}, å¤§å°: {len(response.text)} bytes")

                except requests.exceptions.RequestException as e:
                    self.logger.error(f"[ç­–ç•¥ {url_idx}] è·å–é¡µé¢å¤±è´¥: {e}")
                    if attempt < self.max_retries - 1:
                        wait_time = 2 ** attempt
                        self.logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                    
                except Exception as e:
                    self.logger.error(f"[ç­–ç•¥ {url_idx}] è·å–é¡µé¢æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
                    break
        
        self.logger.error("æ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥äº†ï¼Œæ— æ³•è·å–é¡µé¢")
        return None

    def parse_realtime_page(self, html: str) -> List[Dict[str, Any]]:
        """
        è§£æå®æ—¶çƒ­æœé¡µé¢ï¼ˆhttps://weibo.com/hot/searchï¼‰

        æ”¯æŒå¤šç§çƒ­æœé¡¹ç»“æ„ï¼Œä¼˜å…ˆçº§ï¼š
        1. ALink_none_1w6rm é“¾æ¥ï¼ˆä¸»è¦çƒ­æœï¼‰
        2. æ­£åˆ™åŒ¹é…çš„æœç´¢é“¾æ¥ï¼ˆæ¬¡è¦çƒ­æœï¼‰
        3. å…¶ä»–å¯èƒ½çš„å®¹å™¨

        å‚æ•°ï¼š
            html: é¡µé¢ HTML å­—ç¬¦ä¸²

        è¿”å›ï¼š
            çƒ­æœæ¡ç›®åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« rankã€title
        """
        hot_items = []

        try:
            soup = BeautifulSoup(html, "html.parser")

            # è°ƒè¯•ï¼šæ£€æŸ¥ HTML ç»“æ„
            self.logger.debug(f"HTML é•¿åº¦: {len(html)}")
            
            # æ–¹æ¡ˆ1ï¼šè§£ææ¦œå•è¡¨æ ¼ï¼ˆs.weibo.com/top/summary é¡µé¢ç»“æ„ï¼‰
            table_container = soup.find(id=re.compile(r"pl_top_realtimehot")) or soup
            rows = []
            # ä¸¤ç§å¸¸è§ç»“æ„ï¼štbody > tr æˆ–è€…ç›´æ¥å¤šä¸ª tr
            tbody = table_container.find("tbody")
            if tbody:
                rows = tbody.find_all("tr")
            else:
                rows = table_container.find_all("tr")

            self.logger.info(f"æ–¹æ¡ˆ1: è¡¨æ ¼è¡Œæ•°é‡ {len(rows)}")

            rank_counter = 0
            for tr in rows:
                try:
                    title_td = tr.find("td", class_=re.compile(r"td-02"))

                    if not title_td:
                        continue

                    # è·³è¿‡è¡¨å¤´æˆ–ç½®é¡¶ï¼ˆæ— æœ‰æ•ˆæ’åï¼‰
                    tr_class = (tr.get("class") or [])
                    if any(cls for cls in tr_class if re.search(r"thead|top|ad|banner", cls)):
                        continue

                    a = title_td.find("a")
                    if not a:
                        continue

                    title = a.get_text(strip=True).replace("#", "").strip()
                    if not title:
                        continue

                    rank_counter += 1

                    hot_items.append({"rank": rank_counter, "title": title})
                except Exception as e:
                    self.logger.debug(f"è§£æè¡¨æ ¼è¡Œå¤±è´¥: {e}")
                    continue

            # æ–¹æ¡ˆ2ï¼šå¦‚æœä¸Šé¢ä¸è¶³ï¼Œå…œåº•è§£æå…¶å®ƒå¯èƒ½ç»“æ„ï¼ˆå¦‚å¤‡ç”¨é“¾æ¥ï¼‰
            if len(hot_items) < 20:
                self.logger.info("ä¸»è¡¨æ ¼ä¸è¶³ï¼Œå°è¯•å…œåº•è§£æå€™é€‰é“¾æ¥â€¦")
                import re as _re
                extra_links = soup.find_all("a", href=_re.compile(r"s\.weibo\.com/weibo"))
                seen = {i["title"] for i in hot_items}
                for lk in extra_links:
                    try:
                        t = lk.get_text(strip=True).replace("#", "").strip()
                        if not t or t in seen:
                            continue
                        if len(t) < 2 or len(t) > 100:
                            continue
                        hot_items.append({"rank": len(hot_items) + 1, "title": t})
                        seen.add(t)
                        if len(hot_items) >= 50:
                            break
                    except Exception:
                        continue

            # æŒ‰æ’åæ’åºï¼ˆç¡®ä¿é¡ºåºä¸é¡µé¢ä¸€è‡´ï¼‰
            if hot_items:
                hot_items.sort(key=lambda x: (isinstance(x["rank"], int), x["rank"]))

            # å»é‡å’Œæ¸…ç†ï¼ˆç§»é™¤é‡å¤æˆ–ä½è´¨é‡çš„é¡¹ï¼‰ï¼Œå¹¶é‡æ–°è¿ç»­ç¼–å·
            cleaned_items = []
            seen_titles = set()
            for item in hot_items:
                title = item["title"]

                digit_count = sum(1 for c in title if c.isdigit())
                if digit_count > len(title) / 3:
                    continue

                if title in seen_titles:
                    continue

                cleaned_items.append(item)
                seen_titles.add(title)

            # é‡æ–°æŒ‰ç…§å‡ºç°é¡ºåºè¿ç»­èµ‹ rankï¼Œé¿å…è·³å·
            for idx, item in enumerate(cleaned_items, start=1):
                item["rank"] = idx

            hot_items = cleaned_items

            self.logger.info(f"æˆåŠŸè§£æ {len(hot_items)} ä¸ªçƒ­æœæ¡ç›®")

        except Exception as e:
            self.logger.error(f"è§£æé¡µé¢å¤±è´¥: {e}")

        return hot_items

        return hot_items

    def _extract_heat(self, container) -> str:
        """
        ä»å®¹å™¨ä¸­æå–çƒ­åº¦ä¿¡æ¯

        å‚æ•°ï¼š
            container: BeautifulSoup çš„ div å®¹å™¨å¯¹è±¡

        è¿”å›ï¼š
            çƒ­åº¦å­—ç¬¦ä¸²ï¼ˆå¦‚ "æ²¸"ã€"çˆ†"ã€æ•°å­—ç­‰ï¼‰
        """
        # å°è¯•ä»ä¸åŒä½ç½®æŸ¥æ‰¾çƒ­åº¦æ ‡ç­¾
        # æ ¹æ®å®é™…é¡µé¢ç»“æ„è°ƒæ•´
        heat_spans = container.find_all("span", class_=lambda x: x and "Heat" in x if x else False)

        if heat_spans:
            return heat_spans[0].get_text(strip=True)

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾åŒ…å«çƒ­åº¦çš„å…¶ä»–å…ƒç´ 
        # è¿™é‡Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥è°ƒæ•´
        return "N/A"

    def fetch_realtime_top50(self) -> List[Dict[str, Any]]:
        """
        è·å–å®æ—¶çƒ­æœ Top 50ï¼ˆä½¿ç”¨ Playwright æµè§ˆå™¨è‡ªåŠ¨åŒ–ï¼‰

        è¿”å›ï¼š
            çƒ­æœæ¡ç›®åˆ—è¡¨ï¼ˆæœ€å¤š 50 é¡¹ï¼‰
        """
        # ä¼˜å…ˆä½¿ç”¨ Playwright æ–¹æ³•ï¼ˆæ›´å¯é ï¼‰
        return self.fetch_realtime_top50_with_playwright()

    def fetch_realtime_top50_with_playwright(self) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨ Playwright è·å–å®æ—¶çƒ­æœ Top 50ï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰

        ç›¸æ¯”äºç›´æ¥ HTTP è¯·æ±‚ï¼Œè¿™ä¸ªæ–¹æ³•ï¼š
        1. ä½¿ç”¨çœŸå®æµè§ˆå™¨å¼•æ“ï¼ˆChromiumï¼‰
        2. æ‰§è¡Œ JavaScript ä»£ç ï¼ŒåŠ è½½åŠ¨æ€å†…å®¹
        3. ç»•è¿‡è®¿å®¢éªŒè¯ç³»ç»Ÿ
        4. é€šè¿‡å¤šæ¬¡æ»šåŠ¨åŠ è½½æ›´å¤šçƒ­æœé¡¹
        5. æ›´æ¥è¿‘çœŸå®ç”¨æˆ·è¡Œä¸º

        è¿”å›ï¼š
            çƒ­æœæ¡ç›®åˆ—è¡¨ï¼ˆæœ€å¤š 50 é¡¹ï¼‰

        ä¾èµ–ï¼š
            playwright>=1.40.0ï¼ˆéœ€è¦å…ˆè¿è¡Œï¼špython -m playwright install chromiumï¼‰
        """
        try:
            from playwright.sync_api import sync_playwright
        except ImportError:
            self.logger.error(
                "Playwright æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install playwright && python -m playwright install chromium"
            )
            return []

        html_content = None

        try:
            self.logger.info("æ­£åœ¨ä½¿ç”¨ Playwright åŠ è½½é¡µé¢...")
            
            with sync_playwright() as p:
                # å¯åŠ¨ Chromium æµè§ˆå™¨
                browser = p.chromium.launch(headless=True)
                self.logger.info("æµè§ˆå™¨å¯åŠ¨æˆåŠŸ")

                page = browser.new_page()
                
                # è®¾ç½®æµè§ˆå™¨æ ‡è¯†ï¼Œè¿›ä¸€æ­¥æ¨¡æ‹ŸçœŸå®ç”¨æˆ·
                page.set_extra_http_headers({
                    "Accept-Language": "zh-CN,zh;q=0.9",
                })

                try:
                    # è®¿é—®æ–°çš„æ¦œå•é¡µï¼ˆæ— éœ€ç™»å½•ï¼Œæ›´ç¨³å®šï¼‰
                    self.logger.info(f"æ­£åœ¨è®¿é—® {self.base_url} ...")
                    try:
                        page.goto(
                            self.base_url,
                            wait_until="domcontentloaded",
                            timeout=60000
                        )
                        self.logger.info("é¡µé¢åŠ è½½å®Œæˆ")
                    except Exception as e:
                        self.logger.warning(f"é¡µé¢åŠ è½½è¶…æ—¶ï¼Œç»§ç»­ä½¿ç”¨å·²åŠ è½½å†…å®¹: {e}")

                    # ç­‰å¾…åˆå§‹å†…å®¹åŠ è½½
                    page.wait_for_timeout(1500)

                    # æ¦œå•é¡µé€šå¸¸æ— éœ€æ»šåŠ¨å³å¯è·å¾— Top50
                    # ä¸ºç¨³å¦¥ï¼Œè½»å¾®æ»šåŠ¨ä¸¤æ¬¡è§¦å‘å¯èƒ½çš„æ‡’åŠ è½½
                    for _ in range(2):
                        page.evaluate("window.scrollBy(0, window.innerHeight)")
                        page.wait_for_timeout(400)

                    # è·å–æœ€ç»ˆçš„é¡µé¢ HTML
                    html_content = page.content()
                    self.logger.info(f"é¡µé¢å†…å®¹è·å–æˆåŠŸï¼Œå¤§å°ï¼š{len(html_content)} bytes")

                    # ä¿å­˜è°ƒè¯• HTMLï¼ˆå¯é€‰ï¼‰
                    try:
                        with open("debug_realtime_page_playwright.html", "w", encoding="utf-8") as f:
                            f.write(html_content)
                        self.logger.debug("HTML å·²ä¿å­˜åˆ° debug_realtime_page_playwright.html")
                    except Exception as e:
                        self.logger.debug(f"ä¿å­˜è°ƒè¯• HTML å¤±è´¥: {e}")

                finally:
                    browser.close()
                    self.logger.info("æµè§ˆå™¨å·²å…³é—­")

        except ImportError:
            self.logger.error("Playwright åº“æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨æ­¤æ–¹æ³•")
            return []
        except Exception as e:
            self.logger.error(f"Playwright è·å–é¡µé¢å¤±è´¥: {e}")
            return []

        if not html_content:
            self.logger.error("æœªè·å–åˆ°é¡µé¢å†…å®¹")
            return []

        # è§£æè·å–åˆ°çš„ HTML
        items = self.parse_realtime_page(html_content)
        return items[:50]


if __name__ == "__main__":
    main()

