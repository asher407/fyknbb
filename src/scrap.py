"""
å¾®åšçƒ­æœå†å²æ•°æ®çˆ¬è™«æ¨¡å—

æœ¬æ¨¡å—ç”¨äºçˆ¬å–å¾®åšçƒ­æœå†å²æ•°æ®ç½‘ç«™ï¼ˆhttps://weibo-trending-hot-history.vercel.app/hots/{date}ï¼‰
å¹¶å°†æ•°æ®æŒ‰æ—¥æœŸä¿å­˜ä¸ºJSONæ ¼å¼ã€‚

ç±»å®šä¹‰ï¼š
    WeiboHotScraper: å¾®åšçƒ­æœçˆ¬è™«ä¸»ç±»

ä¸»è¦åŠŸèƒ½ï¼š
    1. çˆ¬å–æŒ‡å®šæ—¥æœŸèŒƒå›´çš„å¾®åšçƒ­æœæ•°æ®
    2. è§£æHTMLé¡µé¢ä¸­çš„çƒ­æœæ¡ç›®ä¿¡æ¯
    3. å°†æ•°æ®æŒ‰æœˆä»½ç»„ç»‡å¹¶ä¿å­˜ä¸ºJSONæ–‡ä»¶
    4. æä¾›é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶

ä½¿ç”¨ç¤ºä¾‹ï¼š
    scraper = WeiboHotScraper(output_dir="data")
    scraper.scrape_range("2025-01-01", "2025-12-12")
"""

import json
import logging
import os
import re
import time
from dataclasses import asdict, dataclass
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple

import requests
from bs4 import BeautifulSoup


@dataclass
class HotItem:
    """
    çƒ­æœæ¡ç›®æ•°æ®ç±»

    å±æ€§ï¼š
        rank: æ’åï¼ˆç¬¬å‡ åï¼‰
        title: çƒ­æœæ ‡é¢˜
        category: åˆ†ç±»ï¼ˆå¦‚ï¼šæ˜æ˜Ÿã€ç¤¾ä¼šç­‰ï¼‰
        heat: çƒ­åº¦å€¼ï¼ˆå•ä½ï¼šä¸‡ï¼‰
        reads: é˜…è¯»é‡ï¼ˆå•ä½ï¼šäº¿/ä¸‡ï¼‰
        discussions: è®¨è®ºé‡ï¼ˆå•ä½ï¼šä¸‡ï¼‰
        originals: åŸåˆ›é‡ï¼ˆå•ä½ï¼šä¸‡ï¼‰
        date: æ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰
    """

    rank: int
    title: str
    category: str
    heat: float
    reads: float
    discussions: float
    originals: float
    date: str


class WeiboHotScraper:
    """
    å¾®åšçƒ­æœçˆ¬è™«ä¸»ç±»

    åŠŸèƒ½ï¼š
        - çˆ¬å–æŒ‡å®šæ—¥æœŸçš„å¾®åšçƒ­æœæ•°æ®
        - è§£æHTMLä¸­çš„çƒ­æœæ¡ç›®ä¿¡æ¯
        - å°†æ•°æ®ä¿å­˜ä¸ºJSONæ ¼å¼æ–‡ä»¶
        - æŒ‰æœˆä»½ç»„ç»‡æ•°æ®æ–‡ä»¶

    å‚æ•°ï¼š
        base_url (str): ç›®æ ‡ç½‘ç«™çš„åŸºç¡€URL
        output_dir (str): è¾“å‡ºç›®å½•è·¯å¾„
        delay (float): è¯·æ±‚é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé˜²æ­¢è¢«å°IP
        timeout (int): è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        max_retries (int): æœ€å¤§é‡è¯•æ¬¡æ•°

    æ–¹æ³•ï¼š
        fetch_page(date: str) -> Optional[str]: è·å–æŒ‡å®šæ—¥æœŸçš„é¡µé¢HTML
        parse_page(html: str, date: str) -> List[HotItem]: è§£æHTMLæå–çƒ­æœæ•°æ®
        save_data(data: List[HotItem], date: str) -> bool: ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶
        scrape_date(date: str) -> bool: çˆ¬å–å¹¶ä¿å­˜æŒ‡å®šæ—¥æœŸçš„æ•°æ®
        scrape_range(start_date: str, end_date: str) -> Dict[str, Any]: çˆ¬å–æ—¥æœŸèŒƒå›´çš„æ•°æ®
    """

    def __init__(
        self,
        base_url: str = "https://weibo-trending-hot-history.vercel.app/hots",
        output_dir: str = "data",
        delay: float = 1.0,
        timeout: int = 10,
        max_retries: int = 3,
    ):
        """
        åˆå§‹åŒ–çˆ¬è™«

        å‚æ•°ï¼š
            base_url: ç›®æ ‡ç½‘ç«™çš„åŸºç¡€URL
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
            delay: è¯·æ±‚é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        self.base_url = base_url
        self.output_dir = output_dir
        self.delay = delay
        self.timeout = timeout
        self.max_retries = max_retries

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)

        # é…ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        )
        self.logger = logging.getLogger(__name__)

        # é…ç½®requestsä¼šè¯
        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                "Accept-Encoding": "gzip, deflate, br",
            }
        )

    def fetch_page(self, date: str) -> Optional[str]:
        """
        è·å–æŒ‡å®šæ—¥æœŸçš„é¡µé¢HTML

        å‚æ•°ï¼š
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸ºYYYY-MM-DD

        è¿”å›ï¼š
            HTMLå­—ç¬¦ä¸²æˆ–Noneï¼ˆå¦‚æœè¯·æ±‚å¤±è´¥ï¼‰
        """
        url = f"{self.base_url}/{date}"

        for attempt in range(self.max_retries):
            try:
                self.logger.info(
                    f"æ­£åœ¨è·å– {date} çš„æ•°æ® (å°è¯• {attempt + 1}/{self.max_retries})"
                )
                response = self.session.get(url, timeout=self.timeout)
                response.raise_for_status()

                # æ£€æŸ¥æ˜¯å¦è¿”å›æœ‰æ•ˆå†…å®¹
                if response.status_code == 200 and len(response.text) > 100:
                    # æ£€æŸ¥é¡µé¢æ˜¯å¦åŒ…å«æœ‰æ•ˆæ•°æ®
                    # æ³¨æ„ï¼šæœ‰äº›é¡µé¢å¯èƒ½åŒ…å«"æ²¡æœ‰æ‰¾åˆ°"æˆ–"404"ä½†ä»æœ‰å†…å®¹ï¼Œéœ€è¦æ›´æ™ºèƒ½çš„åˆ¤æ–­
                    html_text = response.text

                    # æ£€æŸ¥æ˜¯å¦æœ‰çƒ­æœæ¡ç›®çš„ç‰¹å¾
                    has_hot_features = (
                        "æŸ¥çœ‹å¾®åšè¯é¢˜" in html_text
                        or "text-xl" in html_text
                        or "inline-flex" in html_text
                    )

                    # å¦‚æœé¡µé¢æœ‰çƒ­æœç‰¹å¾ï¼Œå³ä½¿åŒ…å«"æ²¡æœ‰æ‰¾åˆ°"æˆ–"404"ä¹Ÿè¿”å›
                    if has_hot_features:
                        self.logger.info(f"æˆåŠŸè·å– {date} çš„æ•°æ®ï¼ˆæœ‰çƒ­æœç‰¹å¾ï¼‰")
                        return html_text

                    # å¦åˆ™ï¼Œæ£€æŸ¥æ˜¯å¦çœŸçš„æ˜¯æ— æ•ˆé¡µé¢
                    if "æ²¡æœ‰æ‰¾åˆ°" in html_text or "404" in html_text:
                        # è¿›ä¸€æ­¥ç¡®è®¤ï¼šæ£€æŸ¥é¡µé¢æ˜¯å¦çœŸçš„å¾ˆå°æˆ–è€…æ²¡æœ‰å®é™…å†…å®¹ç»“æ„
                        soup = BeautifulSoup(html_text, "html.parser")
                        a_tags = soup.find_all("a")

                        # å¦‚æœå‡ ä¹æ²¡æœ‰aæ ‡ç­¾ï¼Œå¯èƒ½æ˜¯çœŸçš„æ²¡æœ‰æ•°æ®
                        if len(a_tags) < 10:  # æ™®é€šé¡µé¢é€šå¸¸æœ‰å¾ˆå¤šaæ ‡ç­¾
                            self.logger.warning(f"{date} æ²¡æœ‰æ•°æ®ï¼ˆé¡µé¢ç»“æ„ç®€å•ï¼‰")
                            return None
                        else:
                            # æœ‰å¾ˆå¤šaæ ‡ç­¾ï¼Œå¯èƒ½åªæ˜¯é¡µé¢åŒ…å«404æ–‡æœ¬ä½†æœ‰å®é™…å†…å®¹
                            self.logger.info(f"è·å– {date} çš„æ•°æ®ï¼ˆæœ‰aæ ‡ç­¾ç»“æ„ï¼‰")
                            return html_text

                    self.logger.info(f"æˆåŠŸè·å– {date} çš„æ•°æ®")
                    return html_text
                else:
                    self.logger.warning(f"{date} è¿”å›äº†ç©ºé¡µé¢æˆ–æ— æ•ˆå“åº”")

            except requests.exceptions.RequestException as e:
                self.logger.error(f"è·å– {date} æ•°æ®å¤±è´¥: {e}")
                if attempt < self.max_retries - 1:
                    wait_time = 2**attempt  # æŒ‡æ•°é€€é¿
                    self.logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    self.logger.error(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè·³è¿‡ {date}")

            except Exception as e:
                self.logger.error(f"è·å– {date} æ•°æ®æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
                break

        return None

    def _extract_number(self, text: str) -> float:
        """
        ä»æ–‡æœ¬ä¸­æå–æ•°å­—ï¼Œå¤„ç†ä¸­æ–‡å•ä½

        å‚æ•°ï¼š
            text: åŒ…å«æ•°å­—å’Œå•ä½çš„æ–‡æœ¬ï¼ˆå¦‚ï¼š"855.15ä¸‡", "1.50äº¿", "8210"ï¼‰

        è¿”å›ï¼š
            è½¬æ¢åçš„æµ®ç‚¹æ•°ï¼ˆç»Ÿä¸€è½¬æ¢ä¸ºä¸‡å•ä½ï¼‰
        """
        try:
            # ç§»é™¤ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
            text = text.strip()

            # åŒ¹é…æ•°å­—éƒ¨åˆ†ï¼ˆæ”¯æŒå°æ•°ï¼‰
            num_match = re.search(r"[\d\.]+", text)
            if not num_match:
                return 0.0

            num = float(num_match.group())

            # æ ¹æ®å•ä½è¿›è¡Œæ¢ç®—ï¼ˆç»Ÿä¸€è½¬æ¢ä¸ºä¸‡å•ä½ï¼‰
            if "äº¿" in text:
                num *= 10000  # äº¿è½¬æ¢ä¸ºä¸‡
            elif "ä¸‡" in text:
                pass  # å·²ç»æ˜¯ä¸‡å•ä½
            else:
                # æ²¡æœ‰å•ä½çš„æ•°å­—ï¼Œé™¤ä»¥10000è½¬æ¢ä¸ºä¸‡å•ä½
                # ä¾‹å¦‚ "8210" -> 0.821ä¸‡
                num /= 10000

            return round(num, 2)
        except Exception as e:
            self.logger.warning(f"è§£ææ•°å­—å¤±è´¥: {text}, é”™è¯¯: {e}")
            return 0.0

    def _parse_rank(self, h2_tag) -> Tuple[int, str]:
        """
        ä»h2æ ‡ç­¾ä¸­è§£ææ’åå’Œæ ‡é¢˜

        å‚æ•°ï¼š
            h2_tag: BeautifulSoupçš„h2æ ‡ç­¾å¯¹è±¡

        è¿”å›ï¼š
            (æ’å, æ ‡é¢˜) å…ƒç»„
        """
        try:
            # å¤„ç†å¯èƒ½çš„ä¸¤ç§æ ¼å¼ï¼š
            # æ ¼å¼1: <h2 class="text-xl">ç¬¬1åï¼šæé“è¢«åˆ¤20å¹´</h2>
            # æ ¼å¼2: <h2 class="text-xl"><span class="sr-only">ç¬¬<!-- -->1<!-- -->åï¼š</span>åŒè½¨ç©ºé™</h2>

            # è·å–å®Œæ•´çš„æ–‡æœ¬ï¼ˆåŒ…æ‹¬spanå†…çš„æ–‡æœ¬ï¼‰
            full_text = h2_tag.get_text(strip=True)

            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ’å
            rank_match = re.search(r"ç¬¬\s*(\d+)\s*å", full_text)
            if rank_match:
                rank = int(rank_match.group(1))

                # æå–æ ‡é¢˜ï¼šç§»é™¤æ’åéƒ¨åˆ†
                # æ³¨æ„ï¼šBeautifulSoupçš„get_text()å·²ç»å¤„ç†äº†HTMLæ³¨é‡Š
                title = re.sub(r"ç¬¬\s*\d+\s*åï¼š?", "", full_text).strip()

                # å¦‚æœæ ‡é¢˜ä¸ºç©ºï¼Œå°è¯•ä»spanåçš„æ–‡æœ¬è·å–
                if not title:
                    # æŸ¥æ‰¾spanæ ‡ç­¾
                    span_tag = h2_tag.find("span", class_="sr-only")
                    if span_tag:
                        # è·å–spanä¹‹åçš„æ‰€æœ‰æ–‡æœ¬
                        span_text = span_tag.get_text(strip=True)
                        title = full_text.replace(span_text, "").strip()

                return rank, title
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ’åæ ¼å¼ï¼Œå°è¯•å…¶ä»–æ ¼å¼
                rank_match = re.search(r"(\d+)", full_text)
                if rank_match:
                    rank = int(rank_match.group(1))
                    # ç§»é™¤æ•°å­—å’Œåˆ†éš”ç¬¦è·å–æ ‡é¢˜
                    title = re.sub(r"^\d+[\.:ï¼š]\s*", "", full_text).strip()
                    return rank, title
                else:
                    # é»˜è®¤æ’åä¸º0
                    return 0, full_text.strip()

        except Exception as e:
            self.logger.warning(f"è§£ææ’åå¤±è´¥: {str(h2_tag)[:50]}..., é”™è¯¯: {e}")
            return 0, h2_tag.get_text(strip=True) if hasattr(
                h2_tag, "get_text"
            ) else str(h2_tag)

    def parse_page(self, html: str, date: str) -> List[HotItem]:
        """
        è§£æHTMLé¡µé¢ï¼Œæå–çƒ­æœæ¡ç›®ä¿¡æ¯

        å‚æ•°ï¼š
            html: é¡µé¢HTMLå­—ç¬¦ä¸²
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸ºYYYY-MM-DD

        è¿”å›ï¼š
            çƒ­æœæ¡ç›®åˆ—è¡¨
        """
        hot_items = []

        try:
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            self.logger.debug(f"HTMLé•¿åº¦: {len(html)}")
            if len(html) < 1000:
                self.logger.warning(f"HTMLå¤ªçŸ­ï¼Œå¯èƒ½æœ‰é—®é¢˜: {html[:500]}...")

            # å°è¯•ä½¿ç”¨lxmlè§£æå™¨ï¼Œå¦‚æœä¸å¯ç”¨åˆ™å›é€€åˆ°html.parser
            try:
                soup = BeautifulSoup(html, "lxml")
                self.logger.debug("ä½¿ç”¨lxmlè§£æå™¨")
            except Exception as e:
                self.logger.warning(f"lxmlè§£æå™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨html.parser: {e}")
                soup = BeautifulSoup(html, "html.parser")
                self.logger.debug("ä½¿ç”¨html.parserè§£æå™¨")

            # æŸ¥æ‰¾æ‰€æœ‰çš„<a>æ ‡ç­¾ï¼Œä½†åªè¿‡æ»¤å‡ºçœŸæ­£çš„çƒ­æœæ¡ç›®
            # çœŸæ­£çš„çƒ­æœæ¡ç›®é€šå¸¸æœ‰ï¼šaria-labelåŒ…å«"æŸ¥çœ‹å¾®åšè¯é¢˜"ï¼Œæˆ–è€…åŒ…å«h2.text-xlå’Œdata div
            all_a_tags = soup.find_all("a")
            hot_a_tags = []

            # è°ƒè¯•ï¼šè®°å½•å‰å‡ ä¸ªaæ ‡ç­¾çš„ç»“æ„
            if len(all_a_tags) == 0:
                self.logger.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•<a>æ ‡ç­¾ï¼Œæ£€æŸ¥HTMLç»“æ„...")
                # æ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰å†…å®¹
                if soup.title:
                    self.logger.debug(f"é¡µé¢æ ‡é¢˜: {soup.title.string}")
                # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–å…ƒç´ 
                h2_tags = soup.find_all("h2")
                self.logger.debug(f"æ‰¾åˆ°{h2_tags}ä¸ªh2æ ‡ç­¾")
            else:
                self.logger.debug(f"æ‰¾åˆ°ç¬¬ä¸€ä¸ªaæ ‡ç­¾é¢„è§ˆ: {str(all_a_tags[0])[:200]}...")

            for a_tag in all_a_tags:
                aria_label = a_tag.get("aria-label", "")
                h2_tag = a_tag.find("h2", class_="text-xl")
                data_container = a_tag.find("div", class_="flex")

                # æ£€æŸ¥æ˜¯å¦ä¸ºçœŸæ­£çš„çƒ­æœæ¡ç›®
                if (("æŸ¥çœ‹å¾®åšè¯é¢˜" in aria_label) and h2_tag and data_container) or (
                    h2_tag and data_container
                ):
                    hot_a_tags.append(a_tag)

            self.logger.info(
                f"æ‰¾åˆ° {len(all_a_tags)} ä¸ª<a>æ ‡ç­¾ï¼Œå…¶ä¸­ {len(hot_a_tags)} ä¸ªæ˜¯çƒ­æœæ¡ç›®"
            )

            for a_tag in hot_a_tags:
                try:
                    # æŸ¥æ‰¾h2æ ‡ç­¾
                    h2_tag = a_tag.find("h2", class_="text-xl")
                    if not h2_tag:
                        continue

                    # è§£ææ’åå’Œæ ‡é¢˜
                    rank, title = self._parse_rank(h2_tag)

                    # æŸ¥æ‰¾åŒ…å«åˆ†ç±»å’Œæ•°æ®çš„divå®¹å™¨
                    data_container = a_tag.find("div", class_="flex")
                    if not data_container:
                        continue

                    # æå–æ‰€æœ‰æ•°æ®æ ‡ç­¾
                    data_tags = data_container.find_all("div", class_="inline-flex")

                    # åˆå§‹åŒ–æ•°æ®å­—æ®µ
                    category = ""
                    heat = 0.0
                    reads = 0.0
                    discussions = 0.0
                    originals = 0.0

                    for data_tag in data_tags:
                        text = data_tag.get_text(strip=True)

                        # æ ¹æ®æ–‡æœ¬å†…å®¹åˆ¤æ–­æ•°æ®ç±»å‹
                        # å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯æ•°æ®å­—æ®µ
                        if "ğŸ”¥" in text:
                            # çƒ­åº¦
                            heat = self._extract_number(text.replace("ğŸ”¥", ""))
                        elif "é˜…è¯»" in text:
                            # é˜…è¯»é‡
                            reads = self._extract_number(text.replace("é˜…è¯»", ""))
                        elif "è®¨è®º" in text:
                            # è®¨è®ºé‡
                            discussions = self._extract_number(text.replace("è®¨è®º", ""))
                        elif "åŸåˆ›" in text:
                            # åŸåˆ›é‡
                            originals = self._extract_number(text.replace("åŸåˆ›", ""))
                        else:
                            # å¤„ç†åˆ†ç±»æ ‡ç­¾ - æ›´å…¨é¢çš„åˆ†ç±»æ£€æµ‹
                            category_keywords = {
                                "æ˜æ˜Ÿ": "æ˜æ˜Ÿ",
                                "ç¤¾ä¼š": "ç¤¾ä¼š",
                                "å¨±ä¹": "å¨±ä¹",
                                "ä½“è‚²": "ä½“è‚²",
                                "ç§‘æŠ€": "ç§‘æŠ€",
                                "æ¸¸æˆ": "æ¸¸æˆ",
                                "ç¾é£Ÿ": "ç¾é£Ÿ",
                                "è´¢ç»": "è´¢ç»",
                                "æ—¶å°š": "æ—¶å°š",
                                "æ•™è‚²": "æ•™è‚²",
                                "å¥åº·": "å¥åº·",
                                "æ—…æ¸¸": "æ—…æ¸¸",
                                "æ±½è½¦": "æ±½è½¦",
                                "åŠ¨æ¼«": "åŠ¨æ¼«",
                                "å†›äº‹": "å†›äº‹",
                                "æ•°ç ": "æ•°ç ",
                                "éŸ³ä¹": "éŸ³ä¹",
                                "ç”µå½±": "ç”µå½±",
                                "ç”µè§†å‰§": "ç”µè§†å‰§",
                                "ç»¼è‰º": "ç»¼è‰º",
                                "æç¬‘": "æç¬‘",
                                "æƒ…æ„Ÿ": "æƒ…æ„Ÿ",
                                "ç”Ÿæ´»": "ç”Ÿæ´»",
                                "å®¶å±…": "å®¶å±…",
                                "è‚²å„¿": "è‚²å„¿",
                                "å® ç‰©": "å® ç‰©",
                                "æ‘„å½±": "æ‘„å½±",
                                "ç»˜ç”»": "ç»˜ç”»",
                                "è¯»ä¹¦": "è¯»ä¹¦",
                                "å†™ä½œ": "å†™ä½œ",
                                "èŒåœº": "èŒåœº",
                                "æ³•å¾‹": "æ³•å¾‹",
                                "æ”¿æ²»": "æ”¿æ²»",
                                "å†å²": "å†å²",
                                "æ–‡åŒ–": "æ–‡åŒ–",
                                "è‰ºæœ¯": "è‰ºæœ¯",
                                "ç§‘å­¦": "ç§‘å­¦",
                                "è‡ªç„¶": "è‡ªç„¶",
                                "ç¯ä¿": "ç¯ä¿",
                                "å…¬ç›Š": "å…¬ç›Š",
                                "å®—æ•™": "å®—æ•™",
                                "å¿ƒç†": "å¿ƒç†",
                                "æ˜Ÿåº§": "æ˜Ÿåº§",
                                "å½©ç¥¨": "å½©ç¥¨",
                                "è‚¡ç¥¨": "è‚¡ç¥¨",
                                "æˆ¿äº§": "æˆ¿äº§",
                                "åˆ›ä¸š": "åˆ›ä¸š",
                                "äº’è”ç½‘": "äº’è”ç½‘",
                                "æ‰‹æœº": "æ‰‹æœº",
                                "ç”µè„‘": "ç”µè„‘",
                                "è½¯ä»¶": "è½¯ä»¶",
                                "ç½‘ç»œ": "ç½‘ç»œ",
                                "ç”µå•†": "ç”µå•†",
                                "ç›´æ’­": "ç›´æ’­",
                                "ç½‘çº¢": "ç½‘çº¢",
                                "ç¾å¦†": "ç¾å¦†",
                                "æœé¥°": "æœé¥°",
                                "é‹åŒ…": "é‹åŒ…",
                                "ç å®": "ç å®",
                                "æ‰‹è¡¨": "æ‰‹è¡¨",
                                "å®¶å…·": "å®¶å…·",
                                "å®¶ç”µ": "å®¶ç”µ",
                                "å¨å…·": "å¨å…·",
                                "é£Ÿå“": "é£Ÿå“",
                                "é¥®æ–™": "é¥®æ–™",
                                "é…’æ°´": "é…’æ°´",
                                "çƒŸè‰": "çƒŸè‰",
                                "è¯å“": "è¯å“",
                                "åŒ»ç–—": "åŒ»ç–—",
                                "åŒ»é™¢": "åŒ»é™¢",
                                "å­¦æ ¡": "å­¦æ ¡",
                                "æ•™è‚²æœºæ„": "æ•™è‚²æœºæ„",
                                "å…¬å¸": "å…¬å¸",
                                "å·¥å‚": "å·¥å‚",
                                "å†œæ‘": "å†œæ‘",
                                "åŸå¸‚": "åŸå¸‚",
                                "äº¤é€š": "äº¤é€š",
                                "èˆªç©º": "èˆªç©º",
                                "é“è·¯": "é“è·¯",
                                "å…¬è·¯": "å…¬è·¯",
                                "æµ·è¿": "æµ·è¿",
                                "å¤©æ°”": "å¤©æ°”",
                                "åœ°éœ‡": "åœ°éœ‡",
                                "å°é£": "å°é£",
                                "æ´ªæ°´": "æ´ªæ°´",
                                "ç«ç¾": "ç«ç¾",
                                "äº‹æ•…": "äº‹æ•…",
                                "çŠ¯ç½ª": "çŠ¯ç½ª",
                                "è­¦å¯Ÿ": "è­¦å¯Ÿ",
                                "æ³•é™¢": "æ³•é™¢",
                                "ç›‘ç‹±": "ç›‘ç‹±",
                                "æ­»äº¡": "æ­»äº¡",
                                "å‡ºç”Ÿ": "å‡ºç”Ÿ",
                                "ç»“å©š": "ç»“å©š",
                                "ç¦»å©š": "ç¦»å©š",
                                "æ‹çˆ±": "æ‹çˆ±",
                                "åˆ†æ‰‹": "åˆ†æ‰‹",
                                "æ±‚å©š": "æ±‚å©š",
                                "å©šç¤¼": "å©šç¤¼",
                                "ç”Ÿæ—¥": "ç”Ÿæ—¥",
                                "èŠ‚æ—¥": "èŠ‚æ—¥",
                                "æ˜¥èŠ‚": "æ˜¥èŠ‚",
                                "ä¸­ç§‹": "ä¸­ç§‹",
                                "ç«¯åˆ": "ç«¯åˆ",
                                "æ¸…æ˜": "æ¸…æ˜",
                                "å›½åº†": "å›½åº†",
                                "å…ƒæ—¦": "å…ƒæ—¦",
                                "åœ£è¯": "åœ£è¯",
                            }

                            # æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«ä»»ä½•åˆ†ç±»å…³é”®è¯
                            for keyword, cat in category_keywords.items():
                                if keyword in text:
                                    category = cat
                                    break

                    # åˆ›å»ºHotItemå¯¹è±¡ï¼ˆç¡®ä¿æœ‰æœ‰æ•ˆæ•°æ®ï¼‰
                    if rank > 0 and title:  # åŸºæœ¬éªŒè¯
                        hot_item = HotItem(
                            rank=rank,
                            title=title,
                            category=category,
                            heat=heat,
                            reads=reads,
                            discussions=discussions,
                            originals=originals,
                            date=date,
                        )
                        hot_items.append(hot_item)
                    else:
                        self.logger.debug(f"è·³è¿‡æ— æ•ˆæ¡ç›®: rank={rank}, title={title}")

                except Exception as e:
                    self.logger.warning(f"è§£æå•ä¸ªçƒ­æœæ¡ç›®å¤±è´¥: {e}")
                    continue

            # æŒ‰æ’åæ’åº
            hot_items.sort(key=lambda x: x.rank)

            self.logger.info(f"æˆåŠŸè§£æ {len(hot_items)} ä¸ªçƒ­æœæ¡ç›®")

            # å¦‚æœæ²¡æœ‰è§£æåˆ°ä»»ä½•æ¡ç›®ï¼Œå¯èƒ½æ˜¯é¡µé¢ç»“æ„ä¸åŒï¼Œå°è¯•å¤‡ç”¨è§£ææ–¹æ³•
            if len(hot_items) == 0:
                self.logger.warning("ä½¿ç”¨ä¸»è§£ææ–¹æ³•æœªæ‰¾åˆ°æ•°æ®ï¼Œå°è¯•å¤‡ç”¨è§£ææ–¹æ³•...")
                hot_items = self._parse_page_backup(soup, date)

        except Exception as e:
            self.logger.error(f"è§£æé¡µé¢å¤±è´¥: {e}")

        return hot_items

    def save_data(self, data: List[HotItem], date: str) -> bool:
        """
        ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶

        å‚æ•°ï¼š
            data: çƒ­æœæ¡ç›®åˆ—è¡¨
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸ºYYYY-MM-DD

        è¿”å›ï¼š
            ä¿å­˜æˆåŠŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        try:
            # è§£ææ—¥æœŸï¼Œåˆ›å»ºæœˆä»½ç›®å½•
            date_obj = datetime.strptime(date, "%Y-%m-%d")
            year_month = date_obj.strftime("%Y-%m")

            # åˆ›å»ºæœˆä»½ç›®å½•
            month_dir = os.path.join(self.output_dir, year_month)
            os.makedirs(month_dir, exist_ok=True)

            # æ„å»ºæ–‡ä»¶è·¯å¾„
            filename = f"{date}.json"
            filepath = os.path.join(month_dir, filename)

            # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            data_dicts = [asdict(item) for item in data]

            # ä¿å­˜ä¸ºJSONæ–‡ä»¶
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(
                    {"date": date, "count": len(data), "data": data_dicts},
                    f,
                    ensure_ascii=False,
                    indent=2,
                )

            self.logger.info(f"æ•°æ®å·²ä¿å­˜åˆ° {filepath}")
            return True

        except Exception as e:
            self.logger.error(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")
            return False

    def scrape_date_with_html(self, date: str, html: str) -> bool:
        """
        ä½¿ç”¨æä¾›çš„HTMLçˆ¬å–å¹¶ä¿å­˜æŒ‡å®šæ—¥æœŸçš„æ•°æ®ï¼ˆç”¨äºæµ‹è¯•ï¼‰

        å‚æ•°ï¼š
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸ºYYYY-MM-DD
            html: é¡µé¢HTMLå­—ç¬¦ä¸²

        è¿”å›ï¼š
            çˆ¬å–æˆåŠŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        self.logger.info(f"å¼€å§‹è§£æ {date} çš„æ•°æ®...")

        # è§£æé¡µé¢
        hot_items = self.parse_page(html, date)
        if not hot_items:
            self.logger.warning(f"{date} æ²¡æœ‰è§£æåˆ°çƒ­æœæ•°æ®")

        # ä¿å­˜æ•°æ®
        success = self.save_data(hot_items, date)

        if success:
            self.logger.info(f"æˆåŠŸå®Œæˆ {date} çš„æ•°æ®è§£æ")
        else:
            self.logger.error(f"{date} çš„æ•°æ®è§£æå¤±è´¥")

        return success

    def scrape_date(self, date: str) -> bool:
        """
        çˆ¬å–å¹¶ä¿å­˜æŒ‡å®šæ—¥æœŸçš„æ•°æ®

        å‚æ•°ï¼š
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸ºYYYY-MM-DD

        è¿”å›ï¼š
            çˆ¬å–æˆåŠŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        self.logger.info(f"å¼€å§‹çˆ¬å– {date} çš„æ•°æ®...")

        # è·å–é¡µé¢HTML
        html = self.fetch_page(date)
        if not html:
            self.logger.error(f"æ— æ³•è·å– {date} çš„é¡µé¢")
            return False

        # ç­‰å¾…å»¶è¿Ÿ
        time.sleep(self.delay)

        # è§£æé¡µé¢
        hot_items = self.parse_page(html, date)
        if not hot_items:
            self.logger.warning(f"{date} æ²¡æœ‰è§£æåˆ°çƒ­æœæ•°æ®")
            # ä»ç„¶å°è¯•ä¿å­˜ç©ºæ•°æ®ä»¥è®°å½•è¯¥æ—¥æœŸå·²è¢«å¤„ç†

        # ä¿å­˜æ•°æ®
        success = self.save_data(hot_items, date)

        if success:
            self.logger.info(f"æˆåŠŸå®Œæˆ {date} çš„æ•°æ®çˆ¬å–")
        else:
            self.logger.error(f"{date} çš„æ•°æ®çˆ¬å–å¤±è´¥")

        return success

    def scrape_range(self, start_date: str, end_date: str) -> Dict[str, Any]:
        """
        çˆ¬å–æŒ‡å®šæ—¥æœŸèŒƒå›´çš„æ•°æ®

        å‚æ•°ï¼š
            start_date: å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYY-MM-DD
            end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYY-MM-DD

        è¿”å›ï¼š
            åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        stats = {"total_dates": 0, "successful": 0, "failed": 0, "failed_dates": []}

        try:
            # è§£ææ—¥æœŸèŒƒå›´
            start = datetime.strptime(start_date, "%Y-%m-%d")
            end = datetime.strptime(end_date, "%Y-%m-%d")

            # ç”Ÿæˆæ—¥æœŸåˆ—è¡¨
            current = start
            date_list = []

            while current <= end:
                date_list.append(current.strftime("%Y-%m-%d"))
                current += timedelta(days=1)

            stats["total_dates"] = len(date_list)
            self.logger.info(f"éœ€è¦çˆ¬å– {len(date_list)} å¤©çš„æ•°æ®")

            # éå†æ—¥æœŸå¹¶çˆ¬å–
            for i, date in enumerate(date_list, 1):
                self.logger.info(f"è¿›åº¦: {i}/{len(date_list)} ({date})")

                try:
                    success = self.scrape_date(date)

                    if success:
                        stats["successful"] += 1
                    else:
                        stats["failed"] += 1
                        stats["failed_dates"].append(date)

                except Exception as e:
                    self.logger.error(f"çˆ¬å– {date} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
                    stats["failed"] += 1
                    stats["failed_dates"].append(date)

                # åœ¨æ—¥æœŸä¹‹é—´æ·»åŠ é¢å¤–å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                if i < len(date_list):
                    time.sleep(self.delay * 0.5)

            # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
            self.logger.info(
                f"çˆ¬å–å®Œæˆï¼æˆåŠŸ: {stats['successful']}, å¤±è´¥: {stats['failed']}"
            )
            if stats["failed_dates"]:
                self.logger.warning(f"å¤±è´¥çš„æ—¥æœŸ: {stats['failed_dates']}")

        except ValueError as e:
            self.logger.error(f"æ—¥æœŸæ ¼å¼é”™è¯¯: {e}")
        except Exception as e:
            self.logger.error(f"çˆ¬å–èŒƒå›´æ•°æ®æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

        return stats

    def _parse_page_backup(self, soup: BeautifulSoup, date: str) -> List[HotItem]:
        """
        å¤‡ç”¨è§£ææ–¹æ³•ï¼Œç”¨äºå¤„ç†ä¸åŒçš„é¡µé¢ç»“æ„

        å‚æ•°ï¼š
            soup: BeautifulSoupå¯¹è±¡
            date: æ—¥æœŸå­—ç¬¦ä¸²

        è¿”å›ï¼š
            çƒ­æœæ¡ç›®åˆ—è¡¨
        """
        hot_items = []

        try:
            # å¤‡ç”¨æ–¹æ³•ï¼šæŸ¥æ‰¾æ‰€æœ‰åŒ…å«çƒ­æœæ•°æ®çš„å®¹å™¨
            # æœ‰äº›é¡µé¢å¯èƒ½æœ‰ä¸åŒçš„ç»“æ„
            containers = soup.find_all("div", class_="rounded-lg")

            for container in containers:
                try:
                    # å°è¯•ä¸åŒçš„é€‰æ‹©å™¨ç»„åˆ
                    h2_tag = container.find("h2", class_="text-xl")
                    if not h2_tag:
                        continue

                    rank, title = self._parse_rank(h2_tag)

                    # æŸ¥æ‰¾æ•°æ®
                    data_div = container.find("div", class_="flex")
                    if not data_div:
                        continue

                    data_tags = data_div.find_all("div", class_="inline-flex")

                    # åˆå§‹åŒ–æ•°æ®å­—æ®µ
                    category = ""
                    heat = 0.0
                    reads = 0.0
                    discussions = 0.0
                    originals = 0.0

                    for data_tag in data_tags:
                        text = data_tag.get_text(strip=True)

                        if (
                            "æ˜æ˜Ÿ" in text
                            or "ç¤¾ä¼š" in text
                            or "å¨±ä¹" in text
                            or "ä½“è‚²" in text
                        ):
                            category = text
                        elif "ğŸ”¥" in text:
                            heat = self._extract_number(text.replace("ğŸ”¥", ""))
                        elif "é˜…è¯»" in text:
                            reads = self._extract_number(text.replace("é˜…è¯»", ""))
                        elif "è®¨è®º" in text:
                            discussions = self._extract_number(text.replace("è®¨è®º", ""))
                        elif "åŸåˆ›" in text:
                            originals = self._extract_number(text.replace("åŸåˆ›", ""))
                        elif "æ¸¸æˆ" in text:
                            category = "æ¸¸æˆ"
                        elif "ç¾é£Ÿ" in text:
                            category = "ç¾é£Ÿ"
                        elif "è´¢ç»" in text:
                            category = "è´¢ç»"

                    if rank > 0 and title:
                        hot_item = HotItem(
                            rank=rank,
                            title=title,
                            category=category,
                            heat=heat,
                            reads=reads,
                            discussions=discussions,
                            originals=originals,
                            date=date,
                        )
                        hot_items.append(hot_item)

                except Exception as e:
                    self.logger.debug(f"å¤‡ç”¨è§£ææ–¹æ³•å¤„ç†å•ä¸ªæ¡ç›®å¤±è´¥: {e}")
                    continue

            self.logger.info(f"å¤‡ç”¨æ–¹æ³•è§£æåˆ° {len(hot_items)} ä¸ªçƒ­æœæ¡ç›®")

        except Exception as e:
            self.logger.error(f"å¤‡ç”¨è§£ææ–¹æ³•å¤±è´¥: {e}")

        return hot_items

    def test_parse(self, date: str = "2024-12-13") -> None:
        """
        æµ‹è¯•è§£æåŠŸèƒ½

        å‚æ•°ï¼š
            date: æµ‹è¯•æ—¥æœŸ
        """
        print(f"æµ‹è¯•è§£æåŠŸèƒ½ - æ—¥æœŸ: {date}")
        print("=" * 60)

        # è·å–é¡µé¢HTML
        html = self.fetch_page(date)
        if not html:
            print(f"æ— æ³•è·å– {date} çš„é¡µé¢")
            return

        # è§£æé¡µé¢
        hot_items = self.parse_page(html, date)

        # æ˜¾ç¤ºå‰5ä¸ªç»“æœ
        print(f"å…±è§£æåˆ° {len(hot_items)} ä¸ªçƒ­æœæ¡ç›®")
        print("\nå‰5ä¸ªçƒ­æœæ¡ç›®:")
        print("-" * 60)

        for i, item in enumerate(hot_items[:5]):
            print(f"{i + 1}. æ’å: {item.rank}")
            print(f"   æ ‡é¢˜: {item.title}")
            print(f"   åˆ†ç±»: {item.category}")
            print(f"   çƒ­åº¦: {item.heat}ä¸‡")
            print(f"   é˜…è¯»: {item.reads}ä¸‡")
            print(f"   è®¨è®º: {item.discussions}ä¸‡")
            print(f"   åŸåˆ›: {item.originals}ä¸‡")
            print()

        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        if hot_items:
            ranks = [item.rank for item in hot_items]
            print(f"æ’åèŒƒå›´: {min(ranks)} - {max(ranks)}")
            print(
                f"å¹³å‡çƒ­åº¦: {sum(item.heat for item in hot_items) / len(hot_items):.2f}ä¸‡"
            )
            print(f"æ€»é˜…è¯»é‡: {sum(item.reads for item in hot_items):.2f}ä¸‡")
            print(f"æ€»è®¨è®ºé‡: {sum(item.discussions for item in hot_items):.2f}ä¸‡")

        print("=" * 60)


def main():
    """
    ä¸»å‡½æ•°ï¼Œç”¨äºæµ‹è¯•å’Œæ¼”ç¤ºçˆ¬è™«åŠŸèƒ½

    åŠŸèƒ½ï¼š
        1. åˆ›å»ºçˆ¬è™«å®ä¾‹
        2. çˆ¬å–æŒ‡å®šæ—¥æœŸèŒƒå›´çš„å¾®åšçƒ­æœæ•°æ®
        3. å°†æ•°æ®ä¿å­˜åˆ°dataç›®å½•
    """
    print("å¾®åšçƒ­æœå†å²æ•°æ®çˆ¬è™«")
    print("=" * 60)
    print("ä½¿ç”¨æ–¹æ³•:")
    print("  1. python src/scrap.py                # è¿è¡Œå®Œæ•´çˆ¬å–")
    print("  2. python src/scrap.py test          # æµ‹è¯•è§£æåŠŸèƒ½")
    print("  3. python src/scrap.py single <date> # çˆ¬å–å•æ—¥æ•°æ®")
    print("=" * 60)

    import sys

    try:
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        scraper = WeiboHotScraper(
            output_dir="data",
            delay=2.0,  # é€‚å½“å»¶è¿Ÿï¼Œé¿å…è¢«å°IP
            max_retries=3,
        )

        # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
        if len(sys.argv) > 1:
            if sys.argv[1] == "test":
                # æµ‹è¯•æ¨¡å¼
                print("è¿è¡Œæµ‹è¯•æ¨¡å¼...")
                test_date = sys.argv[2] if len(sys.argv) > 2 else "2024-12-13"
                scraper.test_parse(test_date)
                return
            elif sys.argv[1] == "single" and len(sys.argv) > 2:
                # å•æ—¥çˆ¬å–æ¨¡å¼
                date = sys.argv[2]
                print(f"çˆ¬å–å•æ—¥æ•°æ®: {date}")
                success = scraper.scrape_date(date)
                if success:
                    print(f"æˆåŠŸçˆ¬å– {date} çš„æ•°æ®")
                else:
                    print(f"çˆ¬å– {date} çš„æ•°æ®å¤±è´¥")
                return
            elif sys.argv[1] == "help":
                print("å¸®åŠ©ä¿¡æ¯:")
                print("  test [date]    - æµ‹è¯•è§£æåŠŸèƒ½ï¼Œå¯é€‰æ—¥æœŸå‚æ•°")
                print("  single <date>  - çˆ¬å–å•æ—¥æ•°æ®")
                print("  help          - æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯")
                return

        # é»˜è®¤æ¨¡å¼ï¼šå®Œæ•´çˆ¬å–
        # è®¾ç½®æ—¥æœŸèŒƒå›´ï¼ˆæ ¹æ®ç”¨æˆ·è¦æ±‚ï¼‰
        start_date = "2025-01-01"
        end_date = "2025-12-12"

        print(f"å¼€å§‹çˆ¬å– {start_date} åˆ° {end_date} çš„æ•°æ®...")
        print("æ•°æ®å°†ä¿å­˜åˆ° data/ ç›®å½•ä¸‹ï¼ŒæŒ‰æœˆä»½ç»„ç»‡")
        print("æ³¨æ„ï¼šè¿™éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
        print("=" * 60)

        # å¼€å§‹çˆ¬å–
        stats = scraper.scrape_range(start_date, end_date)

        # è¾“å‡ºç»“æœ
        print("\nçˆ¬å–å®Œæˆï¼")
        print(f"æ€»å¤©æ•°: {stats['total_dates']}")
        print(f"æˆåŠŸ: {stats['successful']}")
        print(f"å¤±è´¥: {stats['failed']}")

        if stats["failed_dates"]:
            print(f"å¤±è´¥çš„æ—¥æœŸ: {', '.join(stats['failed_dates'][:5])}")
            if len(stats["failed_dates"]) > 5:
                print(f"... ä»¥åŠ {len(stats['failed_dates']) - 5} ä¸ªæ›´å¤š")

        print(f"\næ•°æ®å·²ä¿å­˜åˆ° {scraper.output_dir}/ ç›®å½•")

    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­ï¼Œçˆ¬è™«å·²åœæ­¢")
    except Exception as e:
        print(f"ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
