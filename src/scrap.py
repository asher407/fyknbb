"""
å¾®åšçƒ­æœå†å²æ•°æ®çˆ¬è™«æ¨¡å—

æœ¬æ¨¡å—ç”¨äºçˆ¬å–å¾®åšçƒ­æœå†å²æ•°æ®ç½‘ç«™ï¼ˆhttps://weibo-trending-hot-history.vercel.app/hots/{date}ï¼‰
å¹¶å°†æ•°æ®æŒ‰æ—¥æœŸä¿å­˜ä¸ºJSONæ ¼å¼ã€‚

ç±»å®šä¹‰ï¼š
    WeiboHotScraper: å¾®åšçƒ­æœçˆ¬è™«ä¸»ç±»
    RealtimeHotScraper: å®æ—¶çƒ­æœçˆ¬è™«ç±»

ä¸»è¦åŠŸèƒ½ï¼š
    1. çˆ¬å–æŒ‡å®šæ—¥æœŸèŒƒå›´çš„å¾®åšçƒ­æœæ•°æ®
    2. è§£æHTMLé¡µé¢ä¸­çš„çƒ­æœæ¡ç›®ä¿¡æ¯
    3. å°†æ•°æ®æŒ‰æœˆä»½ç»„ç»‡å¹¶ä¿å­˜ä¸ºJSONæ–‡ä»¶
    4. æä¾›é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶

ä½¿ç”¨ç¤ºä¾‹ï¼š
    scraper = WeiboHotScraper(output_dir="data")
    scraper.scrape_range("2025-01-01", "2025-12-12")
    - å®æ—¶çˆ¬è™«é¡µé¢è¿è¡Œå‘½ä»¤ï¼š
    python -m streamlit run src/gui/app.py
"""

import json
import logging
import os
import re
import time
from dataclasses import asdict, dataclass
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple

import requests
from bs4 import BeautifulSoup


@dataclass
class HotItem:
    """
    çƒ­æœæ¡ç›®æ•°æ®ç±»

    å±æ€§ï¼š
        rank: æ’åï¼ˆç¬¬å‡ åï¼‰
        title: çƒ­æœæ ‡é¢˜
        category: åˆ†ç±»ï¼ˆå¦‚ï¼šæ˜æ˜Ÿã€ç¤¾ä¼šç­‰ï¼‰
        heat: çƒ­åº¦å€¼ï¼ˆå•ä½ï¼šä¸‡ï¼‰
        reads: é˜…è¯»é‡ï¼ˆå•ä½ï¼šäº¿/ä¸‡ï¼‰
        discussions: è®¨è®ºé‡ï¼ˆå•ä½ï¼šä¸‡ï¼‰
        originals: åŸåˆ›é‡ï¼ˆå•ä½ï¼šä¸‡ï¼‰
        date: æ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰
    """

    rank: int
    title: str
    category: str
    heat: float
    reads: float
    discussions: float
    originals: float
    date: str


class WeiboHotScraper:
    """
    å¾®åšçƒ­æœçˆ¬è™«ä¸»ç±»

    åŠŸèƒ½ï¼š
        - çˆ¬å–æŒ‡å®šæ—¥æœŸçš„å¾®åšçƒ­æœæ•°æ®
        - è§£æHTMLä¸­çš„çƒ­æœæ¡ç›®ä¿¡æ¯
        - å°†æ•°æ®ä¿å­˜ä¸ºJSONæ ¼å¼æ–‡ä»¶
        - æŒ‰æœˆä»½ç»„ç»‡æ•°æ®æ–‡ä»¶

    å‚æ•°ï¼š
        base_url (str): ç›®æ ‡ç½‘ç«™çš„åŸºç¡€URL
        output_dir (str): è¾“å‡ºç›®å½•è·¯å¾„
        delay (float): è¯·æ±‚é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé˜²æ­¢è¢«å°IP
        timeout (int): è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        max_retries (int): æœ€å¤§é‡è¯•æ¬¡æ•°

    æ–¹æ³•ï¼š
        fetch_page(date: str) -> Optional[str]: è·å–æŒ‡å®šæ—¥æœŸçš„é¡µé¢HTML
        parse_page(html: str, date: str) -> List[HotItem]: è§£æHTMLæå–çƒ­æœæ•°æ®
        save_data(data: List[HotItem], date: str) -> bool: ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶
        scrape_date(date: str) -> bool: çˆ¬å–å¹¶ä¿å­˜æŒ‡å®šæ—¥æœŸçš„æ•°æ®
        scrape_range(start_date: str, end_date: str) -> Dict[str, Any]: çˆ¬å–æ—¥æœŸèŒƒå›´çš„æ•°æ®
    """

    def __init__(
        self,
        base_url: str = "https://weibo-trending-hot-history.vercel.app/hots",
        output_dir: str = "data",
        delay: float = 1.0,
        timeout: int = 10,
        max_retries: int = 3,
    ):
        """
        åˆå§‹åŒ–çˆ¬è™«

        å‚æ•°ï¼š
            base_url: ç›®æ ‡ç½‘ç«™çš„åŸºç¡€URL
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
            delay: è¯·æ±‚é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        self.base_url = base_url
        self.output_dir = output_dir
        self.delay = delay
        self.timeout = timeout
        self.max_retries = max_retries

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)

        # é…ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        )
        self.logger = logging.getLogger(__name__)

        # é…ç½®requestsä¼šè¯
        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                "Accept-Encoding": "gzip, deflate, br",
            }
        )

    def fetch_page(self, date: str) -> Optional[str]:
        """
        è·å–æŒ‡å®šæ—¥æœŸçš„é¡µé¢HTML

        å‚æ•°ï¼š
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸ºYYYY-MM-DD

        è¿”å›ï¼š
            HTMLå­—ç¬¦ä¸²æˆ–Noneï¼ˆå¦‚æœè¯·æ±‚å¤±è´¥ï¼‰
        """
        url = f"{self.base_url}/{date}"

        for attempt in range(self.max_retries):
            try:
                self.logger.info(
                    f"æ­£åœ¨è·å– {date} çš„æ•°æ® (å°è¯• {attempt + 1}/{self.max_retries})"
                )
                response = self.session.get(url, timeout=self.timeout)
                response.raise_for_status()

                # æ£€æŸ¥æ˜¯å¦è¿”å›æœ‰æ•ˆå†…å®¹
                if response.status_code == 200 and len(response.text) > 100:
                    # æ£€æŸ¥é¡µé¢æ˜¯å¦åŒ…å«æœ‰æ•ˆæ•°æ®
                    # æ³¨æ„ï¼šæœ‰äº›é¡µé¢å¯èƒ½åŒ…å«"æ²¡æœ‰æ‰¾åˆ°"æˆ–"404"ä½†ä»æœ‰å†…å®¹ï¼Œéœ€è¦æ›´æ™ºèƒ½çš„åˆ¤æ–­
                    html_text = response.text

                    # æ£€æŸ¥æ˜¯å¦æœ‰çƒ­æœæ¡ç›®çš„ç‰¹å¾
                    has_hot_features = (
                        "æŸ¥çœ‹å¾®åšè¯é¢˜" in html_text
                        or "text-xl" in html_text
                        or "inline-flex" in html_text
                    )

                    # å¦‚æœé¡µé¢æœ‰çƒ­æœç‰¹å¾ï¼Œå³ä½¿åŒ…å«"æ²¡æœ‰æ‰¾åˆ°"æˆ–"404"ä¹Ÿè¿”å›
                    if has_hot_features:
                        self.logger.info(f"æˆåŠŸè·å– {date} çš„æ•°æ®ï¼ˆæœ‰çƒ­æœç‰¹å¾ï¼‰")
                        return html_text

                    # å¦åˆ™ï¼Œæ£€æŸ¥æ˜¯å¦çœŸçš„æ˜¯æ— æ•ˆé¡µé¢
                    if "æ²¡æœ‰æ‰¾åˆ°" in html_text or "404" in html_text:
                        # è¿›ä¸€æ­¥ç¡®è®¤ï¼šæ£€æŸ¥é¡µé¢æ˜¯å¦çœŸçš„å¾ˆå°æˆ–è€…æ²¡æœ‰å®é™…å†…å®¹ç»“æ„
                        soup = BeautifulSoup(html_text, "html.parser")
                        a_tags = soup.find_all("a")

                        # å¦‚æœå‡ ä¹æ²¡æœ‰aæ ‡ç­¾ï¼Œå¯èƒ½æ˜¯çœŸçš„æ²¡æœ‰æ•°æ®
                        if len(a_tags) < 10:  # æ™®é€šé¡µé¢é€šå¸¸æœ‰å¾ˆå¤šaæ ‡ç­¾
                            self.logger.warning(f"{date} æ²¡æœ‰æ•°æ®ï¼ˆé¡µé¢ç»“æ„ç®€å•ï¼‰")
                            return None
                        else:
                            # æœ‰å¾ˆå¤šaæ ‡ç­¾ï¼Œå¯èƒ½åªæ˜¯é¡µé¢åŒ…å«404æ–‡æœ¬ä½†æœ‰å®é™…å†…å®¹
                            self.logger.info(f"è·å– {date} çš„æ•°æ®ï¼ˆæœ‰aæ ‡ç­¾ç»“æ„ï¼‰")
                            return html_text

                    self.logger.info(f"æˆåŠŸè·å– {date} çš„æ•°æ®")
                    return html_text
                else:
                    self.logger.warning(f"{date} è¿”å›äº†ç©ºé¡µé¢æˆ–æ— æ•ˆå“åº”")

            except requests.exceptions.RequestException as e:
                self.logger.error(f"è·å– {date} æ•°æ®å¤±è´¥: {e}")
                if attempt < self.max_retries - 1:
                    wait_time = 2**attempt  # æŒ‡æ•°é€€é¿
                    self.logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    self.logger.error(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè·³è¿‡ {date}")

            except Exception as e:
                self.logger.error(f"è·å– {date} æ•°æ®æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
                break

        return None

    def _extract_number(self, text: str) -> float:
        """
        ä»æ–‡æœ¬ä¸­æå–æ•°å­—ï¼Œå¤„ç†ä¸­æ–‡å•ä½

        å‚æ•°ï¼š
            text: åŒ…å«æ•°å­—å’Œå•ä½çš„æ–‡æœ¬ï¼ˆå¦‚ï¼š"855.15ä¸‡", "1.50äº¿", "8210"ï¼‰

        è¿”å›ï¼š
            è½¬æ¢åçš„æµ®ç‚¹æ•°ï¼ˆç»Ÿä¸€è½¬æ¢ä¸ºä¸‡å•ä½ï¼‰
        """
        try:
            # ç§»é™¤ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
            text = text.strip()

            # åŒ¹é…æ•°å­—éƒ¨åˆ†ï¼ˆæ”¯æŒå°æ•°ï¼‰
            num_match = re.search(r"[\d\.]+", text)
            if not num_match:
                return 0.0

            num = float(num_match.group())

            # æ ¹æ®å•ä½è¿›è¡Œæ¢ç®—ï¼ˆç»Ÿä¸€è½¬æ¢ä¸ºä¸‡å•ä½ï¼‰
            if "äº¿" in text:
                num *= 10000  # äº¿è½¬æ¢ä¸ºä¸‡
            elif "ä¸‡" in text:
                pass  # å·²ç»æ˜¯ä¸‡å•ä½
            else:
                # æ²¡æœ‰å•ä½çš„æ•°å­—ï¼Œé™¤ä»¥10000è½¬æ¢ä¸ºä¸‡å•ä½
                # ä¾‹å¦‚ "8210" -> 0.821ä¸‡
                num /= 10000

            return round(num, 2)
        except Exception as e:
            self.logger.warning(f"è§£ææ•°å­—å¤±è´¥: {text}, é”™è¯¯: {e}")
            return 0.0

    def _parse_rank(self, h2_tag) -> Tuple[int, str]:
        """
        ä»h2æ ‡ç­¾ä¸­è§£ææ’åå’Œæ ‡é¢˜

        å‚æ•°ï¼š
            h2_tag: BeautifulSoupçš„h2æ ‡ç­¾å¯¹è±¡

        è¿”å›ï¼š
            (æ’å, æ ‡é¢˜) å…ƒç»„
        """
        try:
            # å¤„ç†å¯èƒ½çš„ä¸¤ç§æ ¼å¼ï¼š
            # æ ¼å¼1: <h2 class="text-xl">ç¬¬1åï¼šæé“è¢«åˆ¤20å¹´</h2>
            # æ ¼å¼2: <h2 class="text-xl"><span class="sr-only">ç¬¬<!-- -->1<!-- -->åï¼š</span>åŒè½¨ç©ºé™</h2>

            # è·å–å®Œæ•´çš„æ–‡æœ¬ï¼ˆåŒ…æ‹¬spanå†…çš„æ–‡æœ¬ï¼‰
            full_text = h2_tag.get_text(strip=True)

            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ’å
            rank_match = re.search(r"ç¬¬\s*(\d+)\s*å", full_text)
            if rank_match:
                rank = int(rank_match.group(1))

                # æå–æ ‡é¢˜ï¼šç§»é™¤æ’åéƒ¨åˆ†
                # æ³¨æ„ï¼šBeautifulSoupçš„get_text()å·²ç»å¤„ç†äº†HTMLæ³¨é‡Š
                title = re.sub(r"ç¬¬\s*\d+\s*åï¼š?", "", full_text).strip()

                # å¦‚æœæ ‡é¢˜ä¸ºç©ºï¼Œå°è¯•ä»spanåçš„æ–‡æœ¬è·å–
                if not title:
                    # æŸ¥æ‰¾spanæ ‡ç­¾
                    span_tag = h2_tag.find("span", class_="sr-only")
                    if span_tag:
                        # è·å–spanä¹‹åçš„æ‰€æœ‰æ–‡æœ¬
                        span_text = span_tag.get_text(strip=True)
                        title = full_text.replace(span_text, "").strip()

                return rank, title
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ’åæ ¼å¼ï¼Œå°è¯•å…¶ä»–æ ¼å¼
                rank_match = re.search(r"(\d+)", full_text)
                if rank_match:
                    rank = int(rank_match.group(1))
                    # ç§»é™¤æ•°å­—å’Œåˆ†éš”ç¬¦è·å–æ ‡é¢˜
                    title = re.sub(r"^\d+[\.:ï¼š]\s*", "", full_text).strip()
                    return rank, title
                else:
                    # é»˜è®¤æ’åä¸º0
                    return 0, full_text.strip()

        except Exception as e:
            self.logger.warning(f"è§£ææ’åå¤±è´¥: {str(h2_tag)[:50]}..., é”™è¯¯: {e}")
            return 0, h2_tag.get_text(strip=True) if hasattr(
                h2_tag, "get_text"
            ) else str(h2_tag)

    def parse_page(self, html: str, date: str) -> List[HotItem]:
        """
        è§£æHTMLé¡µé¢ï¼Œæå–çƒ­æœæ¡ç›®ä¿¡æ¯

        å‚æ•°ï¼š
            html: é¡µé¢HTMLå­—ç¬¦ä¸²
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸ºYYYY-MM-DD

        è¿”å›ï¼š
            çƒ­æœæ¡ç›®åˆ—è¡¨
        """
        hot_items = []

        try:
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            self.logger.debug(f"HTMLé•¿åº¦: {len(html)}")
            if len(html) < 1000:
                self.logger.warning(f"HTMLå¤ªçŸ­ï¼Œå¯èƒ½æœ‰é—®é¢˜: {html[:500]}...")

            # å°è¯•ä½¿ç”¨lxmlè§£æå™¨ï¼Œå¦‚æœä¸å¯ç”¨åˆ™å›é€€åˆ°html.parser
            try:
                soup = BeautifulSoup(html, "lxml")
                self.logger.debug("ä½¿ç”¨lxmlè§£æå™¨")
            except Exception as e:
                self.logger.warning(f"lxmlè§£æå™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨html.parser: {e}")
                soup = BeautifulSoup(html, "html.parser")
                self.logger.debug("ä½¿ç”¨html.parserè§£æå™¨")

            # æŸ¥æ‰¾æ‰€æœ‰çš„<a>æ ‡ç­¾ï¼Œä½†åªè¿‡æ»¤å‡ºçœŸæ­£çš„çƒ­æœæ¡ç›®
            # çœŸæ­£çš„çƒ­æœæ¡ç›®é€šå¸¸æœ‰ï¼šaria-labelåŒ…å«"æŸ¥çœ‹å¾®åšè¯é¢˜"ï¼Œæˆ–è€…åŒ…å«h2.text-xlå’Œdata div
            all_a_tags = soup.find_all("a")
            hot_a_tags = []

            # è°ƒè¯•ï¼šè®°å½•å‰å‡ ä¸ªaæ ‡ç­¾çš„ç»“æ„
            if len(all_a_tags) == 0:
                self.logger.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•<a>æ ‡ç­¾ï¼Œæ£€æŸ¥HTMLç»“æ„...")
                # æ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰å†…å®¹
                if soup.title:
                    self.logger.debug(f"é¡µé¢æ ‡é¢˜: {soup.title.string}")
                # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–å…ƒç´ 
                h2_tags = soup.find_all("h2")
                self.logger.debug(f"æ‰¾åˆ°{h2_tags}ä¸ªh2æ ‡ç­¾")
            else:
                self.logger.debug(f"æ‰¾åˆ°ç¬¬ä¸€ä¸ªaæ ‡ç­¾é¢„è§ˆ: {str(all_a_tags[0])[:200]}...")

            for a_tag in all_a_tags:
                aria_label = a_tag.get("aria-label", "")
                h2_tag = a_tag.find("h2", class_="text-xl")
                data_container = a_tag.find("div", class_="flex")

                # æ£€æŸ¥æ˜¯å¦ä¸ºçœŸæ­£çš„çƒ­æœæ¡ç›®
                if (("æŸ¥çœ‹å¾®åšè¯é¢˜" in aria_label) and h2_tag and data_container) or (
                    h2_tag and data_container
                ):
                    hot_a_tags.append(a_tag)

            self.logger.info(
                f"æ‰¾åˆ° {len(all_a_tags)} ä¸ª<a>æ ‡ç­¾ï¼Œå…¶ä¸­ {len(hot_a_tags)} ä¸ªæ˜¯çƒ­æœæ¡ç›®"
            )

            for a_tag in hot_a_tags:
                try:
                    # æŸ¥æ‰¾h2æ ‡ç­¾
                    h2_tag = a_tag.find("h2", class_="text-xl")
                    if not h2_tag:
                        continue

                    # è§£ææ’åå’Œæ ‡é¢˜
                    rank, title = self._parse_rank(h2_tag)

                    # æŸ¥æ‰¾åŒ…å«åˆ†ç±»å’Œæ•°æ®çš„divå®¹å™¨
                    data_container = a_tag.find("div", class_="flex")
                    if not data_container:
                        continue

                    # æå–æ‰€æœ‰æ•°æ®æ ‡ç­¾
                    data_tags = data_container.find_all("div", class_="inline-flex")

                    # åˆå§‹åŒ–æ•°æ®å­—æ®µ
                    category = ""
                    heat = 0.0
                    reads = 0.0
                    discussions = 0.0
                    originals = 0.0

                    for data_tag in data_tags:
                        text = data_tag.get_text(strip=True)

                        # æ ¹æ®æ–‡æœ¬å†…å®¹åˆ¤æ–­æ•°æ®ç±»å‹
                        # å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯æ•°æ®å­—æ®µ
                        if "ğŸ”¥" in text:
                            # çƒ­åº¦
                            heat = self._extract_number(text.replace("ğŸ”¥", ""))
                        elif "é˜…è¯»" in text:
                            # é˜…è¯»é‡
                            reads = self._extract_number(text.replace("é˜…è¯»", ""))
                        elif "è®¨è®º" in text:
                            # è®¨è®ºé‡
                            discussions = self._extract_number(text.replace("è®¨è®º", ""))
                        elif "åŸåˆ›" in text:
                            # åŸåˆ›é‡
                            originals = self._extract_number(text.replace("åŸåˆ›", ""))
                        else:
                            # å¤„ç†åˆ†ç±»æ ‡ç­¾ - æ›´å…¨é¢çš„åˆ†ç±»æ£€æµ‹
                            category_keywords = {
                                "æ˜æ˜Ÿ": "æ˜æ˜Ÿ",
                                "ç¤¾ä¼š": "ç¤¾ä¼š",
                                "å¨±ä¹": "å¨±ä¹",
                                "ä½“è‚²": "ä½“è‚²",
                                "ç§‘æŠ€": "ç§‘æŠ€",
                                "æ¸¸æˆ": "æ¸¸æˆ",
                                "ç¾é£Ÿ": "ç¾é£Ÿ",
                                "è´¢ç»": "è´¢ç»",
                                "æ—¶å°š": "æ—¶å°š",
                                "æ•™è‚²": "æ•™è‚²",
                                "å¥åº·": "å¥åº·",
                                "æ—…æ¸¸": "æ—…æ¸¸",
                                "æ±½è½¦": "æ±½è½¦",
                                "åŠ¨æ¼«": "åŠ¨æ¼«",
                                "å†›äº‹": "å†›äº‹",
                                "æ•°ç ": "æ•°ç ",
                                "éŸ³ä¹": "éŸ³ä¹",
                                "ç”µå½±": "ç”µå½±",
                                "ç”µè§†å‰§": "ç”µè§†å‰§",
                                "ç»¼è‰º": "ç»¼è‰º",
                                "æç¬‘": "æç¬‘",
                                "æƒ…æ„Ÿ": "æƒ…æ„Ÿ",
                                "ç”Ÿæ´»": "ç”Ÿæ´»",
                                "å®¶å±…": "å®¶å±…",
                                "è‚²å„¿": "è‚²å„¿",
                                "å® ç‰©": "å® ç‰©",
                                "æ‘„å½±": "æ‘„å½±",
                                "ç»˜ç”»": "ç»˜ç”»",
                                "è¯»ä¹¦": "è¯»ä¹¦",
                                "å†™ä½œ": "å†™ä½œ",
                                "èŒåœº": "èŒåœº",
                                "æ³•å¾‹": "æ³•å¾‹",
                                "æ”¿æ²»": "æ”¿æ²»",
                                "å†å²": "å†å²",
                                "æ–‡åŒ–": "æ–‡åŒ–",
                                "è‰ºæœ¯": "è‰ºæœ¯",
                                "ç§‘å­¦": "ç§‘å­¦",
                                "è‡ªç„¶": "è‡ªç„¶",
                                "ç¯ä¿": "ç¯ä¿",
                                "å…¬ç›Š": "å…¬ç›Š",
                                "å®—æ•™": "å®—æ•™",
                                "å¿ƒç†": "å¿ƒç†",
                                "æ˜Ÿåº§": "æ˜Ÿåº§",
                                "å½©ç¥¨": "å½©ç¥¨",
                                "è‚¡ç¥¨": "è‚¡ç¥¨",
                                "æˆ¿äº§": "æˆ¿äº§",
                                "åˆ›ä¸š": "åˆ›ä¸š",
                                "äº’è”ç½‘": "äº’è”ç½‘",
                                "æ‰‹æœº": "æ‰‹æœº",
                                "ç”µè„‘": "ç”µè„‘",
                                "è½¯ä»¶": "è½¯ä»¶",
                                "ç½‘ç»œ": "ç½‘ç»œ",
                                "ç”µå•†": "ç”µå•†",
                                "ç›´æ’­": "ç›´æ’­",
                                "ç½‘çº¢": "ç½‘çº¢",
                                "ç¾å¦†": "ç¾å¦†",
                                "æœé¥°": "æœé¥°",
                                "é‹åŒ…": "é‹åŒ…",
                                "ç å®": "ç å®",
                                "æ‰‹è¡¨": "æ‰‹è¡¨",
                                "å®¶å…·": "å®¶å…·",
                                "å®¶ç”µ": "å®¶ç”µ",
                                "å¨å…·": "å¨å…·",
                                "é£Ÿå“": "é£Ÿå“",
                                "é¥®æ–™": "é¥®æ–™",
                                "é…’æ°´": "é…’æ°´",
                                "çƒŸè‰": "çƒŸè‰",
                                "è¯å“": "è¯å“",
                                "åŒ»ç–—": "åŒ»ç–—",
                                "åŒ»é™¢": "åŒ»é™¢",
                                "å­¦æ ¡": "å­¦æ ¡",
                                "æ•™è‚²æœºæ„": "æ•™è‚²æœºæ„",
                                "å…¬å¸": "å…¬å¸",
                                "å·¥å‚": "å·¥å‚",
                                "å†œæ‘": "å†œæ‘",
                                "åŸå¸‚": "åŸå¸‚",
                                "äº¤é€š": "äº¤é€š",
                                "èˆªç©º": "èˆªç©º",
                                "é“è·¯": "é“è·¯",
                                "å…¬è·¯": "å…¬è·¯",
                                "æµ·è¿": "æµ·è¿",
                                "å¤©æ°”": "å¤©æ°”",
                                "åœ°éœ‡": "åœ°éœ‡",
                                "å°é£": "å°é£",
                                "æ´ªæ°´": "æ´ªæ°´",
                                "ç«ç¾": "ç«ç¾",
                                "äº‹æ•…": "äº‹æ•…",
                                "çŠ¯ç½ª": "çŠ¯ç½ª",
                                "è­¦å¯Ÿ": "è­¦å¯Ÿ",
                                "æ³•é™¢": "æ³•é™¢",
                                "ç›‘ç‹±": "ç›‘ç‹±",
                                "æ­»äº¡": "æ­»äº¡",
                                "å‡ºç”Ÿ": "å‡ºç”Ÿ",
                                "ç»“å©š": "ç»“å©š",
                                "ç¦»å©š": "ç¦»å©š",
                                "æ‹çˆ±": "æ‹çˆ±",
                                "åˆ†æ‰‹": "åˆ†æ‰‹",
                                "æ±‚å©š": "æ±‚å©š",
                                "å©šç¤¼": "å©šç¤¼",
                                "ç”Ÿæ—¥": "ç”Ÿæ—¥",
                                "èŠ‚æ—¥": "èŠ‚æ—¥",
                                "æ˜¥èŠ‚": "æ˜¥èŠ‚",
                                "ä¸­ç§‹": "ä¸­ç§‹",
                                "ç«¯åˆ": "ç«¯åˆ",
                                "æ¸…æ˜": "æ¸…æ˜",
                                "å›½åº†": "å›½åº†",
                                "å…ƒæ—¦": "å…ƒæ—¦",
                                "åœ£è¯": "åœ£è¯",
                            }

                            # æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«ä»»ä½•åˆ†ç±»å…³é”®è¯
                            for keyword, cat in category_keywords.items():
                                if keyword in text:
                                    category = cat
                                    break

                    # åˆ›å»ºHotItemå¯¹è±¡ï¼ˆç¡®ä¿æœ‰æœ‰æ•ˆæ•°æ®ï¼‰
                    if rank > 0 and title:  # åŸºæœ¬éªŒè¯
                        hot_item = HotItem(
                            rank=rank,
                            title=title,
                            category=category,
                            heat=heat,
                            reads=reads,
                            discussions=discussions,
                            originals=originals,
                            date=date,
                        )
                        hot_items.append(hot_item)
                    else:
                        self.logger.debug(f"è·³è¿‡æ— æ•ˆæ¡ç›®: rank={rank}, title={title}")

                except Exception as e:
                    self.logger.warning(f"è§£æå•ä¸ªçƒ­æœæ¡ç›®å¤±è´¥: {e}")
                    continue

            # æŒ‰æ’åæ’åº
            hot_items.sort(key=lambda x: x.rank)

            self.logger.info(f"æˆåŠŸè§£æ {len(hot_items)} ä¸ªçƒ­æœæ¡ç›®")

            # å¦‚æœæ²¡æœ‰è§£æåˆ°ä»»ä½•æ¡ç›®ï¼Œå¯èƒ½æ˜¯é¡µé¢ç»“æ„ä¸åŒï¼Œå°è¯•å¤‡ç”¨è§£ææ–¹æ³•
            if len(hot_items) == 0:
                self.logger.warning("ä½¿ç”¨ä¸»è§£ææ–¹æ³•æœªæ‰¾åˆ°æ•°æ®ï¼Œå°è¯•å¤‡ç”¨è§£ææ–¹æ³•...")
                hot_items = self._parse_page_backup(soup, date)

        except Exception as e:
            self.logger.error(f"è§£æé¡µé¢å¤±è´¥: {e}")

        return hot_items

    def save_data(self, data: List[HotItem], date: str) -> bool:
        """
        ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶

        å‚æ•°ï¼š
            data: çƒ­æœæ¡ç›®åˆ—è¡¨
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸ºYYYY-MM-DD

        è¿”å›ï¼š
            ä¿å­˜æˆåŠŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        try:
            # è§£ææ—¥æœŸï¼Œåˆ›å»ºæœˆä»½ç›®å½•
            date_obj = datetime.strptime(date, "%Y-%m-%d")
            year_month = date_obj.strftime("%Y-%m")

            # åˆ›å»ºæœˆä»½ç›®å½•
            month_dir = os.path.join(self.output_dir, year_month)
            os.makedirs(month_dir, exist_ok=True)

            # æ„å»ºæ–‡ä»¶è·¯å¾„
            filename = f"{date}.json"
            filepath = os.path.join(month_dir, filename)

            # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            data_dicts = [asdict(item) for item in data]

            # ä¿å­˜ä¸ºJSONæ–‡ä»¶
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(
                    {"date": date, "count": len(data), "data": data_dicts},
                    f,
                    ensure_ascii=False,
                    indent=2,
                )

            self.logger.info(f"æ•°æ®å·²ä¿å­˜åˆ° {filepath}")
            return True

        except Exception as e:
            self.logger.error(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")
            return False

    def scrape_date_with_html(self, date: str, html: str) -> bool:
        """
        ä½¿ç”¨æä¾›çš„HTMLçˆ¬å–å¹¶ä¿å­˜æŒ‡å®šæ—¥æœŸçš„æ•°æ®ï¼ˆç”¨äºæµ‹è¯•ï¼‰

        å‚æ•°ï¼š
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸ºYYYY-MM-DD
            html: é¡µé¢HTMLå­—ç¬¦ä¸²

        è¿”å›ï¼š
            çˆ¬å–æˆåŠŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        self.logger.info(f"å¼€å§‹è§£æ {date} çš„æ•°æ®...")

        # è§£æé¡µé¢
        hot_items = self.parse_page(html, date)
        if not hot_items:
            self.logger.warning(f"{date} æ²¡æœ‰è§£æåˆ°çƒ­æœæ•°æ®")

        # ä¿å­˜æ•°æ®
        success = self.save_data(hot_items, date)

        if success:
            self.logger.info(f"æˆåŠŸå®Œæˆ {date} çš„æ•°æ®è§£æ")
        else:
            self.logger.error(f"{date} çš„æ•°æ®è§£æå¤±è´¥")

        return success

    def scrape_date(self, date: str) -> bool:
        """
        çˆ¬å–å¹¶ä¿å­˜æŒ‡å®šæ—¥æœŸçš„æ•°æ®

        å‚æ•°ï¼š
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸ºYYYY-MM-DD

        è¿”å›ï¼š
            çˆ¬å–æˆåŠŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        self.logger.info(f"å¼€å§‹çˆ¬å– {date} çš„æ•°æ®...")

        # è·å–é¡µé¢HTML
        html = self.fetch_page(date)
        if not html:
            self.logger.error(f"æ— æ³•è·å– {date} çš„é¡µé¢")
            return False

        # ç­‰å¾…å»¶è¿Ÿ
        time.sleep(self.delay)

        # è§£æé¡µé¢
        hot_items = self.parse_page(html, date)
        if not hot_items:
            self.logger.warning(f"{date} æ²¡æœ‰è§£æåˆ°çƒ­æœæ•°æ®")
            # ä»ç„¶å°è¯•ä¿å­˜ç©ºæ•°æ®ä»¥è®°å½•è¯¥æ—¥æœŸå·²è¢«å¤„ç†

        # ä¿å­˜æ•°æ®
        success = self.save_data(hot_items, date)

        if success:
            self.logger.info(f"æˆåŠŸå®Œæˆ {date} çš„æ•°æ®çˆ¬å–")
        else:
            self.logger.error(f"{date} çš„æ•°æ®çˆ¬å–å¤±è´¥")

        return success

    def scrape_range(self, start_date: str, end_date: str) -> Dict[str, Any]:
        """
        çˆ¬å–æŒ‡å®šæ—¥æœŸèŒƒå›´çš„æ•°æ®

        å‚æ•°ï¼š
            start_date: å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYY-MM-DD
            end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYY-MM-DD

        è¿”å›ï¼š
            åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        stats = {"total_dates": 0, "successful": 0, "failed": 0, "failed_dates": []}

        try:
            # è§£ææ—¥æœŸèŒƒå›´
            start = datetime.strptime(start_date, "%Y-%m-%d")
            end = datetime.strptime(end_date, "%Y-%m-%d")

            # ç”Ÿæˆæ—¥æœŸåˆ—è¡¨
            current = start
            date_list = []

            while current <= end:
                date_list.append(current.strftime("%Y-%m-%d"))
                current += timedelta(days=1)

            stats["total_dates"] = len(date_list)
            self.logger.info(f"éœ€è¦çˆ¬å– {len(date_list)} å¤©çš„æ•°æ®")

            # éå†æ—¥æœŸå¹¶çˆ¬å–
            for i, date in enumerate(date_list, 1):
                self.logger.info(f"è¿›åº¦: {i}/{len(date_list)} ({date})")

                try:
                    success = self.scrape_date(date)

                    if success:
                        stats["successful"] += 1
                    else:
                        stats["failed"] += 1
                        stats["failed_dates"].append(date)

                except Exception as e:
                    self.logger.error(f"çˆ¬å– {date} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
                    stats["failed"] += 1
                    stats["failed_dates"].append(date)

                # åœ¨æ—¥æœŸä¹‹é—´æ·»åŠ é¢å¤–å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                if i < len(date_list):
                    time.sleep(self.delay * 0.5)

            # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
            self.logger.info(
                f"çˆ¬å–å®Œæˆï¼æˆåŠŸ: {stats['successful']}, å¤±è´¥: {stats['failed']}"
            )
            if stats["failed_dates"]:
                self.logger.warning(f"å¤±è´¥çš„æ—¥æœŸ: {stats['failed_dates']}")

        except ValueError as e:
            self.logger.error(f"æ—¥æœŸæ ¼å¼é”™è¯¯: {e}")
        except Exception as e:
            self.logger.error(f"çˆ¬å–èŒƒå›´æ•°æ®æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

        return stats

    def _parse_page_backup(self, soup: BeautifulSoup, date: str) -> List[HotItem]:
        """
        å¤‡ç”¨è§£ææ–¹æ³•ï¼Œç”¨äºå¤„ç†ä¸åŒçš„é¡µé¢ç»“æ„

        å‚æ•°ï¼š
            soup: BeautifulSoupå¯¹è±¡
            date: æ—¥æœŸå­—ç¬¦ä¸²

        è¿”å›ï¼š
            çƒ­æœæ¡ç›®åˆ—è¡¨
        """
        hot_items = []

        try:
            # å¤‡ç”¨æ–¹æ³•ï¼šæŸ¥æ‰¾æ‰€æœ‰åŒ…å«çƒ­æœæ•°æ®çš„å®¹å™¨
            # æœ‰äº›é¡µé¢å¯èƒ½æœ‰ä¸åŒçš„ç»“æ„
            containers = soup.find_all("div", class_="rounded-lg")

            for container in containers:
                try:
                    # å°è¯•ä¸åŒçš„é€‰æ‹©å™¨ç»„åˆ
                    h2_tag = container.find("h2", class_="text-xl")
                    if not h2_tag:
                        continue

                    rank, title = self._parse_rank(h2_tag)

                    # æŸ¥æ‰¾æ•°æ®
                    data_div = container.find("div", class_="flex")
                    if not data_div:
                        continue

                    data_tags = data_div.find_all("div", class_="inline-flex")

                    # åˆå§‹åŒ–æ•°æ®å­—æ®µ
                    category = ""
                    heat = 0.0
                    reads = 0.0
                    discussions = 0.0
                    originals = 0.0

                    for data_tag in data_tags:
                        text = data_tag.get_text(strip=True)

                        if (
                            "æ˜æ˜Ÿ" in text
                            or "ç¤¾ä¼š" in text
                            or "å¨±ä¹" in text
                            or "ä½“è‚²" in text
                        ):
                            category = text
                        elif "ğŸ”¥" in text:
                            heat = self._extract_number(text.replace("ğŸ”¥", ""))
                        elif "é˜…è¯»" in text:
                            reads = self._extract_number(text.replace("é˜…è¯»", ""))
                        elif "è®¨è®º" in text:
                            discussions = self._extract_number(text.replace("è®¨è®º", ""))
                        elif "åŸåˆ›" in text:
                            originals = self._extract_number(text.replace("åŸåˆ›", ""))
                        elif "æ¸¸æˆ" in text:
                            category = "æ¸¸æˆ"
                        elif "ç¾é£Ÿ" in text:
                            category = "ç¾é£Ÿ"
                        elif "è´¢ç»" in text:
                            category = "è´¢ç»"

                    if rank > 0 and title:
                        hot_item = HotItem(
                            rank=rank,
                            title=title,
                            category=category,
                            heat=heat,
                            reads=reads,
                            discussions=discussions,
                            originals=originals,
                            date=date,
                        )
                        hot_items.append(hot_item)

                except Exception as e:
                    self.logger.debug(f"å¤‡ç”¨è§£ææ–¹æ³•å¤„ç†å•ä¸ªæ¡ç›®å¤±è´¥: {e}")
                    continue

            self.logger.info(f"å¤‡ç”¨æ–¹æ³•è§£æåˆ° {len(hot_items)} ä¸ªçƒ­æœæ¡ç›®")

        except Exception as e:
            self.logger.error(f"å¤‡ç”¨è§£ææ–¹æ³•å¤±è´¥: {e}")

        return hot_items

    def test_parse(self, date: str = "2024-12-13") -> None:
        """
        æµ‹è¯•è§£æåŠŸèƒ½

        å‚æ•°ï¼š
            date: æµ‹è¯•æ—¥æœŸ
        """
        print(f"æµ‹è¯•è§£æåŠŸèƒ½ - æ—¥æœŸ: {date}")
        print("=" * 60)

        # è·å–é¡µé¢HTML
        html = self.fetch_page(date)
        if not html:
            print(f"æ— æ³•è·å– {date} çš„é¡µé¢")
            return

        # è§£æé¡µé¢
        hot_items = self.parse_page(html, date)

        # æ˜¾ç¤ºå‰5ä¸ªç»“æœ
        print(f"å…±è§£æåˆ° {len(hot_items)} ä¸ªçƒ­æœæ¡ç›®")
        print("\nå‰5ä¸ªçƒ­æœæ¡ç›®:")
        print("-" * 60)

        for i, item in enumerate(hot_items[:5]):
            print(f"{i + 1}. æ’å: {item.rank}")
            print(f"   æ ‡é¢˜: {item.title}")
            print(f"   åˆ†ç±»: {item.category}")
            print(f"   çƒ­åº¦: {item.heat}ä¸‡")
            print(f"   é˜…è¯»: {item.reads}ä¸‡")
            print(f"   è®¨è®º: {item.discussions}ä¸‡")
            print(f"   åŸåˆ›: {item.originals}ä¸‡")
            print()

        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        if hot_items:
            ranks = [item.rank for item in hot_items]
            print(f"æ’åèŒƒå›´: {min(ranks)} - {max(ranks)}")
            print(
                f"å¹³å‡çƒ­åº¦: {sum(item.heat for item in hot_items) / len(hot_items):.2f}ä¸‡"
            )
            print(f"æ€»é˜…è¯»é‡: {sum(item.reads for item in hot_items):.2f}ä¸‡")
            print(f"æ€»è®¨è®ºé‡: {sum(item.discussions for item in hot_items):.2f}ä¸‡")

        print("=" * 60)


def main_realtime():
    """
    å®æ—¶çƒ­æœæ•°æ®è·å–å·¥å…·
    
    åŠŸèƒ½ï¼š
        åœ¨ Streamlit å¤–ç‹¬ç«‹è¿è¡Œï¼Œè·å–å®æ—¶çƒ­æœæ•°æ®å¹¶ä¿å­˜åˆ°æ–‡ä»¶
    """
    print("=" * 60)
    print("å®æ—¶çƒ­æœæ•°æ®è·å–å·¥å…·")
    print("=" * 60)
    
    output_file = "output/realtime_hot.json"
    
    print(f"\næ­£åœ¨è·å–å®æ—¶çƒ­æœ...")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
    print("-" * 60)
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    scraper = RealtimeHotScraper(
        timeout=15,
        max_retries=3,
        delay=1.0
    )
    
    # ä½¿ç”¨ fetch_and_save æ–¹æ³•
    success = scraper.fetch_and_save(output_file)
    
    if success:
        # è¯»å–ä¿å­˜çš„æ•°æ®æ¥æ˜¾ç¤º
        try:
            with open(output_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            hot_items = data.get('data', [])
            
            print(f"\nâœ“ æˆåŠŸè·å– {len(hot_items)} ä¸ªçƒ­æœ")
            print(f"âœ“ æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
            print(f"âœ“ æ›´æ–°æ—¶é—´: {data.get('timestamp', 'unknown')}")
            
            print("\nå‰ 10 ä¸ªçƒ­æœï¼š")
            print("-" * 60)
            for item in hot_items[:10]:
                print(f"{item['rank']:2d}. {item['title']}")
            
            if len(hot_items) > 10:
                print(f"... ä»¥åŠ {len(hot_items) - 10} ä¸ªæ›´å¤š")
            
            print("\n" + "=" * 60)
            print("æç¤ºï¼š")
            print("- å¯ä»¥å°†æ­¤è„šæœ¬æ·»åŠ åˆ°å®šæ—¶ä»»åŠ¡ä¸­å®šæœŸæ›´æ–°æ•°æ®")
            print("- Windows: ä½¿ç”¨ä»»åŠ¡è®¡åˆ’ç¨‹åº")
            print("- Linux/Mac: ä½¿ç”¨ crontab")
            print("- ç¤ºä¾‹ (æ¯å°æ—¶): 0 * * * * cd /path/to/fyknbb && python src/scrap.py realtime")
            print("=" * 60)
            
            return 0
        except Exception as e:
            print(f"\nâœ— è¯»å–ä¿å­˜çš„æ•°æ®å¤±è´¥: {e}")
            return 1
    else:
        print("\nâœ— æœªèƒ½è·å–çƒ­æœæ•°æ®")
        print("\nå¯èƒ½çš„åŸå› ï¼š")
        print("1. ç½‘ç»œè¿æ¥é—®é¢˜")
        print("2. å¾®åšè®¿å®¢éªŒè¯ï¼ˆæœ€å¸¸è§ï¼‰")
        print("3. é¡µé¢ç»“æ„å˜åŒ–")
        
        print("\nå»ºè®®ï¼š")
        print("1. ç¡®ä¿å·²å®‰è£… Playwright:")
        print("   pip install playwright")
        print("   python -m playwright install chromium")
        print("2. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("3. ç¨åé‡è¯•")
        
        print("\n" + "=" * 60)
        return 1


def main():
    """
    ä¸»å‡½æ•°ï¼Œç”¨äºæµ‹è¯•å’Œæ¼”ç¤ºçˆ¬è™«åŠŸèƒ½

    åŠŸèƒ½ï¼š
        1. åˆ›å»ºçˆ¬è™«å®ä¾‹
        2. çˆ¬å–æŒ‡å®šæ—¥æœŸèŒƒå›´çš„å¾®åšçƒ­æœæ•°æ®
        3. å°†æ•°æ®ä¿å­˜åˆ°dataç›®å½•
    """
    print("å¾®åšçƒ­æœå†å²æ•°æ®çˆ¬è™«")
    print("=" * 60)
    print("ä½¿ç”¨æ–¹æ³•:")
    print("  1. python src/scrap.py                # è¿è¡Œå®Œæ•´çˆ¬å–")
    print("  2. python src/scrap.py test          # æµ‹è¯•è§£æåŠŸèƒ½")
    print("  3. python src/scrap.py single <date> # çˆ¬å–å•æ—¥æ•°æ®")
    print("  4. python src/scrap.py realtime      # è·å–å®æ—¶çƒ­æœ")
    print("=" * 60)

    import sys

    try:
        # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
        if len(sys.argv) > 1:
            if sys.argv[1] == "realtime":
                # å®æ—¶çƒ­æœæ¨¡å¼
                return main_realtime()
            elif sys.argv[1] == "test":
                # æµ‹è¯•æ¨¡å¼
                print("è¿è¡Œæµ‹è¯•æ¨¡å¼...")
                test_date = sys.argv[2] if len(sys.argv) > 2 else "2024-12-13"
                
                # åˆ›å»ºçˆ¬è™«å®ä¾‹
                scraper = WeiboHotScraper(
                    output_dir="data",
                    delay=2.0,
                    max_retries=3,
                )
                scraper.test_parse(test_date)
                return
            elif sys.argv[1] == "single" and len(sys.argv) > 2:
                # å•æ—¥çˆ¬å–æ¨¡å¼
                date = sys.argv[2]
                print(f"çˆ¬å–å•æ—¥æ•°æ®: {date}")
                
                # åˆ›å»ºçˆ¬è™«å®ä¾‹
                scraper = WeiboHotScraper(
                    output_dir="data",
                    delay=2.0,
                    max_retries=3,
                )
                success = scraper.scrape_date(date)
                if success:
                    print(f"æˆåŠŸçˆ¬å– {date} çš„æ•°æ®")
                else:
                    print(f"çˆ¬å– {date} çš„æ•°æ®å¤±è´¥")
                return
            elif sys.argv[1] == "help":
                print("å¸®åŠ©ä¿¡æ¯:")
                print("  test [date]    - æµ‹è¯•è§£æåŠŸèƒ½ï¼Œå¯é€‰æ—¥æœŸå‚æ•°")
                print("  single <date>  - çˆ¬å–å•æ—¥æ•°æ®")
                print("  realtime       - è·å–å®æ—¶çƒ­æœ")
                print("  help          - æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯")
                return

        # é»˜è®¤æ¨¡å¼ï¼šå®Œæ•´çˆ¬å–
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        scraper = WeiboHotScraper(
            output_dir="data",
            delay=2.0,  # é€‚å½“å»¶è¿Ÿï¼Œé¿å…è¢«å°IP
            max_retries=3,
        )
        
        # è®¾ç½®æ—¥æœŸèŒƒå›´ï¼ˆæ ¹æ®ç”¨æˆ·è¦æ±‚ï¼‰
        start_date = "2024-05-20"
        end_date = "2024-12-31"

        print(f"å¼€å§‹çˆ¬å– {start_date} åˆ° {end_date} çš„æ•°æ®...")
        print("æ•°æ®å°†ä¿å­˜åˆ° data/ ç›®å½•ä¸‹ï¼ŒæŒ‰æœˆä»½ç»„ç»‡")
        print("æ³¨æ„ï¼šè¿™éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
        print("=" * 60)

        # å¼€å§‹çˆ¬å–
        stats = scraper.scrape_range(start_date, end_date)

        # è¾“å‡ºç»“æœ
        print("\nçˆ¬å–å®Œæˆï¼")
        print(f"æ€»å¤©æ•°: {stats['total_dates']}")
        print(f"æˆåŠŸ: {stats['successful']}")
        print(f"å¤±è´¥: {stats['failed']}")

        if stats["failed_dates"]:
            print(f"å¤±è´¥çš„æ—¥æœŸ: {', '.join(stats['failed_dates'][:5])}")
            if len(stats["failed_dates"]) > 5:
                print(f"... ä»¥åŠ {len(stats['failed_dates']) - 5} ä¸ªæ›´å¤š")

        print(f"\næ•°æ®å·²ä¿å­˜åˆ° {scraper.output_dir}/ ç›®å½•")

    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­ï¼Œçˆ¬è™«å·²åœæ­¢")
    except Exception as e:
        print(f"ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        import traceback

        traceback.print_exc()


class RealtimeHotScraper:
    """
    å¾®åšå®æ—¶çƒ­æœçˆ¬è™«

    åŠŸèƒ½ï¼š
        - çˆ¬å–å¾®åšå®æ—¶çƒ­æœ Top 50
        - è§£ææ’åã€æ ‡é¢˜ã€çƒ­åº¦
        - æ”¯æŒç¼“å­˜å’Œé”™è¯¯å¤„ç†

    æ•°æ®ç»“æ„ï¼š
        {
            "rank": 3,
            "title": "æ­å¼€æ—¥æœ¬ä¼ªè£…å—å®³è€…çš„çœŸé¢ç›®"
        }
    """

    def __init__(
        self,
        timeout: int = 10,
        max_retries: int = 3,
        delay: float = 1.0,
    ):
        """
        åˆå§‹åŒ–å®æ—¶çƒ­æœçˆ¬è™«

        å‚æ•°ï¼š
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            delay: è¯·æ±‚é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰
        """
        # é»˜è®¤ä½¿ç”¨æ¦œå•é¡µï¼ˆæ›´ç¨³å®šã€æ— éœ€ç™»å½•ï¼‰
        self.base_url = "https://s.weibo.com/top/summary?cate=realtimehot"
        self.timeout = timeout
        self.max_retries = max_retries
        self.delay = delay

        # é…ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        )
        self.logger = logging.getLogger(__name__)

        # é…ç½®requestsä¼šè¯
        self.session = requests.Session()
        # æ›´å®Œæ•´çš„ User-Agent åˆ—è¡¨ï¼ˆæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨ï¼‰
        user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
        ]
        import random
        chosen_ua = random.choice(user_agents)
        
        self.session.headers.update(
            {
                "User-Agent": chosen_ua,
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8,en-US;q=0.7",
                "Accept-Encoding": "gzip, deflate, br, zstd",
                "Cache-Control": "max-age=0",
                "Sec-Ch-Ua": '"Chromium";v="131", "Not_A Brand";v="24"',
                "Sec-Ch-Ua-Mobile": "?0",
                "Sec-Ch-Ua-Platform": '"Windows"',
                "Sec-Fetch-Dest": "document",
                "Sec-Fetch-Mode": "navigate",
                "Sec-Fetch-Site": "none",
                "Sec-Fetch-User": "?1",
                "Upgrade-Insecure-Requests": "1",
                "DNT": "1",
                "Connection": "keep-alive",
            }
        )
        
        # åˆå§‹åŒ– sessionï¼šå…ˆè®¿é—®å¾®åšé¦–é¡µå»ºç«‹ä¼šè¯
        self._init_session()

    def _init_session(self):
        """åˆå§‹åŒ–ä¼šè¯ï¼šè®¿é—®å¾®åšé¦–é¡µè·å–å¿…è¦çš„ cookie"""
        try:
            self.logger.info("æ­£åœ¨åˆå§‹åŒ–ä¼šè¯...")
            # å…ˆè®¿é—®å¾®åšé¦–é¡µ
            init_url = "https://weibo.com"
            response = self.session.get(init_url, timeout=10, allow_redirects=True)
            
            if response.status_code == 200:
                self.logger.info(f"ä¼šè¯åˆå§‹åŒ–æˆåŠŸï¼Œè·å¾— {len(self.session.cookies)} ä¸ª cookie")
                # æ·»åŠ ä¸€äº›å¸¸ç”¨çš„ cookieï¼ˆå¦‚æœæ²¡æœ‰çš„è¯ï¼‰
                if 'SUB' not in self.session.cookies:
                    self.session.cookies.update({
                        'WEIBOCN_FROM': 'feed',
                        'MLOGIN': '0',
                    })
            else:
                self.logger.warning(f"ä¼šè¯åˆå§‹åŒ–å¤±è´¥ï¼šçŠ¶æ€ç  {response.status_code}")
            
            # çŸ­æš‚å»¶è¿Ÿ
            time.sleep(0.5)
        except Exception as e:
            self.logger.warning(f"ä¼šè¯åˆå§‹åŒ–å‡ºé”™ï¼ˆç»§ç»­å°è¯•ï¼‰: {e}")

    def fetch_realtime_page(self) -> Optional[str]:
        """
        è·å–å¾®åšå®æ—¶çƒ­æœé¡µé¢ HTML

        å°è¯•å¤šä¸ªç­–ç•¥ï¼š
        1. ä½¿ç”¨Cookieå’Œé«˜çº§å¤´éƒ¨è®¿é—®æ¦œå•é¡µ
        2. å°è¯•çƒ­æœé¡µé¢
        3. ç›´æ¥è®¿é—®å¾®åšé¦–é¡µ

        è¿”å›ï¼š
            HTML å­—ç¬¦ä¸²æˆ– Noneï¼ˆå¦‚æœè¯·æ±‚å¤±è´¥ï¼‰
        """
        # å°è¯•çš„ URL åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
        urls_to_try = [
            # ç­–ç•¥1ï¼šå¸¦å®Œæ•´Headerçš„æ¦œå•é¡µ
            ("https://s.weibo.com/top/summary?cate=realtimehot", "complete_headers"),
            # ç­–ç•¥2ï¼šçƒ­æœé¡µé¢
            ("https://weibo.com/hot/search", "standard"),
            # ç­–ç•¥3ï¼šå¾®åšé¦–é¡µ
            ("https://weibo.com/", "standard"),
        ]

        for url_idx, (url, header_type) in enumerate(urls_to_try, 1):
            for attempt in range(self.max_retries):
                try:
                    self.logger.info(f"[ç­–ç•¥ {url_idx}] æ­£åœ¨è·å–å®æ—¶çƒ­æœé¡µé¢ (å°è¯• {attempt + 1}/{self.max_retries})")
                    self.logger.debug(f"ç›®æ ‡ URL: {url}")
                    
                    # æ ¹æ®ç­–ç•¥ç±»å‹è®¾ç½®ä¸åŒçš„å¤´éƒ¨
                    if header_type == "complete_headers":
                        # å®Œæ•´çš„æµè§ˆå™¨å¤´éƒ¨ï¼Œæ¨¡æ‹ŸçœŸå®ç”¨æˆ·
                        headers = {
                            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0",
                            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
                            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8,en-US;q=0.7",
                            "Accept-Encoding": "gzip, deflate, br",
                            "Sec-Fetch-Dest": "document",
                            "Sec-Fetch-Mode": "navigate",
                            "Sec-Fetch-Site": "none",
                            "Sec-Fetch-User": "?1",
                            "Upgrade-Insecure-Requests": "1",
                            "Cache-Control": "max-age=0",
                        }
                        self.session.headers.update(headers)
                    
                    response = self.session.get(
                        url, 
                        timeout=self.timeout, 
                        allow_redirects=True,
                        verify=True
                    )
                    response.raise_for_status()

                    # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘åˆ°è®¿å®¢éªŒè¯é¡µé¢
                    if "passport.weibo.com/visitor" in response.url:
                        self.logger.warning(f"[ç­–ç•¥ {url_idx}] è¢«é‡å®šå‘åˆ°è®¿å®¢éªŒè¯é¡µé¢ï¼Œéœ€è¦Playwright")
                        continue

                    if response.status_code == 200:
                        html_text = response.text
                        self.logger.info(f"[ç­–ç•¥ {url_idx}] æˆåŠŸè·å–é¡µé¢ (æœ€ç»ˆ URL: {response.url}, å¤§å°: {len(html_text)} bytes)")
                        
                        # æ£€æŸ¥é¡µé¢å¤§å°
                        if len(html_text) < 5000:
                            self.logger.warning(f"[ç­–ç•¥ {url_idx}] é¡µé¢å†…å®¹å¤ªçŸ­ï¼Œå¯èƒ½æ— æ•ˆ")
                            continue
                        
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«è®¿å®¢éªŒè¯
                        html_lower = html_text.lower()
                        if "visitor" in html_lower and "passport" in html_lower:
                            self.logger.warning(f"[ç­–ç•¥ {url_idx}] é¡µé¢åŒ…å«è®¿å®¢éªŒè¯å†…å®¹")
                            continue
                        
                        # æ£€æŸ¥çƒ­æœç›¸å…³å†…å®¹
                        has_hot_content = (
                            "pl_top_realtimehot" in html_text
                            or "td-02" in html_text
                            or ("çƒ­æœ" in html_text and "æ’è¡Œ" in html_text)
                        )
                        
                        if has_hot_content:
                            self.logger.info(f"[ç­–ç•¥ {url_idx}] é¡µé¢åŒ…å«çƒ­æœå†…å®¹ï¼Œè¿”å›")
                            return html_text
                        else:
                            self.logger.warning(f"[ç­–ç•¥ {url_idx}] é¡µé¢æ— çƒ­æœå†…å®¹")
                    else:
                        self.logger.warning(f"[ç­–ç•¥ {url_idx}] çŠ¶æ€ç å¼‚å¸¸: {response.status_code}")

                except requests.exceptions.Timeout:
                    self.logger.warning(f"[ç­–ç•¥ {url_idx}] è¶…æ—¶ (å°è¯• {attempt + 1}/{self.max_retries})")
                    if attempt < self.max_retries - 1:
                        time.sleep(2 ** attempt)
                        
                except requests.exceptions.RequestException as e:
                    self.logger.error(f"[ç­–ç•¥ {url_idx}] è¯·æ±‚å¤±è´¥: {e}")
                    if attempt < self.max_retries - 1:
                        time.sleep(2 ** attempt)
                    
                except Exception as e:
                    self.logger.error(f"[ç­–ç•¥ {url_idx}] æœªçŸ¥é”™è¯¯: {e}")
                    break
        
        self.logger.error("æ‰€æœ‰HTTPç­–ç•¥éƒ½å¤±è´¥ï¼Œå¿…é¡»ä½¿ç”¨Playwright")
        return None

    def parse_realtime_page(self, html: str) -> List[Dict[str, Any]]:
        """
        è§£æå®æ—¶çƒ­æœé¡µé¢ï¼ˆhttps://weibo.com/hot/searchï¼‰

        æ”¯æŒå¤šç§çƒ­æœé¡¹ç»“æ„ï¼Œä¼˜å…ˆçº§ï¼š
        1. ALink_none_1w6rm é“¾æ¥ï¼ˆä¸»è¦çƒ­æœï¼‰
        2. æ­£åˆ™åŒ¹é…çš„æœç´¢é“¾æ¥ï¼ˆæ¬¡è¦çƒ­æœï¼‰
        3. å…¶ä»–å¯èƒ½çš„å®¹å™¨

        å‚æ•°ï¼š
            html: é¡µé¢ HTML å­—ç¬¦ä¸²

        è¿”å›ï¼š
            çƒ­æœæ¡ç›®åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« rankã€title
        """
        hot_items = []

        try:
            soup = BeautifulSoup(html, "html.parser")

            # è°ƒè¯•ï¼šæ£€æŸ¥ HTML ç»“æ„
            self.logger.debug(f"HTML é•¿åº¦: {len(html)}")
            
            # æ–¹æ¡ˆ1ï¼šè§£ææ¦œå•è¡¨æ ¼ï¼ˆs.weibo.com/top/summary é¡µé¢ç»“æ„ï¼‰
            table_container = soup.find(id=re.compile(r"pl_top_realtimehot")) or soup
            rows = []
            # ä¸¤ç§å¸¸è§ç»“æ„ï¼štbody > tr æˆ–è€…ç›´æ¥å¤šä¸ª tr
            tbody = table_container.find("tbody")
            if tbody:
                rows = tbody.find_all("tr")
            else:
                rows = table_container.find_all("tr")

            self.logger.info(f"æ–¹æ¡ˆ1: è¡¨æ ¼è¡Œæ•°é‡ {len(rows)}")

            rank_counter = 0
            for tr in rows:
                try:
                    title_td = tr.find("td", class_=re.compile(r"td-02"))

                    if not title_td:
                        continue

                    # è·³è¿‡è¡¨å¤´æˆ–ç½®é¡¶ï¼ˆæ— æœ‰æ•ˆæ’åï¼‰
                    tr_class = (tr.get("class") or [])
                    if any(cls for cls in tr_class if re.search(r"thead|top|ad|banner", cls)):
                        continue

                    a = title_td.find("a")
                    if not a:
                        continue

                    title = a.get_text(strip=True).replace("#", "").strip()
                    if not title:
                        continue

                    rank_counter += 1

                    hot_items.append({"rank": rank_counter, "title": title})
                except Exception as e:
                    self.logger.debug(f"è§£æè¡¨æ ¼è¡Œå¤±è´¥: {e}")
                    continue

            # æ–¹æ¡ˆ2ï¼šå¦‚æœä¸Šé¢ä¸è¶³ï¼Œå…œåº•è§£æå…¶å®ƒå¯èƒ½ç»“æ„ï¼ˆå¦‚å¤‡ç”¨é“¾æ¥ï¼‰
            if len(hot_items) < 20:
                self.logger.info("ä¸»è¡¨æ ¼ä¸è¶³ï¼Œå°è¯•å…œåº•è§£æå€™é€‰é“¾æ¥â€¦")
                import re as _re
                extra_links = soup.find_all("a", href=_re.compile(r"s\.weibo\.com/weibo"))
                seen = {i["title"] for i in hot_items}
                for lk in extra_links:
                    try:
                        t = lk.get_text(strip=True).replace("#", "").strip()
                        if not t or t in seen:
                            continue
                        if len(t) < 2 or len(t) > 100:
                            continue
                        hot_items.append({"rank": len(hot_items) + 1, "title": t})
                        seen.add(t)
                        if len(hot_items) >= 50:
                            break
                    except Exception:
                        continue

            # æŒ‰æ’åæ’åºï¼ˆç¡®ä¿é¡ºåºä¸é¡µé¢ä¸€è‡´ï¼‰
            if hot_items:
                hot_items.sort(key=lambda x: (isinstance(x["rank"], int), x["rank"]))

            # å»é‡å’Œæ¸…ç†ï¼ˆç§»é™¤é‡å¤æˆ–ä½è´¨é‡çš„é¡¹ï¼‰ï¼Œå¹¶é‡æ–°è¿ç»­ç¼–å·
            cleaned_items = []
            seen_titles = set()
            for item in hot_items:
                title = item["title"]

                digit_count = sum(1 for c in title if c.isdigit())
                if digit_count > len(title) / 3:
                    continue

                if title in seen_titles:
                    continue

                cleaned_items.append(item)
                seen_titles.add(title)

            # é‡æ–°æŒ‰ç…§å‡ºç°é¡ºåºè¿ç»­èµ‹ rankï¼Œé¿å…è·³å·
            for idx, item in enumerate(cleaned_items, start=1):
                item["rank"] = idx

            hot_items = cleaned_items

            self.logger.info(f"æˆåŠŸè§£æ {len(hot_items)} ä¸ªçƒ­æœæ¡ç›®")

        except Exception as e:
            self.logger.error(f"è§£æé¡µé¢å¤±è´¥: {e}")

        return hot_items

        return hot_items

    def _extract_heat(self, container) -> str:
        """
        ä»å®¹å™¨ä¸­æå–çƒ­åº¦ä¿¡æ¯

        å‚æ•°ï¼š
            container: BeautifulSoup çš„ div å®¹å™¨å¯¹è±¡

        è¿”å›ï¼š
            çƒ­åº¦å­—ç¬¦ä¸²ï¼ˆå¦‚ "æ²¸"ã€"çˆ†"ã€æ•°å­—ç­‰ï¼‰
        """
        # å°è¯•ä»ä¸åŒä½ç½®æŸ¥æ‰¾çƒ­åº¦æ ‡ç­¾
        # æ ¹æ®å®é™…é¡µé¢ç»“æ„è°ƒæ•´
        heat_spans = container.find_all("span", class_=lambda x: x and "Heat" in x if x else False)

        if heat_spans:
            return heat_spans[0].get_text(strip=True)

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾åŒ…å«çƒ­åº¦çš„å…¶ä»–å…ƒç´ 
        # è¿™é‡Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥è°ƒæ•´
        return "N/A"

    def load_from_file(self, file_path: str = "output/realtime_hot.json") -> Optional[List[Dict[str, Any]]]:
        """
        ä»æ–‡ä»¶åŠ è½½å®æ—¶çƒ­æœæ•°æ®
        
        å‚æ•°ï¼š
            file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        
        è¿”å›ï¼š
            çƒ­æœæ¡ç›®åˆ—è¡¨æˆ– None
        """
        try:
            if not os.path.exists(file_path):
                self.logger.warning(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return None
            
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # æ£€æŸ¥æ•°æ®æ˜¯å¦æœ‰æ•ˆ
            if 'data' not in data or not data['data']:
                self.logger.warning("æ•°æ®æ–‡ä»¶æ ¼å¼æ— æ•ˆæˆ–ä¸ºç©º")
                return None
            
            # æ£€æŸ¥æ•°æ®æ—¶é—´ï¼ˆå¯é€‰ï¼šå¦‚æœå¤ªæ—§å¯ä»¥æç¤ºï¼‰
            if 'timestamp' in data:
                from datetime import datetime
                timestamp = datetime.fromisoformat(data['timestamp'])
                age_hours = (datetime.now() - timestamp).total_seconds() / 3600
                
                if age_hours > 24:
                    self.logger.warning(f"æ•°æ®å·²è¿‡æœŸ {age_hours:.1f} å°æ—¶ï¼Œå»ºè®®æ›´æ–°")
                else:
                    self.logger.info(f"ä½¿ç”¨ç¼“å­˜æ•°æ®ï¼ˆ{age_hours:.1f} å°æ—¶å‰ï¼‰")
            
            return data['data']
        except Exception as e:
            self.logger.error(f"è¯»å–æ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
            return None

    def save_to_file(self, data: List[Dict[str, Any]], file_path: str = "output/realtime_hot.json") -> bool:
        """
        ä¿å­˜å®æ—¶çƒ­æœæ•°æ®åˆ°æ–‡ä»¶
        
        å‚æ•°ï¼š
            data: çƒ­æœæ¡ç›®åˆ—è¡¨
            file_path: ä¿å­˜è·¯å¾„
        
        è¿”å›ï¼š
            ä¿å­˜æˆåŠŸè¿”å› True
        """
        try:
            # åˆ›å»ºè¾“å‡ºç›®å½•
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            
            # å‡†å¤‡æ•°æ®
            save_data = {
                "timestamp": datetime.now().isoformat(),
                "count": len(data),
                "data": data
            }
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"æ•°æ®å·²ä¿å­˜åˆ° {file_path}")
            return True
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
            return False

    def fetch_realtime_top50(self, use_cache: bool = True, cache_file: str = "output/realtime_hot.json") -> List[Dict[str, Any]]:
        """
        è·å–å®æ—¶çƒ­æœ Top 50

        ä¼˜å…ˆçº§ï¼š
        1. å¦‚æœ use_cache=Trueï¼Œå…ˆå°è¯•ä»ç¼“å­˜æ–‡ä»¶è¯»å–ï¼ˆå¦‚æœè¿˜æœ‰æ•ˆï¼‰
        2. å°è¯• Playwright æ–¹æ³•ï¼ˆé¦–é€‰ï¼Œèƒ½ç»•è¿‡è®¿å®¢éªŒè¯ï¼‰
        3. å°è¯• HTTP è¯·æ±‚æ–¹æ³•ï¼ˆå¤‡ç”¨ï¼‰
        4. å¦‚æœéƒ½å¤±è´¥ä½†æœ‰ç¼“å­˜ï¼Œè¿”å›ç¼“å­˜æ•°æ®ï¼ˆå³ä½¿è¿‡æœŸï¼‰
        
        å‚æ•°ï¼š
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜æ–‡ä»¶
            cache_file: ç¼“å­˜æ–‡ä»¶è·¯å¾„
        
        è¿”å›ï¼š
            çƒ­æœæ¡ç›®åˆ—è¡¨ï¼ˆæœ€å¤š 50 é¡¹ï¼‰
        """
        # ç­–ç•¥ 1: å°è¯•ä»ç¼“å­˜æ–‡ä»¶è¯»å–ï¼ˆä½†è¦æ£€æŸ¥æœ‰æ•ˆæ€§ï¼‰
        cached_data = None
        if use_cache:
            cached_data = self.load_from_file(cache_file)
            if cached_data:
                # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆï¼ˆå°‘äº1å°æ—¶ï¼‰
                try:
                    import json
                    with open(cache_file, 'r', encoding='utf-8') as f:
                        cache_meta = json.load(f)
                    if 'timestamp' in cache_meta:
                        from datetime import datetime
                        timestamp = datetime.fromisoformat(cache_meta['timestamp'])
                        age_hours = (datetime.now() - timestamp).total_seconds() / 3600
                        if age_hours < 1:  # å°‘äº1å°æ—¶çš„ç¼“å­˜
                            self.logger.info(f"ä»ç¼“å­˜æ–‡ä»¶åŠ è½½äº† {len(cached_data)} ä¸ªçƒ­æœï¼ˆ{age_hours:.1f}å°æ—¶å‰ï¼‰")
                            return cached_data[:50]
                except Exception:
                    pass
        
        # ç­–ç•¥ 2: ä¼˜å…ˆå°è¯• Playwright æ–¹æ³•ï¼ˆèƒ½ç»•è¿‡è®¿å®¢éªŒè¯ï¼‰
        self.logger.info("å°è¯• Playwright æ–¹æ³•...")
        try:
            items = self.fetch_realtime_top50_with_playwright()
            if items:
                self.logger.info(f"Playwright æ–¹æ³•æˆåŠŸè·å– {len(items)} ä¸ªçƒ­æœ")
                # ä¿å­˜åˆ°ç¼“å­˜
                self.save_to_file(items, cache_file)
                return items
        except Exception as e:
            self.logger.warning(f"Playwright æ–¹æ³•å¤±è´¥: {e}")
        
        # ç­–ç•¥ 3: å°è¯• HTTP è¯·æ±‚æ–¹æ³•ï¼ˆå¤‡ç”¨ï¼‰
        self.logger.info("å°è¯• HTTP è¯·æ±‚æ–¹æ³•...")
        html = self.fetch_realtime_page()
        if html:
            items = self.parse_realtime_page(html)[:50]
            if items:
                self.logger.info(f"HTTP è¯·æ±‚æ–¹æ³•æˆåŠŸè·å– {len(items)} ä¸ªçƒ­æœ")
                # ä¿å­˜åˆ°ç¼“å­˜
                self.save_to_file(items, cache_file)
                return items
            else:
                self.logger.warning("HTTP è¯·æ±‚æ–¹æ³•æœªèƒ½è§£æåˆ°çƒ­æœæ•°æ®")
        else:
            self.logger.warning("HTTP è¯·æ±‚æ–¹æ³•å¤±è´¥ï¼ˆå¯èƒ½é‡åˆ°è®¿å®¢éªŒè¯ï¼‰")
        
        # ç­–ç•¥ 4: å¦‚æœéƒ½å¤±è´¥ï¼Œè¿”å›ç¼“å­˜æ•°æ®ï¼ˆå³ä½¿è¿‡æœŸï¼‰æˆ–ç©ºåˆ—è¡¨
        if cached_data:
            self.logger.warning(f"æ‰€æœ‰è·å–æ–¹æ³•éƒ½å¤±è´¥ï¼Œä½¿ç”¨ç¼“å­˜æ•°æ®ï¼ˆå¯èƒ½å·²è¿‡æœŸï¼‰")
            return cached_data[:50]
        
        # éƒ½å¤±è´¥äº†
        self.logger.error("æ— æ³•è·å–å®æ—¶çƒ­æœæ•°æ®ï¼ˆæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼‰")
        return []

    def fetch_and_save(self, output_file: str = "output/realtime_hot.json") -> bool:
        """
        è·å–å®æ—¶çƒ­æœå¹¶ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆç‹¬ç«‹è„šæœ¬ä½¿ç”¨ï¼‰
        
        å‚æ•°ï¼š
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        
        è¿”å›ï¼š
            æˆåŠŸè¿”å› True
        """
        self.logger.info("å¼€å§‹è·å–å®æ—¶çƒ­æœæ•°æ®...")
        
        # ä¸ä½¿ç”¨ç¼“å­˜ï¼Œå¼ºåˆ¶è·å–æ–°æ•°æ®
        items = self.fetch_realtime_top50(use_cache=False, cache_file=output_file)
        
        if items:
            self.logger.info(f"æˆåŠŸè·å– {len(items)} ä¸ªçƒ­æœ")
            return True
        else:
            self.logger.error("è·å–å®æ—¶çƒ­æœå¤±è´¥")
            return False

    def fetch_realtime_top50_with_playwright(self) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨ Playwright è·å–å®æ—¶çƒ­æœ Top 50ï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰

        ç›¸æ¯”äºç›´æ¥ HTTP è¯·æ±‚ï¼Œè¿™ä¸ªæ–¹æ³•ï¼š
        1. ä½¿ç”¨çœŸå®æµè§ˆå™¨å¼•æ“ï¼ˆChromiumï¼‰
        2. æ‰§è¡Œ JavaScript ä»£ç ï¼ŒåŠ è½½åŠ¨æ€å†…å®¹
        3. ç»•è¿‡è®¿å®¢éªŒè¯ç³»ç»Ÿ
        4. é€šè¿‡å¤šæ¬¡æ»šåŠ¨åŠ è½½æ›´å¤šçƒ­æœé¡¹
        5. æ›´æ¥è¿‘çœŸå®ç”¨æˆ·è¡Œä¸º

        æ³¨æ„ï¼šåœ¨ Windows ä¸Šçš„ Streamlit ç¯å¢ƒä¸­ï¼Œä½¿ç”¨å­è¿›ç¨‹æ¥é¿å… asyncio å†²çª

        è¿”å›ï¼š
            çƒ­æœæ¡ç›®åˆ—è¡¨ï¼ˆæœ€å¤š 50 é¡¹ï¼‰

        ä¾èµ–ï¼š
            playwright>=1.40.0ï¼ˆéœ€è¦å…ˆè¿è¡Œï¼špython -m playwright install chromiumï¼‰
        """
        import sys
        is_streamlit = 'streamlit' in sys.modules
        
        if is_streamlit:
            # åœ¨ Streamlit ç¯å¢ƒä¸­ä½¿ç”¨å­è¿›ç¨‹è¿è¡Œ Playwright
            self.logger.info("åœ¨ Streamlit ç¯å¢ƒä¸­è¿è¡Œ Playwrightï¼ˆå­è¿›ç¨‹æ¨¡å¼ï¼‰")
            import subprocess
            import json
            import tempfile
            import os
            
            try:
                # åˆ›å»ºä¸´æ—¶æ–‡ä»¶å­˜å‚¨ç»“æœ
                with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.json', encoding='utf-8') as tmp:
                    tmp_file = tmp.name
                
                # Python ä»£ç å­—ç¬¦ä¸²ï¼šåœ¨å­è¿›ç¨‹ä¸­è¿è¡Œ Playwright
                script = f"""
import sys
sys.path.insert(0, {repr(os.getcwd())})
from src.scrap import RealtimeHotScraper
import json

scraper = RealtimeHotScraper()
items = scraper._run_playwright_browser()

with open({repr(tmp_file)}, 'w', encoding='utf-8') as f:
    json.dump(items, f, ensure_ascii=False)
"""
                
                # è¿è¡Œå­è¿›ç¨‹ï¼ˆä½¿ç”¨ stderr=DEVNULL å¿½ç•¥ç¼–ç é”™è¯¯ï¼‰
                result = subprocess.run(
                    [sys.executable, '-c', script],
                    stdout=subprocess.DEVNULL,
                    stderr=subprocess.DEVNULL,
                    timeout=120,
                )
                
                if result.returncode == 0:
                    # è¯»å–ç»“æœ
                    try:
                        with open(tmp_file, 'r', encoding='utf-8') as f:
                            items = json.load(f)
                        self.logger.info(f"å­è¿›ç¨‹æˆåŠŸè¿”å› {len(items)} ä¸ªçƒ­æœ")
                        return items
                    finally:
                        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                        try:
                            os.remove(tmp_file)
                        except:
                            pass
                else:
                    self.logger.error(f"å­è¿›ç¨‹æ‰§è¡Œå¤±è´¥")
                    return []
                    
            except subprocess.TimeoutExpired:
                self.logger.error("å­è¿›ç¨‹è¶…æ—¶")
                return []
            except Exception as e:
                self.logger.error(f"å­è¿›ç¨‹æ‰§è¡Œå‡ºé”™: {e}")
                return []
        else:
            # é Streamlit ç¯å¢ƒï¼Œç›´æ¥è¿è¡Œ
            return self._run_playwright_browser()
    
    def _run_playwright_browser(self) -> List[Dict[str, Any]]:
        """
        çœŸå®çš„ Playwright æµè§ˆå™¨è¿è¡Œé€»è¾‘
        
        è¿”å›ï¼š
            çƒ­æœæ¡ç›®åˆ—è¡¨
        """
        try:
            from playwright.sync_api import sync_playwright
        except ImportError:
            return []

        html_content = None

        try:
            self.logger.info("æ­£åœ¨ä½¿ç”¨ Playwright åŠ è½½é¡µé¢...")
            
            with sync_playwright() as p:
                # å¯åŠ¨ Chromium æµè§ˆå™¨
                self.logger.debug("å¯åŠ¨ Chromium æµè§ˆå™¨")
                browser = p.chromium.launch(headless=True, args=['--no-sandbox', '--disable-setuid-sandbox'])
                self.logger.info("æµè§ˆå™¨å¯åŠ¨æˆåŠŸ")

                page = browser.new_page()
                
                # è®¾ç½®æµè§ˆå™¨æ ‡è¯†ï¼Œè¿›ä¸€æ­¥æ¨¡æ‹ŸçœŸå®ç”¨æˆ·
                page.set_extra_http_headers({
                    "Accept-Language": "zh-CN,zh;q=0.9",
                })

                try:
                    # è®¿é—®æ–°çš„æ¦œå•é¡µï¼ˆæ— éœ€ç™»å½•ï¼Œæ›´ç¨³å®šï¼‰
                    self.logger.info(f"æ­£åœ¨è®¿é—® {self.base_url} ...")
                    try:
                        page.goto(
                            self.base_url,
                            wait_until="domcontentloaded",
                            timeout=60000
                        )
                        self.logger.info("é¡µé¢åŠ è½½å®Œæˆ")
                    except Exception as e:
                        self.logger.warning(f"é¡µé¢åŠ è½½è¶…æ—¶ï¼Œç»§ç»­ä½¿ç”¨å·²åŠ è½½å†…å®¹: {e}")

                    # ç­‰å¾…åˆå§‹å†…å®¹åŠ è½½
                    page.wait_for_timeout(1500)

                    # æ¦œå•é¡µé€šå¸¸æ— éœ€æ»šåŠ¨å³å¯è·å¾— Top50
                    # ä¸ºç¨³å¦¥ï¼Œè½»å¾®æ»šåŠ¨ä¸¤æ¬¡è§¦å‘å¯èƒ½çš„æ‡’åŠ è½½
                    for i in range(2):
                        page.evaluate("window.scrollBy(0, window.innerHeight)")
                        page.wait_for_timeout(400)

                    # è·å–æœ€ç»ˆçš„é¡µé¢ HTML
                    html_content = page.content()
                    self.logger.info(f"é¡µé¢å†…å®¹è·å–æˆåŠŸï¼Œå¤§å°ï¼š{len(html_content)} bytes")

                    # ä¿å­˜è°ƒè¯• HTMLï¼ˆå¯é€‰ï¼‰
                    try:
                        with open("debug_realtime_page_playwright.html", "w", encoding="utf-8") as f:
                            f.write(html_content)
                        self.logger.debug("HTML å·²ä¿å­˜åˆ° debug_realtime_page_playwright.html")
                    except Exception as e:
                        self.logger.debug(f"ä¿å­˜è°ƒè¯• HTML å¤±è´¥: {e}")

                finally:
                    browser.close()
                    self.logger.info("æµè§ˆå™¨å·²å…³é—­")

        except ImportError:
            self.logger.error("Playwright åº“æœªå®‰è£…")
            return []
        except Exception as e:
            self.logger.error(f"Playwright è·å–é¡µé¢å¤±è´¥: {e}")
            return []

        if not html_content:
            self.logger.error("æœªè·å–åˆ°é¡µé¢å†…å®¹")
            return []

        # è§£æè·å–åˆ°çš„ HTML
        items = self.parse_realtime_page(html_content)
        return items[:50]


if __name__ == "__main__":
    main()

